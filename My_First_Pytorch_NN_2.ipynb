{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rete neurale con Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo articolo voglio costruire la mia prima rete neurale con Pytorch.\n",
    "Premesso che ho già una infarinatura teorica delle reti neurali, voglio vedere come implementarne una usando PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo è quello di cui abbiamo bisogno:<br>\n",
    "\\- un dataset <br>\n",
    "\\- un modello della rete neurale <br>\n",
    "\\- la definizione di una funzione errore <br>\n",
    "\\- la definizione di un ottimizzatore <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizieremo vedendo gli elementi che ci servono per definire una rete neurale semplice, limitandoci a layer lineari e ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In seguito vedremo come mettere insieme i layer per costruire il modello della rete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alla fine implementeremo una rete, e la ottimizzeremo per un problema specifico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importare i packackes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il Layer lineare - uscita singola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il layer lineare <strong>torch.nn.Linear</strong> si chiama così perchè realizza una combinazione lineare degli ingressi.<br>\n",
    "Nel caso (improbabile) di rete con una feature di ingresso (x, scalare) e una di uscita (y, scalare), l'operazione è semplicemente: $w x + b$<br>\n",
    "Dove $w$ è chiamato peso (weight) e $b$ è chiamato bias.<br>\n",
    "\n",
    "Nel caso di rete con N ingressi, $x$ è un tensore N-dimensionale. $y$ è un tensore M-dimensionale, con M features di uscita.<br>\n",
    "Così la relazione ingresso-uscita della rete neurale è:  \n",
    "\n",
    "$y = w \\cdot x + b$<br>\n",
    "\n",
    "Con $w$ tensore MxN e $b$ tensore M-dimensionale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0464], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay1 = nn.Linear(in_features=10, out_features=1) #definisce il layer linear\n",
    "\n",
    "torch.manual_seed(1234) #inizializza il generatore pseudorandom con un seed, in modo da avere sempre gli stessi random generati\n",
    "\n",
    "idata = torch.rand(10) #genero una sequenza di 10 numeri random compresi tra [0,1)\n",
    "\n",
    "lay1(idata) #calcola l'uscita del layer linear con input idata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questi sono alcuni dei parametri del layer lineare:\n",
    "- **weigth** uguale al numero degli ingressi (o input features),\n",
    "- **bias** che coincide col numero delle uscite (o output features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.1471,  0.1833, -0.0853,  0.2205,  0.0211, -0.3075,  0.1609, -0.0509,\n",
       "           0.0621, -0.1562]], requires_grad=True), Parameter containing:\n",
       " tensor([0.0652], requires_grad=True))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay1.weight, lay1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il layer lineare realizza il calcolo $w x+b$.\n",
    "\n",
    "Facciamo una rapida verifica, comparando l'uscita $w x + b$ con il calcolo lay1(idata).\n",
    "\n",
    "Nel caso di uscita singola ho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0464])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.matmul(lay1.weight.data, idata)+lay1.bias).data #w*x+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si vede i due risultati sono uguali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il layer linear - più uscite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideriamo il caso a due nodi, ovvero due labels (uscite della rete neurale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0827, 0.3945], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)                           #inizializziamo il generatore di numeri pseudorandom\n",
    "idata = torch.rand(10)                           #creiamo un tensore di dati per l'ingresso\n",
    "lay2 = nn.Linear(in_features=10, out_features=2) #definiamo un layer lineare\n",
    "y2 = lay2(idata)                                 #calcoliamo l'uscita del layer\n",
    "y2                                               #visualizziamo l'uscita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo i pesi e i bias del layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.1168,  0.1183, -0.2684, -0.1919, -0.1161, -0.0621, -0.2412,  0.2071,\n",
       "          -0.0746,  0.1015],\n",
       "         [ 0.2236,  0.0589,  0.0865,  0.3052, -0.1426,  0.1002, -0.1407,  0.2260,\n",
       "           0.2526, -0.2916]], requires_grad=True), Parameter containing:\n",
       " tensor([0.2699, 0.1510], requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay2.weight, lay2.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso faccio il calcolo manualmente come $w\\cdot x+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0827, 0.3945])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2_0 = torch.matmul(lay2.weight[0,:].data,idata)+lay2.bias[0].data #uscita w*x+b del nodo 0\n",
    "out2_1 = torch.matmul(lay2.weight[1,:].data,idata)+lay2.bias[1].data #uscita w*x+b del nodo 1\n",
    "out2 = torch.tensor([out2_0, out2_1])\n",
    "out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si vede i due risultati y2 e out2 sono uguali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione ReLU (Rectifier Linear Unit) è tra le funzioni di attivazione più utilizzate. La funzione, non lineare, è così definita:\n",
    "\n",
    "\\begin{equation*}\n",
    "relu(x) = \n",
    "\\begin{cases}\n",
    "  0 & \\text{for }x<0\\\\    \n",
    "  x & \\text{for }x\\geqslant\t0\\\\  \n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "In maniera grossolana si può dire che le funzioni di attivazione non lineari sono necessarie nelle reti neurali per realizzare funzioni complesse. Altrimenti avremmo solo combinazioni lineari degli ingressi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il layer <strong>nn.ReLU()</strong> ha un numero di uscite pari al numero di ingressi. Ad ogni ingresso xi corrisponde una uscita yi, e la relazione tra i due è:\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i = relu(x_i)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo il layer ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay3 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo un tensore 10-dimensionale, di dati random con distribuzione normale standard (media zero e varianza unitaria):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0386, -0.8861, -0.4709, -0.4269, -0.0283,  1.4220, -0.3886, -0.8903,\n",
       "        -0.9601, -0.4087])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idata = torch.randn(10)\n",
    "idata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si vede ci sono sia valori positivi che negativi. Il layer ReLU dovrebbe assegnare il valore 0 a tutti i valori negativi. Verifichiamolo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.4220, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay3(idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sembra che il layer ReLU faccia il suo lavoro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack di layer lineari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono due modi per mettere in cascata (o in stack) due layer.<br>\n",
    "Il più semplice è quello di assegnare l'uscita del primo layer all'ingresso del secondo layer.<br>\n",
    "Come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3289, 0.3651], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay1 = nn.Linear(in_features=10, out_features=5) #definizione del layer 1\n",
    "lay2 = nn.Linear(in_features=5, out_features=2)  #definizione del layer 2\n",
    "\n",
    "x1 = idata    #assegniamo i dati generati all'ingresso del layer 1\n",
    "y1 = lay1(x1) #uscita layer 1\n",
    "x2 = y1       #uscita del layer 1 all'ingresso del layer 2\n",
    "y2 = lay2(x2) #uscita layer 2\n",
    "\n",
    "y2            #visualizziamo l'uscita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modo più immediato (specialmente se ci sono più layer) è quello di usare il modulo <strong>torch.nn.Sequential</strong>.<br>\n",
    "Questo modulo realizza lo stack come desiderato, ovvero l'uscita del layer 1 viene usata come ingresso del layer 2.<br>\n",
    "Con questa definizione avremo un modulo rete neurale il cui ingresso coincide con l'ingresso del layer 1, e la cui uscita coincide con l'uscita del layer 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = nn.Sequential(\n",
    "    lay1,\n",
    "    lay2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'uscita del modulo <strong>my_nn</strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3289, 0.3651], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnn = idata        #assegniamo i dati generati all'ingresso del modulo my_nn\n",
    "ynn = my_nn(idata) #calcoliamo l'uscita\n",
    "ynn                #visualizziamo l'uscita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si vede il risultato è uguale nei due casi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per un numero elevato di layer è sicuramente più comodo usare <strong>nn.Sequential</strong>. <br>\n",
    "Nell'esempio precedente ho voluto usare lay1 e lay2 anche in my_nn, per evitare di avere inizializzazioni diverse dei pesi e dei bias, e verificare che i calcoli generati nei due casi siano uguali.<br>\n",
    "In realtà non ho bisogno di usare le variabili aggiuntive lay1 e lay2 per i layer. L'uso più naturale del layer <strong>nn.Sequential</strong> è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = nn.Sequential(\n",
    "    nn.Linear(in_features=10, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo modo ho definito una rete neurale, ma non ho fatto il training della rete, cioè non ho ottimizzato i pesi e i bias per fare in modo che l'errore sia minimo.<br>\n",
    "In questo momento, i pesi e i bias sono random, per cui la rete neurale svolge una funzione random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di fare il training della rete, vediamo come implementare la rete neurale usando la classe torch.nn.Module, che presenta alcuni vantaggi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la classe Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno dei modi per implementare una rete neurale è usare la classe <strong>torch.nn.Module</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchNN(nn.Module):\n",
    "    \n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Nel constructur definisco i Linear layer e li assegno a variabili membri della classe.\n",
    "        \"\"\"\n",
    "        super(PyTorchNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=10, out_features=5)\n",
    "        self.layer2 = nn.Linear(in_features=5,  out_features=2)\n",
    "    \n",
    "    #predictor\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Nella funzione forward ho un tensore di dati in input, e un tensore di risultati in uscita.\n",
    "        Posso usare i layer definiti nel constructor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # L'uscita si può definire così:\n",
    "        x1 = F.relu(self.layer1(x)) \n",
    "        x2 = F.relu(self.layer2(x1))\n",
    "        \n",
    "        #oppure così:\n",
    "        x3 = nn.Sequential(self.layer1, \n",
    "                           nn.ReLU(),\n",
    "                           self.layer2, \n",
    "                           nn.ReLU())(x)\n",
    "        \n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyNN = PyTorchNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generiamo dei dati random. La funzione randn(N) genera un tensore N-dimensionale di numeri random, con distribuzione normale standard (ovvero con media nulla e varianza 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = torch.randn(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applichiamo i dati all'ingresso della rete neurale. In uscita avremo il tensore ynn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5180], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ynn = MyNN(idata)\n",
    "ynn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancora una volta voglio verificare che i calcoli siano giusti, ovvero che la semplice rete neurale da me progettata faccia i calcoli che mi aspetto. Per vedere se ho capito bene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questi sono i pesi del layer 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1820,  0.1631,  0.2234, -0.3068, -0.2684, -0.3079,  0.1193,  0.2545,\n",
       "         -0.2452, -0.1464],\n",
       "        [ 0.1006, -0.2065,  0.2686,  0.0737, -0.0880,  0.0205,  0.0986, -0.1118,\n",
       "         -0.2450,  0.0021],\n",
       "        [ 0.0058,  0.0064, -0.0461,  0.2030, -0.0882, -0.0306,  0.1300, -0.1990,\n",
       "          0.0847, -0.0699],\n",
       "        [ 0.1517, -0.1715,  0.0117,  0.0309, -0.2544, -0.2299,  0.1213, -0.0920,\n",
       "          0.1878, -0.3124],\n",
       "        [-0.1563, -0.2605,  0.1263, -0.0092, -0.0590, -0.0526, -0.2472,  0.0897,\n",
       "          0.0079, -0.2182]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyNN.layer1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E questi sono i bias del layer 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1190, -0.0064, -0.3058,  0.1701,  0.1691], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyNN.layer1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo layer esegue un'operazione del tipo <strong>relu($w_1 \\cdot x_1 + b_1$)</strong>. Dove $w_1$ e $b_1$ sono pesi e bias del layer 1.\n",
    "\n",
    "Applichiamo i dati di input <strong>idata</strong> al primo layer, l'uscita sarà il tensore yt1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.3123, 0.0000, 0.0000, 0.7073], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt1 = F.relu(torch.matmul(MyNN.layer1.weight,idata)+MyNN.layer1.bias)\n",
    "yt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'uscita del primo layer è applicata all'ingresso del secondo layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = yt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il secondo layer, esegue l'operazione <strong>relu($w_2 \\cdot x_2 + b_2$)</strong>. Con $w_2$ e $b_2$ indico i pesi e i bias del layer 2.\n",
    "Adesso applichiamo il tensore x2 al secondo layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5180], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt2 = F.relu(torch.matmul(MyNN.layer2.weight,x2) + MyNN.layer2.bias)\n",
    "yt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamo arrivati al risultato che volevamo, i calcoli fatti nei due modi sono equivalenti (ynn = yt2).<br> \n",
    "La mia comprensione (e spero anche la vostra) dell'implementazione di una rete neurale in Pytorch è corretta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un altro modo per accedere ai parametri -pesi e bias- è usare la funzione <strong>parameters()</strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1820,  0.1631,  0.2234, -0.3068, -0.2684, -0.3079,  0.1193,  0.2545,\n",
       "          -0.2452, -0.1464],\n",
       "         [ 0.1006, -0.2065,  0.2686,  0.0737, -0.0880,  0.0205,  0.0986, -0.1118,\n",
       "          -0.2450,  0.0021],\n",
       "         [ 0.0058,  0.0064, -0.0461,  0.2030, -0.0882, -0.0306,  0.1300, -0.1990,\n",
       "           0.0847, -0.0699],\n",
       "         [ 0.1517, -0.1715,  0.0117,  0.0309, -0.2544, -0.2299,  0.1213, -0.0920,\n",
       "           0.1878, -0.3124],\n",
       "         [-0.1563, -0.2605,  0.1263, -0.0092, -0.0590, -0.0526, -0.2472,  0.0897,\n",
       "           0.0079, -0.2182]], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.1190, -0.0064, -0.3058,  0.1701,  0.1691], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.0843, -0.3088,  0.0180,  0.3375,  0.4094],\n",
       "         [-0.3376, -0.2020,  0.3482,  0.2186,  0.2768]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.2226,  0.3853], requires_grad=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(MyNN.parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manca un nome associato a ciascun elemento della lista, ma il significato è chiaro lo stesso. La lista contiene i tensori weight e bias del layer 1, e i tensori weight e bias del layer 2.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il training della rete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La rete neurale da noi definita ha dei parametri che sono i pesi (weight) e i bias (non ci provo neanche a tradurlo), rispettivamente $w_{i,j}$ e  $b_{i,j}$.<br>\n",
    "Cambiando questi parametri cambia la capacità della rete neurale di svolgere la funzione da noi richiesta, ovvero la capacità di <strong>fittare</strong> il dataset input-output a disposizione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per definire quanto bene la rete neurale è in grado di fittare il dataset occorre definire una metrica, che ci dica quanto bene questo fitting sia stato realizzato. Questa metrica è la funzione errore, o anche loss function o anche cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il training della rete avviene in modo iterativo.\n",
    "Alla prima iterazione i parametri -pesi e bias- sono inizializzati in maniera random.\n",
    "Ad ogni iterazione l'uscita della rete viene calcolata utilizzando i parametri -pesi e bias- correnti, partendo dagli ingressi si possono calcolare le uscita, secondo lo schema propagativo visto in precedenza, questo è il calcolo <strong>forward</strong> (in avanti).<br> \n",
    "Utilizzando le uscite così calolate e i target si calcola l'errore in questa iterazione.\n",
    "\n",
    "Una funzione di errore molto usata è <strong>l'errore quadratico medio</strong>, in PyTorch implementato nel modulo torch.nn.MSELoss. L'errore quadratico medio allo step n è:\n",
    "\n",
    "\\begin{equation*}\n",
    "Loss^{(n)}(w_{i,j},b_{i,j}) =  \\frac{\\sum_{k=1}^M{(ynn_k^{(n)} - ytarget_k)^2}}{N} \n",
    "\\end{equation*}\n",
    "\n",
    "$ynn_k^{(n)}$ è la k-esima uscita della rete neurale allo step n.<br>\n",
    "$ytarget_k$ è la k-esima label, ovvero la parte di ouput del dataset.<br>\n",
    "$M$ è il numero delle features di uscita.<br>\n",
    "$N$ è il numero di samples del dataset.\n",
    "\n",
    "Adesso voglio minimizzare l'errore, cioè voglio sapere i valori da dare ai parametri -pesi e bias- per minimizzare l'errore.<br>\n",
    "Per questo posso utilizzare un classico algoritmo di minimizzazione del gradiente (<em>gradient descent algorithm</em>), che fa uso delle derivate parziali della funzione errore rispetto a ciascun parametro per trovare la direzione in cui si ha la diminuzione più rapida del gradiente.\n",
    "\n",
    "Per calcolare le derivate parziali in maniera computazionalmente efficiente si usa un algoritmo chiamato <strong>backpropagation</strong>. L'algoritmo di backpropagation ha bisogno di una pagina a sé per essere spiegato, per cui al momento lo lascio alla vostra buona volontà.<br>\n",
    "Inoltre, sempre per essere più efficienti la funzione errore può essere calcolata su un numero ridotto di samples invece che su tutti. Si vedano <em>stochastic gradient descent</em> o <em>mini-batch gradient descent</em>.\n",
    "\n",
    "Dopo avere calcolato le derivate parziali si possono aggiornare i parametri della rete:\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i,j}(n+1)  = w_{i,j}(n) - l_r \\cdot \\frac{\\partial Loss^{(n)}(w_{i,j},b_{i,j})}{\\partial w_{i,j}} \\hspace{1cm} \\forall i,j\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "b_{i,j}(n+1)  = b_{i,j}(n) - l_r \\cdot \\frac{\\partial Loss^{(n)}(w_{i,j},b_{i,j})}{\\partial b_{i,j}} \\hspace{1cm} \\forall i,j\n",
    "\\end{equation*}\n",
    "\n",
    "$l_r$ è il learning rate. Questo, come dice il nome, controlla la velocità con cui i parametri convergeranno (se) verso l'ottimo. Un $l_r$ troppo alto comprometterà la convergenza dell'algoritmo. Un valore troppo basso renderà la convergenza troppo lenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La funzione errore (o di perdita, o loss function, o cost function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo il package <strong>torch.optim</strong> che contiene gli algoritmi di ottimizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo una rete di 3 layer lineari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorchNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=3, out_features=3)\n",
    "        self.layer2 = nn.Linear(in_features=3,  out_features=3)\n",
    "        self.layer3 = nn.Linear(in_features=3,  out_features=3)\n",
    "    def forward(self, x):\n",
    "        x3 = nn.Sequential(self.layer1,\n",
    "                           self.layer2,\n",
    "                           self.layer3,\n",
    "                          )(x)\n",
    "        return x3\n",
    "\n",
    "MyNN = PyTorchNN()                      #instanziamo la rete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo un <strong>dataset</strong> di dati. I dati di ingresso, sono delle triplette \\[x0,x1,x2\\] di interi random.<br>\n",
    "Generiamo delle uscite come combinazioni lineari degli ingressi:<br>\n",
    "y0 = x0<br>\n",
    "y1 = x0 + x1<br>\n",
    "y2 = x0 + x1 + x2<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definiamo un dataset di training data\n",
    "idata = torch.randint(0,100,(1000,3), dtype=torch.float) #dati di ingresso\n",
    "\n",
    "odata = torch.empty(idata.shape)                         #generiamo un tensore non inizializzato\n",
    "odata[:,0] = idata[:,0]                                  #y0\n",
    "odata[:,1] = idata[:,0]+idata[:,1]                       #y1\n",
    "odata[:,2] = idata[:,0]+idata[:,1]+idata[:,2]            #y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricapitolando il problema, sia per me che per voi.<br>\n",
    "<strong>(idata, odata)</strong> è il dataset che voglio interpolare con una rete neurale. Ovvero fornendo in ingresso <strong>idata</strong>, voglio che l'uscita della rete sia il più vicino possibile a <strong>odata</strong>. <br>\n",
    "Ogni ingresso ed ogni uscita è costituito da una tripletta di valori [x0,x1,x2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premesso che trovo la notazione abbastanza poco intuitiva, vediamo come si definisce la funzione di errore. E come si usa.\n",
    "Di seguito il codice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynn = MyNN(idata)                 #calcola l'uscita della rete neurale con i dati di ingresso specificati\n",
    "loss = torch.nn.MSELoss()         #definisce quale funzione errore: Mean Squared Error (errore quadratico medio)\n",
    "error = loss(ynn, odata)          #calcola l'errore tra il target e l'uscita corrente\n",
    "error.backward()                  #calcola i gradienti rispetto a tutti i tensori weight e bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo passo è quello di specificare quale funzione errore si vuole utilizzare. In questo caso ho optato per <strong>torch.nn.MSELoss</strong>.<br>\n",
    "Successivamente calcolo l'errore attuale, cioè usando i weight e i bias correnti, semplicemente eseguendo <strong>loss(ynn,ydata)</strong>.<br>\n",
    "Infine con la funzione <strong>backward</strong> eseguo la backpropagation che calcola i gradienti dell'errore rispetto a tutti i tensori weight e bias.<br>\n",
    "La funzione backward() è definita automaticamente. Per saperne di più si può visitare la pagina sul package [torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso bisogna definire un ottimizzatore. Questo fa uso dei gradient calcolati e aggiorna i parametri (weight e bias) con una strategia che dipende dall'ottimizzatore usato.\n",
    "Innanzitutto occorre definire l'ottimizzatore, bisogna specificare quali parametri vengono ottimizzati, semplicemente <strong>MyNN.parameters()</strong>, e il learning rate (<strong>lr</strong>) o velocità di apprendimento.\n",
    "Nel nostro caso usiamo lo SGD (Stochastic Gradient Descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(MyNN.parameters(),lr=1e-5) #definizione dell'ottimizzatore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per qualche motivo sconosciuto, prima di chiamare la funzione backward() per calcolare i gradienti, occorre azzerare questi ultimi, con il seguente comando: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo avere calcolato i gradienti, si possono aggiornare i parametri che vengono ottimizzati: weight e bias.<br>\n",
    "Le formule per l'aggiornamento dei weight dallo step n a quello n+1 sono:\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{i,j}(n+1)  = w_{i,j}(n) - lr \\cdot \\frac{\\partial loss(n)}{\\partial w_{i,j}} \\hspace{1cm} \\forall i,j\n",
    "\\end{equation*}\n",
    "\n",
    "E analogamente per l'aggiornamento dei bias.\n",
    "\n",
    "\\begin{equation*}\n",
    "b_{i,j}(n+1)  = b_{i,j}(n) - lr \\cdot \\frac{\\partial loss(n)}{\\partial b_{i,j}} \\hspace{1cm} \\forall i,j\n",
    "\\end{equation*}\n",
    "\n",
    "Per fortuna non dobbiamo aggiornare i parametri manualmente, ma c'è la funzione <strong>step()</strong> dell'ottimizzatore che si occupa di farlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riassumendo, i passaggi per eseguire l'ottimizzazione sono indicati nel loop sottostante: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13654.5498, grad_fn=<MseLossBackward>)\n",
      "tensor(218.6053, grad_fn=<MseLossBackward>)\n",
      "tensor(167.0868, grad_fn=<MseLossBackward>)\n",
      "tensor(18.1930, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0852, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0662, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0659, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0656, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0653, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0651, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):             #esegue X iterazioni\n",
    "    ynn = MyNN(idata)            #calcola l'uscita della rete neurale\n",
    "    error = loss(ynn, odata)      #calcola l'errore corrente\n",
    "    error.backward()              #calcola i gradienti\n",
    "    optimizer.step()              #aggiorna i parametri\n",
    "    optimizer.zero_grad()         #azzera i gradienti\n",
    "    if np.mod(i,500)==0:\n",
    "        print(error)              #visualizza l'errore ogni 100 iterazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si vede sopra l'errore è passato da un valore iniziale molto alto di 14998 un valore di 0.0297. Quindi il training ha funzionato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una sommaria comparazione di <strong>odata</strong> e <strong>ynn</strong> mostra che i valori sono sufficientemente vicini per lo scopo di questo articolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9.,  62.,  67.],\n",
       "        [ 34.,  76., 161.],\n",
       "        [ 94., 114., 179.],\n",
       "        ...,\n",
       "        [ 12.,  74.,  91.],\n",
       "        [  2.,  24., 118.],\n",
       "        [ 89., 178., 198.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  8.6458,  61.3783,  66.6256],\n",
       "        [ 33.9838,  75.9714, 160.9826],\n",
       "        [ 94.0394, 114.0688, 179.0417],\n",
       "        ...,\n",
       "        [ 11.7349,  73.5347,  90.7197],\n",
       "        [  1.8258,  23.6940, 117.8154],\n",
       "        [ 89.1233, 178.2168, 198.1309]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ynn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancora una volta possiamo visualizzare i parametri della rete usando la funzione <strong>.parameters()</strong>.<br>\n",
    "L'ordine con cui i parametri sono visualizzati è:<br>\n",
    "layer 1 weight<br>\n",
    "layer 1 bias<br>\n",
    "layer 2 weight<br>\n",
    "layer 2 bias<br>\n",
    "layer 3 weight<br>\n",
    "layer 3 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.9169, -0.7815, -0.4430],\n",
       "         [ 0.3783, -0.2287, -0.7817],\n",
       "         [-0.6621,  0.5824, -0.6628]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.5573, -0.3013, -0.0453], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.5907,  0.8831, -0.0578],\n",
       "         [-0.7613,  0.0973,  0.6890],\n",
       "         [-0.9343,  0.4026, -0.5058]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0814,  0.2346, -0.0710], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.3759, -0.0076,  0.7957],\n",
       "         [ 0.0768,  0.8859,  0.5748],\n",
       "         [-0.8592,  0.2056,  0.5961]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0554, -0.4490, -0.1941], requires_grad=True)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(MyNN.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd: differenziazione automatica "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una cosa che mi da parecchi giramenti di testa nell'uso di Pytorch è la gestione dei gradienti. Vediamo di capirci qualcosa insieme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innanzitutto notiamo che se definiamo un tensore questo non ha nessun attributo require_grad, solamente le sue componenti numeriche. Per esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens1 = torch.Tensor([2,2,3,4,5])\n",
    "tens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo è un tensore contenente esclusivamente dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se voglio che il gradiente sia calcolato rispetto a questo tensore devo dichiararlo esplicitamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 3., 4., 5.], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens1.requires_grad = True\n",
    "#oppure\n",
    "tens1.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso ho il nuovo attributo <strong>requires_grad</strong> per tens1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 3., 4., 5.], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alternativa, avrei potuto definire tutto in un comando. Si noti che devo definire il <strong>dtype</strong> del tensore come float per settare l'attributo requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4,5], dtype=torch.float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oppure posso lasciare che il dtyte sia definito automaticamente, usando la notazione numero puntato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.,2.,3.,4.,5.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lascio che il dtype venga automaticamente settato come integer, ottengo un messaggio di errore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-08fff2ad484a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4,5], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo una ipotetica <strong>funzione errore</strong> tra tens1 e un tensore di riferimento (tens_r):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2000, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens1 = torch.tensor([1,2,3,4,5], dtype=torch.float, requires_grad=True)        #definisco un tensore dei dati\n",
    "tens_r = torch.Tensor([1,1,3,4,5])          #definisco il tensore di riferimento\n",
    "\n",
    "Err_function = torch.mean(tens1 - tens_r)   #definisco la funzione errore come media della differenza tra i due tensori\n",
    "Err_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Della funzione errore sopra definita posso calcolare i gradienti usando la funzione <strong>backward()</strong>. Questa è automaicamente definita da PyTorch.<br>\n",
    "Nel caso in cui la funzione di cui si voglia calcolare il gradiente è uno scalare, la funzione backward() può essere eseguita senza alcun argomento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Err_function.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso posso finalmente vedere il valore del gradiente della funzione errore rispetto ad ognuno dei parametri di tens1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perchè ottengo il valore 0.2 per ogni componente del tensore?\n",
    "\n",
    "Scriviamo il tensore tens1 come $tens_1 = [w_1, w_2, w_3, w_4, w_5]$ e il tensore di riferimento come $tens_r = [r_1, r_2, r_3, r_4, r_5]$\n",
    "\n",
    "\n",
    "Scrivendo la funzione errore come\n",
    "\\begin{equation*}\n",
    "Err\\_function(w_1,w_2,w_3,w_4,w_5) = \\frac{\\sum_{i=1}^5{(w_i - r_i)}}{5} = \\frac{ (w_1 - r_1) + (w_2 - r_2) + (w_3 - r_3) + (w_4 - r_4) + (w_5 - r_5) }{5}\n",
    "\\end{equation*}\n",
    "<br>\n",
    "calcolo adesso le derivate parziali rispetto a $w_i$\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial Err\\_function}{\\partial w_{i}} = \\frac{1}{5} = 0.2 \\hspace{1cm} \\forall i\n",
    "\\end{equation*}\n",
    "<br>\n",
    "Queste derivate parziali sono le componenti del gradiente. E coincidono con il valore calcolato da Pytorch con tens1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da notare ancora che la definizione di autograd come package per la differenziazione automatica si riferisce al fatto che non bisogna specifica quali derivate generare, ma queste vengono calcolate automaticamente usando la funzione backward() una volta definita la funzione e le variabili rispetto alle quali è richiesta la derivata parziale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Nota.</strong> La funzione errore sopra utilizzata, ha il pregio di rendere i calcoli semplici. In realtà come funzione errore non va bene, perchè ciascun contributo $(w_i-r_i)$ può avere segno positivo o negativo. In questo modo un contributo positivo (per esempio +5) e uno negativo (per esempio -5) darebbero un errore netto nullo. Cioè darebbero l'impressione di fittare pefettamente i samples, cosa che ovviamente non è.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riferimenti\n",
    "1. Pytorch.org - [AUTOGRAD: AUTOMATIC DIFFERENTIATION](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
