{"paragraphs":[{"text":"%md\n## SparkContext\nCandidates are expected to know how to use the SparkContext to control basic configuration settings such as spark.sql.shuffle.partitions.","user":"anonymous","dateUpdated":"2019-12-08T23:17:46+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>SparkContext</h2>\n<p>Candidates are expected to know how to use the SparkContext to control basic configuration settings such as spark.sql.shuffle.partitions.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575840608075_-1454541570","id":"20191208-223008_393211527","dateCreated":"2019-12-08T22:30:08+0100","dateStarted":"2019-12-08T23:17:46+0100","dateFinished":"2019-12-08T23:17:52+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1533"},{"text":"spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\r\nspark.conf.set(\"spark.executor.memory\", \"6g\")","user":"anonymous","dateUpdated":"2019-12-12T15:53:41+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575843466230_-286619688","id":"20191208-231746_1391846984","dateCreated":"2019-12-08T23:17:46+0100","dateStarted":"2019-12-12T15:53:42+0100","dateFinished":"2019-12-12T15:53:57+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1534"},{"text":"print(spark.conf.get(\"spark.sql.shuffle.partitions\") + \",\" + spark.conf.get(\"spark.executor.memory\"))","user":"anonymous","dateUpdated":"2019-12-12T15:54:33+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"6,6g"}]},"apps":[],"jobName":"paragraph_1575843481897_-1352670986","id":"20191208-231801_1338398669","dateCreated":"2019-12-08T23:18:01+0100","dateStarted":"2019-12-12T15:54:33+0100","dateFinished":"2019-12-12T15:54:33+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1535"},{"text":"spark.conf.getAll.foreach(println)","user":"anonymous","dateUpdated":"2019-12-12T15:54:49+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(zeppelin.pyspark.python,C:\\Users\\home\\Anaconda3\\envs\\conda2020\\python)\r\n(spark.driver.host,10.0.75.1)\r\n(zeppelin.dep.localrepo,local-repo)\r\n(zeppelin.spark.sql.stacktrace,false)\r\n(spark.driver.port,51529)\r\n(master,local[*])\r\n(spark.repl.class.uri,spark://10.0.75.1:51529/classes)\r\n(zeppelin.spark.useHiveContext,true)\r\n(spark.repl.class.outputDir,C:\\Users\\home\\AppData\\Local\\Temp\\spark384852474003408976)\r\n(zeppelin.spark.sql.interpolation,false)\r\n(zeppelin.spark.importImplicit,true)\r\n(zeppelin.interpreter.output.limit,102400)\r\n(spark.app.name,Zeppelin)\r\n(zeppelin.R.cmd,R)\r\n(zeppelin.spark.maxResult,1000)\r\n(zeppelin.pyspark.useIPython,true)\r\n(zeppelin.spark.concurrentSQL,false)\r\n(zeppelin.spark.enableSupportedVersionCheck,true)\r\n(zeppelin.spark.printREPLOutput,true)\r\n(zeppelin.dep.additionalRemoteRepository,spark-packages,http://dl.bintray.com/spark-packages/maven,false;)\r\n(spark.executor.id,driver)\r\n(zeppelin.spark.useNew,true)\r\n(spark.useHiveContext,true)\r\n(spark.master,local)\r\n(zeppelin.R.image.width,100%)\r\n(zeppelin.spark.ui.hidden,false)\r\n(zeppelin.interpreter.localRepo,C:\\zeppelin-0.8.2-bin-all/local-repo/spark)\r\n(args,deprecation)\r\n(spark.executor.memory,6g)\r\n(spark.driver.allowMultipleContexts,false)\r\n(zeppelin.R.render.options,out.format = 'html', comment = NA, echo = FALSE, results = 'asis', message = F, warning = F, fig.retina = 2)\r\n(zeppelin.interpreter.max.poolsize,10)\r\n(spark.app.id,local-1576162433295)\r\n(zeppelin.R.knitr,true)\r\n(spark.sql.shuffle.partitions,6)\r\n"}]},"apps":[],"jobName":"paragraph_1575843504546_-699539699","id":"20191208-231824_1113669698","dateCreated":"2019-12-08T23:18:24+0100","dateStarted":"2019-12-12T15:54:49+0100","dateFinished":"2019-12-12T15:54:50+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1536"},{"text":"%md\nPrima di definire un nuovo SparkContext devo cancellare quello vecchio","user":"anonymous","dateUpdated":"2019-12-08T23:41:59+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Prima di definire un nuovo SparkContext devo cancellare quello vecchio</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575844875670_1414510794","id":"20191208-234115_293208618","dateCreated":"2019-12-08T23:41:15+0100","dateStarted":"2019-12-08T23:41:59+0100","dateFinished":"2019-12-08T23:41:59+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1537"},{"text":"sc.stop()","user":"anonymous","dateUpdated":"2019-12-12T15:55:09+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575844318442_-2041775792","id":"20191208-233158_1715604013","dateCreated":"2019-12-08T23:31:58+0100","dateStarted":"2019-12-12T15:55:09+0100","dateFinished":"2019-12-12T15:55:09+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1538"},{"text":"%md\nposso usare `spark.conf` oppure `SparkConk` di **org.apache.spark.SparkConf**, i due sono equivalenti","user":"anonymous","dateUpdated":"2019-12-12T15:55:12+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>posso usare <code>spark.conf</code> oppure <code>SparkConk</code> di <strong>org.apache.spark.SparkConf</strong>, i due sono equivalenti</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575845336548_1893062850","id":"20191208-234856_995728250","dateCreated":"2019-12-08T23:48:56+0100","dateStarted":"2019-12-12T15:55:12+0100","dateFinished":"2019-12-12T15:55:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1539"},{"text":"import org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nval conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"provaApp\")\nval sc = new SparkContext(conf)","user":"anonymous","dateUpdated":"2019-12-12T15:55:15+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.SparkConf\r\nimport org.apache.spark.SparkContext\r\nconf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@6f3fc821\r\nsc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@51117f47\n"}]},"apps":[],"jobName":"paragraph_1575843906680_-1369025197","id":"20191208-232506_784248409","dateCreated":"2019-12-08T23:25:06+0100","dateStarted":"2019-12-12T15:55:15+0100","dateFinished":"2019-12-12T15:55:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1540"},{"text":"%md\nThere are two mandatory settings of any Spark application that have to be defined before this Spark application could be run — spark.master and spark.app.name.\n\nParametri (più importanti) dello SparkConf (vedi [link](!https://spark.apache.org/docs/latest/configuration.html) per la lista completa):\n\n| property name  | default   | meaning  |\n|---|---|---|\n| spark.app.name  |  none |  |\n| spark.driver.cores  | 1   | Number of cores to use for the driver process, #only in cluster mode.  |\n| spark.driver.memory  | 1g  |   |\n|spark.executor.memory|1g| |\n|spark.local.dir|/tmp|Directory to use for \"scratch\" space in Spark, including map output files and RDDs that get stored on disk. |\n|spark.master|none|The deploy mode of Spark driver program, either \"client\" or \"cluster\", Which means to launch driver program locally (\"client\") or remotely (\"cluster\") on one of the nodes inside the cluster.|\n\n\n","user":"anonymous","dateUpdated":"2019-12-12T15:57:06+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>There are two mandatory settings of any Spark application that have to be defined before this Spark application could be run — spark.master and spark.app.name.</p>\n<p>Parametri (più importanti) dello SparkConf (vedi <a href=\"!https://spark.apache.org/docs/latest/configuration.html\">link</a> per la lista completa):</p>\n<table>\n  <thead>\n    <tr>\n      <th>property name </th>\n      <th>default </th>\n      <th>meaning </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>spark.app.name </td>\n      <td>none </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>spark.driver.cores </td>\n      <td>1 </td>\n      <td>Number of cores to use for the driver process, #only in cluster mode. </td>\n    </tr>\n    <tr>\n      <td>spark.driver.memory </td>\n      <td>1g </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>spark.executor.memory</td>\n      <td>1g</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>spark.local.dir</td>\n      <td>/tmp</td>\n      <td>Directory to use for &ldquo;scratch&rdquo; space in Spark, including map output files and RDDs that get stored on disk. </td>\n    </tr>\n    <tr>\n      <td>spark.master</td>\n      <td>none</td>\n      <td>The deploy mode of Spark driver program, either &ldquo;client&rdquo; or &ldquo;cluster&rdquo;, Which means to launch driver program locally (&ldquo;client&rdquo;) or remotely (&ldquo;cluster&rdquo;) on one of the nodes inside the cluster.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"jobName":"paragraph_1575844824061_-734867992","id":"20191208-234024_900986684","dateCreated":"2019-12-08T23:40:24+0100","dateStarted":"2019-12-12T15:57:06+0100","dateFinished":"2019-12-12T15:57:06+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1541"},{"text":"conf.toDebugString","user":"anonymous","dateUpdated":"2019-12-12T15:57:46+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res7: String =\r\nspark.app.name=provaApp\r\nspark.driver.allowMultipleContexts=false\r\nspark.master=local[*]\n"}]},"apps":[],"jobName":"paragraph_1575845302170_-741980385","id":"20191208-234822_1356753041","dateCreated":"2019-12-08T23:48:22+0100","dateStarted":"2019-12-12T15:57:46+0100","dateFinished":"2019-12-12T15:57:46+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1542"},{"text":"%sql\r\nSET spark.sql.shuffle.partitions = 2;\r\nSET spark.executor.memory = 1g;","user":"anonymous","dateUpdated":"2019-12-12T15:58:13+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"spark.sql.shuffle.partitions should be int, but was 2;\r\nSET spark.executor.memory = 1g;\nset zeppelin.spark.sql.stacktrace = true to see full stacktrace"}]},"apps":[],"jobName":"paragraph_1575846843542_1897471542","id":"20191209-001403_19931058","dateCreated":"2019-12-09T00:14:03+0100","dateStarted":"2019-12-12T15:58:13+0100","dateFinished":"2019-12-12T15:58:13+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:1543"},{"text":"print(spark.conf.get(\"spark.sql.shuffle.partitions\") + \",\" + spark.conf.get(\"spark.executor.memory\"))","user":"anonymous","dateUpdated":"2019-12-12T15:59:42+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"6,6g"}]},"apps":[],"jobName":"paragraph_1575849216221_-833660391","id":"20191209-005336_1737210193","dateCreated":"2019-12-09T00:53:36+0100","dateStarted":"2019-12-12T15:59:42+0100","dateFinished":"2019-12-12T15:59:42+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1544"},{"text":"%md\n## SparkSession\nCandidates are expected to know how to:\n- Create a DataFrame/Dataset from a collection (e.g. list or set)","user":"anonymous","dateUpdated":"2019-12-12T16:02:32+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>SparkSession</h2>\n<p>Candidates are expected to know how to:<br/>- Create a DataFrame/Dataset from a collection (e.g. list or set)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575849278391_-1575998593","id":"20191209-005438_844637410","dateCreated":"2019-12-09T00:54:38+0100","dateStarted":"2019-12-12T16:02:32+0100","dateFinished":"2019-12-12T16:02:32+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1545"},{"text":"%md\nriguardo la creazione di DataFrame vedere questo [articolo](!https://medium.com/@mrpowers/manually-creating-spark-dataframes-b14dae906393)","user":"anonymous","dateUpdated":"2019-12-09T15:09:39+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>riguardo la creazione di DataFrame vedere questo <a href=\"!https://medium.com/@mrpowers/manually-creating-spark-dataframes-b14dae906393\">articolo</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575900546889_-1074393814","id":"20191209-150906_329859745","dateCreated":"2019-12-09T15:09:06+0100","dateStarted":"2019-12-09T15:09:39+0100","dateFinished":"2019-12-09T15:09:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1546"},{"text":"%md\n### Creazione di un dataframe - Metodo 1\nPer creare un dataframe da una lista di valori si può usare il metodo **toDF**.\nIn alcuni casi occorre importare spark.implicits._ per poter fare la conversione da lista a datafame.","user":"anonymous","dateUpdated":"2019-12-19T11:02:23+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creazione di un dataframe - Metodo 1</h3>\n<p>Per creare un dataframe da una lista di valori si può usare il metodo <strong>toDF</strong>.<br/>In alcuni casi occorre importare spark.implicits._ per poter fare la conversione da lista a datafame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575900154389_557036731","id":"20191209-150234_1543111474","dateCreated":"2019-12-09T15:02:34+0100","dateStarted":"2019-12-19T11:02:23+0100","dateFinished":"2019-12-19T11:02:25+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1547"},{"text":"import spark.implicits._\n\nval lista_numeri = List(1,2,3,4,5,6,7)\nval list_df = lista_numeri.toDF()","user":"anonymous","dateUpdated":"2019-12-12T17:50:40+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\r\nlista_numeri: List[Int] = List(1, 2, 3, 4, 5, 6, 7)\r\nlist_df: org.apache.spark.sql.DataFrame = [value: int]\n"}]},"apps":[],"jobName":"paragraph_1575899542625_-2071219905","id":"20191209-145222_6272954","dateCreated":"2019-12-09T14:52:22+0100","dateStarted":"2019-12-12T17:50:35+0100","dateFinished":"2019-12-12T17:50:36+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1548"},{"text":"spark.version","user":"anonymous","dateUpdated":"2019-12-12T17:27:37+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res13: String = 2.2.1\n"}]},"apps":[],"jobName":"paragraph_1575902308387_-806784717","id":"20191209-153828_1991109153","dateCreated":"2019-12-09T15:38:28+0100","dateStarted":"2019-12-12T17:27:37+0100","dateFinished":"2019-12-12T17:27:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1549"},{"text":"%md\n### Creazione di un dataframe - Metodo 2\nUso il metodo di [Sparksession](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession) **createDataFrame()** definito come:\n`def createDataFrame(rows: rdd[Row], schema: StructType): DataFrame`\nIl metodo necessita due parametri: un **RDD** di **Row** e uno schema definito con **StructType**\nLo schema è definito così:\n`StructType( \n    List ( StructField(\"name1\", TypeName1), StructField(\"name2\", TypeName2) ...) \n)`","user":"anonymous","dateUpdated":"2019-12-19T11:02:30+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creazione di un dataframe - Metodo 2</h3>\n<p>Uso il metodo di <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession\">Sparksession</a> <strong>createDataFrame()</strong> definito come:<br/><code>def createDataFrame(rows: rdd[Row], schema: StructType): DataFrame</code><br/>Il metodo necessita due parametri: un <strong>RDD</strong> di <strong>Row</strong> e uno schema definito con <strong>StructType</strong><br/>Lo schema è definito così:<br/><code>StructType( \n    List ( StructField(&quot;name1&quot;, TypeName1), StructField(&quot;name2&quot;, TypeName2) ...) \n)</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575901307576_-664743819","id":"20191209-152147_799141739","dateCreated":"2019-12-09T15:21:47+0100","dateStarted":"2019-12-19T11:02:30+0100","dateFinished":"2019-12-19T11:02:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1550"},{"text":"import org.apache.spark.sql.Row\r\nimport org.apache.spark.sql.types.{StructField, StructType, IntegerType, StringType}\r\n\r\n//creiamo un dataset\r\nval someData = sc.parallelize( Seq(\r\n                                    Row(8, \"bat\"),\r\n                                    Row(64, \"mouse\"),\r\n                                    Row(-27, \"horse\")\r\n                                  )\r\n                             )\r\n\r\n//definiamo lo schema\r\nval someSchema = StructType ( List(\r\n                                    StructField(\"number\", IntegerType, true),\r\n                                    StructField(\"word\", StringType, true)\r\n                                    )\r\n                            )\r\n\r\n//dobbiamo dare come parametri un RDD (creato con sc.parallelize) e lo schema\r\nval someDF = spark.createDataFrame( someData, someSchema )","user":"anonymous","dateUpdated":"2019-12-12T17:46:20+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.Row\r\nimport org.apache.spark.sql.types.{StructField, StructType, IntegerType, StringType}\r\nsomeData: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[0] at parallelize at <console>:37\r\nsomeSchema: org.apache.spark.sql.types.StructType = StructType(StructField(number,IntegerType,true), StructField(word,StringType,true))\r\nsomeDF: org.apache.spark.sql.DataFrame = [number: int, word: string]\n"}]},"apps":[],"jobName":"paragraph_1575900383771_90485770","id":"20191209-150623_1701344785","dateCreated":"2019-12-09T15:06:23+0100","dateStarted":"2019-12-12T17:45:10+0100","dateFinished":"2019-12-12T17:45:11+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1551"},{"text":"%md\nUn altro modo di usare  **StructType** è il seguente:","user":"anonymous","dateUpdated":"2019-12-12T18:12:41+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Un altro modo di usare <strong>StructType</strong> è il seguente:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576169689710_-613879637","id":"20191212-175449_1322998139","dateCreated":"2019-12-12T17:54:49+0100","dateStarted":"2019-12-12T18:12:41+0100","dateFinished":"2019-12-12T18:12:41+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1552"},{"text":"import org.apache.spark.sql.types.StructType\r\n\r\nval schema = new StructType()\r\n  .add($\"id\".long.copy(nullable = false))\r\n  .add($\"city\".string)\r\n  .add($\"country\".string)\r\n\r\nschema.printTreeString\r\n\r\nimport org.apache.spark.sql.DataFrameReader\r\n\r\nval r: DataFrameReader = spark.read.schema(schema)","user":"anonymous","dateUpdated":"2019-12-09T17:42:47+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- id: long (nullable = false)\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n\r\nimport org.apache.spark.sql.types.StructType\r\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,false), StructField(city,StringType,true), StructField(country,StringType,true))\r\nimport org.apache.spark.sql.DataFrameReader\r\nr: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@140fe2cd\r\nres44: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@140fe2cd\n"}]},"apps":[],"jobName":"paragraph_1575908601127_309163795","id":"20191209-172321_1676365353","dateCreated":"2019-12-09T17:23:21+0100","dateStarted":"2019-12-09T17:42:30+0100","dateFinished":"2019-12-09T17:42:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1553"},{"text":"%md\n-- creare un dataframe da un Map\n\nIn questo caso lo schema può essere settato automaticamente da Spark.\n\nAnche con un dato di partenza in formato **Map** lo schema è sempre:\nMap -> parallelize -> RDD -> DataFrame","user":"anonymous","dateUpdated":"2019-12-12T18:00:52+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; creare un dataframe da un Map</p>\n<p>In questo caso lo schema può essere settato automaticamente da Spark.</p>\n<p>Anche con un dato di partenza in formato <strong>Map</strong> lo schema è sempre:<br/>Map -&gt; parallelize -&gt; RDD -&gt; DataFrame</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575904300804_757742814","id":"20191209-161140_1190648093","dateCreated":"2019-12-09T16:11:40+0100","dateStarted":"2019-12-12T18:00:52+0100","dateFinished":"2019-12-12T18:00:52+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1554"},{"text":"val df = spark.createDataFrame( sc.parallelize( Map((\"x\", 24), (\"y\", 25), (\"z\", 26)).toSeq ) )\ndf.withColumnRenamed(\"_1\", \"nome\").withColumnRenamed(\"_2\", \"età\").show()","user":"anonymous","dateUpdated":"2019-12-12T17:56:26+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+---+\n|nome|età|\n+----+---+\n|   x| 24|\n|   y| 25|\n|   z| 26|\n+----+---+\n\r\ndf: org.apache.spark.sql.DataFrame = [_1: string, _2: int]\n"}]},"apps":[],"jobName":"paragraph_1575902831921_-1934190881","id":"20191209-154711_1751171205","dateCreated":"2019-12-09T15:47:11+0100","dateStarted":"2019-12-09T16:48:33+0100","dateFinished":"2019-12-09T16:48:33+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1555"},{"text":"%md\n- Create a DataFrame for a range of numbers\n","user":"anonymous","dateUpdated":"2019-12-09T16:43:14+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Create a DataFrame for a range of numbers</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575904038036_1544157178","id":"20191209-160718_634751735","dateCreated":"2019-12-09T16:07:18+0100","dateStarted":"2019-12-09T16:43:14+0100","dateFinished":"2019-12-09T16:43:14+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1556"},{"text":"//anche in questo caso si può usare toDF\nsc.parallelize(1 to 10).toDF()","user":"anonymous","dateUpdated":"2019-12-09T17:11:39+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res16: org.apache.spark.sql.DataFrame = [value: int]\n"}]},"apps":[],"jobName":"paragraph_1575906379442_1420602994","id":"20191209-164619_759620927","dateCreated":"2019-12-09T16:46:19+0100","dateStarted":"2019-12-09T16:48:07+0100","dateFinished":"2019-12-09T16:48:07+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1557"},{"text":"//oppure senza passare do un rdd \nimport org.apache.spark.sql.DataFrame\nval ar = 1 to 10\nval df: DataFrame = ar.toDF()","user":"anonymous","dateUpdated":"2019-12-09T17:51:21+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.DataFrame\r\nar: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\r\ndf: org.apache.spark.sql.DataFrame = [value: int]\n"}]},"apps":[],"jobName":"paragraph_1575910062662_1575645885","id":"20191209-174742_65377942","dateCreated":"2019-12-09T17:47:42+0100","dateStarted":"2019-12-09T17:50:45+0100","dateFinished":"2019-12-09T17:50:45+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1558"},{"text":"%md\r\n- Access the DataFrameReaders\r\n","user":"anonymous","dateUpdated":"2019-12-09T17:21:44+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Access the DataFrameReaders</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575908491949_-815642632","id":"20191209-172131_84593321","dateCreated":"2019-12-09T17:21:31+0100","dateStarted":"2019-12-09T17:21:44+0100","dateFinished":"2019-12-09T17:21:44+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1559"},{"text":"val df = spark.read.csv(\"exampl1.csv\")","user":"anonymous","dateUpdated":"2019-12-09T17:23:15+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]\n"}]},"apps":[],"jobName":"paragraph_1575908507408_1287173059","id":"20191209-172147_35366691","dateCreated":"2019-12-09T17:21:47+0100","dateStarted":"2019-12-09T17:23:15+0100","dateFinished":"2019-12-09T17:23:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1560"},{"text":"%md\nVediamo alcuni esempi presi da [The Internal of Spark SQL](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataFrameReader.html)","user":"anonymous","dateUpdated":"2019-12-12T18:10:50+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Vediamo alcuni esempi presi da <a href=\"https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataFrameReader.html\">The Internal of Spark SQL</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575909657365_-515893458","id":"20191209-174057_1442239035","dateCreated":"2019-12-09T17:40:57+0100","dateStarted":"2019-12-12T18:10:50+0100","dateFinished":"2019-12-12T18:10:50+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1561"},{"text":"val csvLine = \"0,Warsaw,Poland\"\r\n\r\nimport org.apache.spark.sql.Dataset\r\nval cities: Dataset[String] = Seq(csvLine).toDS\r\n\r\ncities.show\r\n\r\n// Define schema explicitly (as below)\r\n// or\r\n// option(\"header\", true) + option(\"inferSchema\", true)\r\nimport org.apache.spark.sql.types.StructType\r\nval schema = new StructType()\r\n  .add($\"id\".long.copy(nullable = false))\r\n  .add($\"city\".string)\r\n  .add($\"country\".string)\r\n\r\nschema.printTreeString\r\n\r\nimport org.apache.spark.sql.DataFrame\r\n\r\nval citiesDF: DataFrame = spark\r\n  .read\r\n  .schema(schema)\r\n  .csv(cities)\r\n  \r\ncitiesDF.show","user":"anonymous","dateUpdated":"2019-12-09T17:37:59+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------+\n|          value|\n+---------------+\n|0,Warsaw,Poland|\n+---------------+\n\r\nroot\n |-- id: long (nullable = false)\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n\r\n+---+------+-------+\n| id|  city|country|\n+---+------+-------+\n|  0|Warsaw| Poland|\n+---+------+-------+\n\r\ncsvLine: String = 0,Warsaw,Poland\r\nimport org.apache.spark.sql.Dataset\r\ncities: org.apache.spark.sql.Dataset[String] = [value: string]\r\nimport org.apache.spark.sql.types.StructType\r\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,false), StructField(city,StringType,true), StructField(country,StringType,true))\r\nimport org.apache.spark.sql.DataFrame\r\ncitiesDF: org.apache.spark.sql.DataFrame = [id: bigint, city: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1575909412345_-1490276277","id":"20191209-173652_451601145","dateCreated":"2019-12-09T17:36:52+0100","dateStarted":"2019-12-09T17:37:59+0100","dateFinished":"2019-12-09T17:37:59+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1562"},{"text":"%md\n- Register User Defined Functions (UDFs)","user":"anonymous","dateUpdated":"2019-12-09T17:53:15+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Register User Defined Functions (UDFs)</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575906109867_-1893085455","id":"20191209-164149_1780866584","dateCreated":"2019-12-09T16:41:49+0100","dateStarted":"2019-12-09T17:53:15+0100","dateFinished":"2019-12-09T17:53:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1563"},{"text":"%md\nDobbiamo registrare una funzione da noi definita (UDF), che potremo poi usare per operare sui dataframe o in una espressione SQL.\n\nUsiamo il metodo **udf()** con parametro la funzione da registrare. Nel nostro caso la funzione da registrare è:\n`x(Double) => {x*x*x}`","user":"anonymous","dateUpdated":"2019-12-12T18:27:47+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Dobbiamo registrare una funzione da noi definita (UDF), che potremo poi usare per operare sui dataframe o in una espressione SQL.</p>\n<p>Usiamo il metodo <strong>udf()</strong> con parametro la funzione da registrare. Nel nostro caso la funzione da registrare è:<br/><code>x(Double) =&gt; {x*x*x}</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575906108831_716586280","id":"20191209-164148_1095261788","dateCreated":"2019-12-09T16:41:48+0100","dateStarted":"2019-12-12T18:27:47+0100","dateFinished":"2019-12-12T18:27:47+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1564"},{"text":"//importiamo udf\nimport org.apache.spark.sql.functions.udf\n\nval power3udf = udf( (x:Double) => { x*x*x } )\n\nval df = sc.parallelize(0 to 10 by 2).toDF()\n\ndf.show()\n\ndf.select(power3udf( $\"value\" )).show()","user":"admin","dateUpdated":"2019-12-12T18:40:57+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+\n|value|\n+-----+\n|    0|\n|    2|\n|    4|\n|    6|\n|    8|\n|   10|\n+-----+\n\r\n+----------+\n|UDF(value)|\n+----------+\n|       0.0|\n|       8.0|\n|      64.0|\n|     216.0|\n|     512.0|\n|    1000.0|\n+----------+\n\r\nimport org.apache.spark.sql.functions.udf\r\npower3udf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(DoubleType)))\r\ndf: org.apache.spark.sql.DataFrame = [value: int]\n"}]},"apps":[],"jobName":"paragraph_1575906104939_842648445","id":"20191209-164144_1827773364","dateCreated":"2019-12-09T16:41:44+0100","dateStarted":"2019-12-12T18:40:57+0100","dateFinished":"2019-12-12T18:41:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1565"},{"text":"%md\nDi seguito un altro esempio di registrazione di una UDF, l'esempio è preso dall'articolo di Medium  [Spark User Defined Functions (UDFs)](https://medium.com/@mrpowers/spark-user-defined-functions-udfs-6c849e39443b).\nNell'esempio ho una funzione che converte le stringhe di input in minuscolo e rimuove tutti gli spazi.","user":"admin","dateUpdated":"2019-12-12T18:51:54+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Di seguito un altro esempio di registrazione di una UDF, l&rsquo;esempio è preso dall&rsquo;articolo di Medium <a href=\"https://medium.com/@mrpowers/spark-user-defined-functions-udfs-6c849e39443b\">Spark User Defined Functions (UDFs)</a>.<br/>Nell&rsquo;esempio ho una funzione che converte le stringhe di input in minuscolo e rimuove tutti gli spazi.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576172574999_-1894284742","id":"20191212-184254_596233998","dateCreated":"2019-12-12T18:42:54+0100","dateStarted":"2019-12-12T18:51:54+0100","dateFinished":"2019-12-12T18:51:54+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1566"},{"text":"import org.apache.spark.sql.types.StringType\r\n\r\ndef lowerRemoveAllWhitespace(s: String): String = {\r\n  s.toLowerCase().replaceAll(\"\\\\s\", \"\")\r\n}\r\n\r\nval lowerRemoveAllWhitespaceUDF = udf[String, String](lowerRemoveAllWhitespace)\r\n\r\nval sourceDF = List((\"  HI THERE     \"),\r\n                    (\" GivE mE PresenTS     \")\r\n                   ).toDF()\r\n\r\nsourceDF.show()\r\n\r\nsourceDF.select(\r\n  lowerRemoveAllWhitespaceUDF(col(\"value\")).as(\"clean_value\")\r\n).show()","user":"admin","dateUpdated":"2019-12-12T18:52:09+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+\n|               value|\n+--------------------+\n|       HI THERE     |\n| GivE mE PresenTS...|\n+--------------------+\n\r\n+--------------+\n|   clean_value|\n+--------------+\n|       hithere|\n|givemepresents|\n+--------------+\n\r\nimport org.apache.spark.sql.types.StringType\r\nlowerRemoveAllWhitespace: (s: String)String\r\nlowerRemoveAllWhitespaceUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\r\nsourceDF: org.apache.spark.sql.DataFrame = [value: string]\n"}]},"apps":[],"jobName":"paragraph_1575917399651_977110280","id":"20191209-194959_1831441693","dateCreated":"2019-12-09T19:49:59+0100","dateStarted":"2019-12-12T18:52:09+0100","dateFinished":"2019-12-12T18:52:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1567"},{"text":"%md\nLa funzione è registrata in dataframe. Non ci resta che registrarla anche in SQL.","user":"admin","dateUpdated":"2019-12-12T18:55:15+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>La funzione è registrata in dataframe. Non ci resta che registrarla anche in SQL.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576173246989_773163977","id":"20191212-185406_539607392","dateCreated":"2019-12-12T18:54:06+0100","dateStarted":"2019-12-12T18:55:15+0100","dateFinished":"2019-12-12T18:55:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1568"},{"text":"//registare la funzione in SQL\r\nval df = (1 to 100 by 3).toDF(\"num\")\r\ndf.show(5)\r\nspark.udf.register(\"power2\", (x: Double) => x*x)\r\n//adesso che la funzione è registrata posso usarla in una query sql, nel formato Scala\r\ndf.selectExpr(\"power2(num)\").show(5)\r\n//posso usare la funzione anche nel formato SQL\r\n//prima creando una view temporanea\r\ndf.createOrReplaceTempView(\"udfExampleSQL3\")\r\nspark.sql(\"SELECT power3(num) AS p2 FROM udfExampleSQL3\").show(5)","user":"admin","dateUpdated":"2019-12-12T19:06:37+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+\n|num|\n+---+\n|  1|\n|  4|\n|  7|\n| 10|\n| 13|\n+---+\nonly showing top 5 rows\n\r\n+-------------------------------+\n|UDF:power2(cast(num as double))|\n+-------------------------------+\n|                            1.0|\n|                           16.0|\n|                           49.0|\n|                          100.0|\n|                          169.0|\n+-------------------------------+\nonly showing top 5 rows\n\r\n+-----+\n|   p2|\n+-----+\n|  1.0|\n| 16.0|\n| 49.0|\n|100.0|\n|169.0|\n+-----+\nonly showing top 5 rows\n\r\ndf: org.apache.spark.sql.DataFrame = [num: int]\n"}]},"apps":[],"jobName":"paragraph_1575912535969_-1502572621","id":"20191209-182855_435960561","dateCreated":"2019-12-09T18:28:55+0100","dateStarted":"2019-12-12T19:04:38+0100","dateFinished":"2019-12-12T19:04:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1569"},{"text":"%md\r\n## DataFrameReader\r\n\r\n### Read data for the \"core\" data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)","user":"admin","dateUpdated":"2019-12-12T19:11:47+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrameReader</h2>\n<h3>Read data for the &ldquo;core&rdquo; data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575926947456_-1243108255","id":"20191209-222907_932594899","dateCreated":"2019-12-09T22:29:07+0100","dateStarted":"2019-12-12T19:11:47+0100","dateFinished":"2019-12-12T19:11:47+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1570"},{"text":"val dataFrame = spark.read.json(\"example.json\") \r\nval dataFrame = spark.read.csv(\"example.csv\") \r\nval dataFrame = spark.read.parquet(\"example.parquet\")\r\nval dataFrame = spark.read.jdbc(url,\"person\",prop)\r\nval dataFrame = spark.read.text(\"file.txt\")","user":"admin","dateUpdated":"2019-12-10T09:55:11+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575929693834_-1523636789","id":"20191209-231453_154806882","dateCreated":"2019-12-09T23:14:53+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1571"},{"text":"%md\nUna sintassi equivalente per importare i file è la seguente:","user":"admin","dateUpdated":"2019-12-12T19:07:53+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Una sintassi equivalente per importare i file è la seguente:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576174028864_179431335","id":"20191212-190708_238350304","dateCreated":"2019-12-12T19:07:08+0100","dateStarted":"2019-12-12T19:07:53+0100","dateFinished":"2019-12-12T19:07:53+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1572"},{"text":"spark.read.format(\"csv\").load(\"example.csv\")","user":"anonymous","dateUpdated":"2019-12-12T19:07:06+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res10: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]\n"}]},"apps":[],"jobName":"paragraph_1575968915934_-124525370","id":"20191210-100835_1246440689","dateCreated":"2019-12-10T10:08:35+0100","dateStarted":"2019-12-10T10:09:16+0100","dateFinished":"2019-12-10T10:09:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1573"},{"text":"%md\r\n### How to configure options for specific formats\r\n","user":"admin","dateUpdated":"2019-12-12T19:11:57+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"databaseName":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to configure options for specific formats</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575927262915_1309406287","id":"20191209-223422_742724847","dateCreated":"2019-12-09T22:34:22+0100","dateStarted":"2019-12-12T19:11:57+0100","dateFinished":"2019-12-12T19:11:57+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1574"},{"text":"%md\nDevo usare il metodo **option(\"parametro\", \"valore\")**, mettendo anche più option in cascata.","user":"admin","dateUpdated":"2019-12-12T19:11:16+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Devo usare il metodo <strong>option(&ldquo;parametro&rdquo;, &ldquo;valore&rdquo;)</strong>, mettendo anche più option in cascata.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576174228562_-1715279243","id":"20191212-191028_108858606","dateCreated":"2019-12-12T19:10:28+0100","dateStarted":"2019-12-12T19:11:16+0100","dateFinished":"2019-12-12T19:11:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1575"},{"text":"val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"exampl1.csv\")\n//or \nval df2 = spark.read.options(Map((\"header\", \"true\"), (\"inferSchema\",\"true\"))).csv(\"exampl1.csv\")\n//or\nval df3 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"mode\", \"FAILFAST\").load(\"exampl1.csv\")","user":"anonymous","dateUpdated":"2019-12-10T10:12:20+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [eta: int, amici: int]\r\ndf2: org.apache.spark.sql.DataFrame = [eta: int, amici: int]\r\ndf3: org.apache.spark.sql.DataFrame = [eta: int, amici: int]\n"}]},"apps":[],"jobName":"paragraph_1575929691444_-1388387565","id":"20191209-231451_1479988188","dateCreated":"2019-12-09T23:14:51+0100","dateStarted":"2019-12-10T10:12:20+0100","dateFinished":"2019-12-10T10:12:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1576"},{"text":"%md\nPer vedere le opzioni disponibili per i metodi di [DataFrameReader](https://spark.apache.org/docs/2.2.1/api/scala/#org.apache.spark.sql.DataFrameReader), ad esempio csv, si può fare riferimento alla documentazione della API.\nAd esempio per conoscere tutte le opzioni disponibili per csv si può clikkare sulla freccia per espandere il campo csv (vedere la figura sotto).\n![figura](https://www.1week4.com/wp-content/uploads/2019/12/spark-api-dataframereader-options.jpg)","user":"admin","dateUpdated":"2019-12-12T19:09:38+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Per vedere le opzioni disponibili per i metodi di <a href=\"https://spark.apache.org/docs/2.2.1/api/scala/#org.apache.spark.sql.DataFrameReader\">DataFrameReader</a>, ad esempio csv, si può fare riferimento alla documentazione della API.<br/>Ad esempio per conoscere tutte le opzioni disponibili per csv si può clikkare sulla freccia per espandere il campo csv (vedere la figura sotto).<br/><img src=\"https://www.1week4.com/wp-content/uploads/2019/12/spark-api-dataframereader-options.jpg\" alt=\"figura\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575931195975_-81848269","id":"20191209-233955_1825649355","dateCreated":"2019-12-09T23:39:55+0100","dateStarted":"2019-12-12T19:09:38+0100","dateFinished":"2019-12-12T19:09:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1577"},{"text":"%md\nAlcune delle opzioni sono per l'importazione dei file csv sono:\n- header\n- inferSchema\n- mode\n- ...","user":"anonymous","dateUpdated":"2019-12-10T11:12:42+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Alcune delle opzioni sono per l&rsquo;importazione dei file csv sono:<br/>- header<br/>- inferSchema<br/>- mode<br/>- &hellip;</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575926647615_-843164516","id":"20191209-222407_39639960","dateCreated":"2019-12-09T22:24:07+0100","dateStarted":"2019-12-10T11:12:42+0100","dateFinished":"2019-12-10T11:12:42+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1578"},{"text":"%md\n### How to read data from non-core formats using format() and load()","user":"admin","dateUpdated":"2019-12-12T19:12:08+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to read data from non-core formats using format() and load()</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575928614828_-796565143","id":"20191209-225654_1498953618","dateCreated":"2019-12-09T22:56:54+0100","dateStarted":"2019-12-12T19:12:08+0100","dateFinished":"2019-12-12T19:12:08+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1579"},{"text":"%md\nSupponiamo di avere un file in cui i dati sono salvati seguendo un formato particolare, per esempio:\n- il separatore è il segno meno **-** invece della virgola **,**\n- nel file ci sono linee di commenti che iniziano con **<!--**\n- il file è indentato per cui ci sono spazi vuoti all'inizio delle righe\n- etc.\n\nper ognuno di questi casi particolari trovo una opzione di **load**.","user":"anonymous","dateUpdated":"2019-12-10T11:28:05+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Supponiamo di avere un file in cui i dati sono salvati seguendo un formato particolare, per esempio:<br/>- il separatore è il segno meno <strong>-</strong> invece della virgola <strong>,</strong><br/>- nel file ci sono linee di commenti che iniziano con <strong>&lt;!&ndash;</strong><br/>- il file è indentato per cui ci sono spazi vuoti all&rsquo;inizio delle righe<br/>- etc.</p>\n<p>per ognuno di questi casi particolari trovo una opzione di <strong>load</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575973368689_47263036","id":"20191210-112248_463581520","dateCreated":"2019-12-10T11:22:48+0100","dateStarted":"2019-12-10T11:28:05+0100","dateFinished":"2019-12-10T11:28:05+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1580"},{"text":"val df = spark.read.format(\"txt\").option(\"sep\",\"-\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").option(\"ignoreLeadingWhiteSpace\",\"true\").option(\"comment\", \"<!--\").load(data_file)","user":"anonymous","dateUpdated":"2019-12-10T11:27:01+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575972833639_133509361","id":"20191210-111353_98572472","dateCreated":"2019-12-10T11:13:53+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1581"},{"text":"%md\r\n### How to construct and specify a schema using the StructType classes\r\n","user":"admin","dateUpdated":"2019-12-12T19:12:18+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to construct and specify a schema using the StructType classes</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575972832766_210402550","id":"20191210-111352_766045483","dateCreated":"2019-12-10T11:13:52+0100","dateStarted":"2019-12-12T19:12:18+0100","dateFinished":"2019-12-12T19:12:18+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1582"},{"text":"import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType}\n\nval my_schema = StructType( List(\n                                StructField(\"nome\", StringType),\n                                StructField(\"cognome\", StringType, nullable=true),\n                                StructField(\"altezza\", IntegerType, nullable=true), \n                                StructField(\"peso\", FloatType, nullable=true),\n                                StructField(\"età\", IntegerType)\n                                )\n                          )\n\nval df = spark.read.format(\"csv\").schema(my_schema).option(\"mode\",\"PERMISSIVE\").load(\"exampl2.csv\")\ndf.show()","user":"anonymous","dateUpdated":"2019-12-10T11:50:05+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+---------+-------+----+---+\n|    nome|  cognome|altezza|peso|età|\n+--------+---------+-------+----+---+\n|   Pippo|  Paolini|    167|72.3| 44|\n|   Luigi|    Russo|    178|89.2|  5|\n|Giovanna|    Rosso|    175|80.0| 44|\n|Giuseppe|  Bianchi|    165|75.8|  9|\n|  Amedeo|    Verdi|    167|56.2| 18|\n|   Luisa|Valentino|    182|64.9| 15|\n+--------+---------+-------+----+---+\n\r\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType}\r\nmy_schema: org.apache.spark.sql.types.StructType = StructType(StructField(nome,StringType,true), StructField(cognome,StringType,true), StructField(altezza,IntegerType,true), StructField(peso,FloatType,true), StructField(età,IntegerType,true))\r\ndf: org.apache.spark.sql.DataFrame = [nome: string, cognome: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1575972831940_75517357","id":"20191210-111351_1574884426","dateCreated":"2019-12-10T11:13:51+0100","dateStarted":"2019-12-10T11:50:05+0100","dateFinished":"2019-12-10T11:50:05+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1583"},{"text":"%md\n### How to specify a DDL-formatted schema","user":"admin","dateUpdated":"2019-12-12T19:12:36+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to specify a DDL-formatted schema</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575972831076_1219739733","id":"20191210-111351_248682753","dateCreated":"2019-12-10T11:13:51+0100","dateStarted":"2019-12-12T19:12:36+0100","dateFinished":"2019-12-12T19:12:36+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1584"},{"text":"%md\nil metodo **schema** del DataFrameReader consente di specificare o schema dei dati usando un formato DDL. Esempio:\n`spark.read.schema(\"a INT, b STRING, c DOUBLE\").csv(\"test.csv\")`","user":"anonymous","dateUpdated":"2019-12-10T13:01:49+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>il metodo <strong>schema</strong> del DataFrameReader consente di specificare o schema dei dati usando un formato DDL. Esempio:<br/><code>spark.read.schema(&quot;a INT, b STRING, c DOUBLE&quot;).csv(&quot;test.csv&quot;)</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575979215236_664173509","id":"20191210-130015_1610535786","dateCreated":"2019-12-10T13:00:15+0100","dateStarted":"2019-12-10T13:01:49+0100","dateFinished":"2019-12-10T13:01:49+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1585"},{"text":"//purtroppo anche l'esempio della API non funziona nella mia versione di Spark\n//l'errore dice che si aspetta uno StructType invece di una stringa\nspark.read.schema(\"a INT, b STRING, c DOUBLE\").csv(\"test.csv\")","user":"anonymous","dateUpdated":"2019-12-10T13:07:51+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:31: error: type mismatch;\r\n found   : String(\"a INT, b STRING, c DOUBLE\")\r\n required: org.apache.spark.sql.types.StructType\r\n       spark.read.schema(\"a INT, b STRING, c DOUBLE\").csv(\"test.csv\")\r\n                         ^\n"}]},"apps":[],"jobName":"paragraph_1575979564660_-191537312","id":"20191210-130604_1116103219","dateCreated":"2019-12-10T13:06:04+0100","dateStarted":"2019-12-10T13:06:08+0100","dateFinished":"2019-12-10T13:06:08+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:1586"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576003213424_-323663687","id":"20191210-194013_1256688542","dateCreated":"2019-12-10T19:40:13+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1587"},{"text":"%md\r\n## DataFrameWriter\r\n\r\n### Write data to the \"core\" data formats (csv, json, jdbc, orc, parquet, text and tables)","user":"admin","dateUpdated":"2019-12-12T19:12:52+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrameWriter</h2>\n<h3>Write data to the &ldquo;core&rdquo; data formats (csv, json, jdbc, orc, parquet, text and tables)</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575980609416_-613860425","id":"20191210-132329_1436810178","dateCreated":"2019-12-10T13:23:29+0100","dateStarted":"2019-12-12T19:12:52+0100","dateFinished":"2019-12-12T19:12:52+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1588"},{"text":"df.write.parquet(\"myparquetfile\")\r\ndf.write.saveAsTable(\"mytable\")\r\ndf.write.csv(\"data\")","user":"anonymous","dateUpdated":"2019-12-12T19:13:29+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575980608757_-1841819175","id":"20191210-132328_1299705672","dateCreated":"2019-12-10T13:23:28+0100","dateStarted":"2019-12-10T13:25:15+0100","dateFinished":"2019-12-10T13:25:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1589"},{"text":"df.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"/my-tsv-file.tsv\")","user":"anonymous","dateUpdated":"2019-12-10T14:35:49+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575980608445_-966301830","id":"20191210-132328_552244365","dateCreated":"2019-12-10T13:23:28+0100","dateStarted":"2019-12-10T14:35:49+0100","dateFinished":"2019-12-10T14:35:49+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1590"},{"text":"df.write.csv(\"data.csv\")","user":"anonymous","dateUpdated":"2019-12-10T14:33:31+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575980607869_-1520345816","id":"20191210-132327_190454110","dateCreated":"2019-12-10T13:23:27+0100","dateStarted":"2019-12-10T14:33:31+0100","dateFinished":"2019-12-10T14:33:31+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1591"},{"text":"%md\n### Overwriting existing files","user":"admin","dateUpdated":"2019-12-12T19:13:06+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Overwriting existing files</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575972830189_1970112386","id":"20191210-111350_872219591","dateCreated":"2019-12-10T11:13:50+0100","dateStarted":"2019-12-12T19:13:06+0100","dateFinished":"2019-12-12T19:13:06+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1592"},{"text":"df.write.mode(\"overwrite\").csv(\"data.csv\")","user":"anonymous","dateUpdated":"2019-12-12T19:13:18+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575985003310_810766751","id":"20191210-143643_83693407","dateCreated":"2019-12-10T14:36:43+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1593"},{"text":"%md\n### How to configure options for specific formats","user":"admin","dateUpdated":"2019-12-12T19:13:40+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to configure options for specific formats</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575985001854_955862329","id":"20191210-143641_1543266406","dateCreated":"2019-12-10T14:36:41+0100","dateStarted":"2019-12-12T19:13:40+0100","dateFinished":"2019-12-12T19:13:40+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1594"},{"text":"csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"my-tsv-file.tsv\")","user":"anonymous","dateUpdated":"2019-12-10T14:38:47+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575985069916_1826935237","id":"20191210-143749_633513408","dateCreated":"2019-12-10T14:37:49+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1595"},{"text":"df.write.option(\"sep\", \",\")\n        .option(\"quote\", \"U+005C\")\n        .option(\"escape\", \"U+005C\")\n        .option(\"charToEscapeQuoteEscaping\", \"\\0\")\n        .option(\"escapeQuotes\", \"true\")\n        .option(\"quoteAll\", false)\n        ...\n","user":"anonymous","dateUpdated":"2019-12-10T14:47:54+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575984999899_1715888988","id":"20191210-143639_1616145122","dateCreated":"2019-12-10T14:36:39+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1596"},{"text":"%md\n### How to write a data source to 1 single file or N separate files","user":"admin","dateUpdated":"2019-12-12T19:13:53+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to write a data source to 1 single file or N separate files</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575985065287_-1685161008","id":"20191210-143745_2125326608","dateCreated":"2019-12-10T14:37:45+0100","dateStarted":"2019-12-12T19:13:53+0100","dateFinished":"2019-12-12T19:13:53+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1597"},{"text":"//df is currently stored in how many partition\nval df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"globalpowerplantdatabasev120/*.csv\")\ndf.rdd.partitions.size","user":"anonymous","dateUpdated":"2019-12-10T15:55:16+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [country: string, country_long: string ... 22 more fields]\r\nres58: Int = 1\n"}]},"apps":[],"jobName":"paragraph_1575985704308_-1598802875","id":"20191210-144824_1897653351","dateCreated":"2019-12-10T14:48:24+0100","dateStarted":"2019-12-10T15:55:16+0100","dateFinished":"2019-12-10T15:55:17+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1598"},{"text":"df.repartition(5)\ndf.rdd.getNumPartitions\n","user":"anonymous","dateUpdated":"2019-12-10T15:53:21+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res55: Int = 1\n"}]},"apps":[],"jobName":"paragraph_1575985839783_380617815","id":"20191210-145039_1021740747","dateCreated":"2019-12-10T14:50:39+0100","dateStarted":"2019-12-10T15:53:21+0100","dateFinished":"2019-12-10T15:53:21+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1599"},{"text":"df.coalesce(1) //coalesce restrict to thisnumber of partition and try to reduce the data movement effort, for example keeping some partitions unchanged\r\ndf.repartition(1) //just shuffle all number to a different number of partitions","user":"anonymous","dateUpdated":"2019-12-10T14:53:52+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res49: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country: string, country_long: string ... 22 more fields]\n"}]},"apps":[],"jobName":"paragraph_1575985063639_-135763753","id":"20191210-143743_1845380236","dateCreated":"2019-12-10T14:37:43+0100","dateStarted":"2019-12-10T14:53:52+0100","dateFinished":"2019-12-10T14:53:52+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1600"},{"text":"%md\n### How to write partitioned data\n### How to bucket data by a given set of columns\n","user":"admin","dateUpdated":"2019-12-12T19:14:16+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to write partitioned data</h3>\n<h3>How to bucket data by a given set of columns</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575985955602_-1704268692","id":"20191210-145235_582686940","dateCreated":"2019-12-10T14:52:35+0100","dateStarted":"2019-12-12T19:14:16+0100","dateFinished":"2019-12-12T19:14:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1601"},{"text":"df.write\r\n  .partitionBy(\"ProductKey\")\r\n  .bucketBy(42, \"OrderDateKey\")\r\n  .saveAsTable(\"orders_partitioned_bucketed\"))","user":"anonymous","dateUpdated":"2019-12-10T14:54:57+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575986068418_1102276703","id":"20191210-145428_643315789","dateCreated":"2019-12-10T14:54:28+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1602"},{"text":"%md\n## DataFrame\n### Have a working understanding of every action such as take(), collect(), and foreach()","user":"admin","dateUpdated":"2019-12-12T19:14:26+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrame</h2>\n<h3>Have a working understanding of every action such as take(), collect(), and foreach()</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1575986123103_1108148705","id":"20191210-145523_1589489823","dateCreated":"2019-12-10T14:55:23+0100","dateStarted":"2019-12-12T19:14:26+0100","dateFinished":"2019-12-12T19:14:26+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1603"},{"text":"//con take(n) ottengo le prime n righe del dataframe \ndf.take(1)\n//collect invece non prende alcun parametro. come risultato ho tutto il dataframe. Può essere un'operazione \"costosa\" in quanto tutti i dati dagli executor vengono mandati al driver\ndf.collect()","user":"anonymous","dateUpdated":"2019-12-10T18:15:22+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res71: Array[org.apache.spark.sql.Row] = Array([1.0,a], [2.0,b], [3.0,c], [1.0,d], [2.0,e], [3.0,f])\n"}]},"apps":[],"jobName":"paragraph_1575986121705_1222115163","id":"20191210-145521_1840941900","dateCreated":"2019-12-10T14:55:21+0100","dateStarted":"2019-12-10T18:00:46+0100","dateFinished":"2019-12-10T18:00:46+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1604"},{"text":"%md\n\n### Have a working understanding of the various transformations and how they work such as producing a distinct set, filtering data, repartitioning and coalescing, \nperforming joins and unions as well as producing aggregates. ","user":"admin","dateUpdated":"2019-12-12T19:15:21+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Have a working understanding of the various transformations and how they work such as producing a distinct set, filtering data, repartitioning and coalescing,</h3>\n<p>performing joins and unions as well as producing aggregates.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575996958299_-1011267694","id":"20191210-175558_1033071390","dateCreated":"2019-12-10T17:55:58+0100","dateStarted":"2019-12-12T19:15:21+0100","dateFinished":"2019-12-12T19:15:21+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1605"},{"text":"%md\n#### FILTERING","user":"admin","dateUpdated":"2019-12-12T19:15:27+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>FILTERING</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1576106500998_-1406988839","id":"20191212-002140_1784748128","dateCreated":"2019-12-12T00:21:40+0100","dateStarted":"2019-12-12T19:15:27+0100","dateFinished":"2019-12-12T19:15:27+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1606"},{"text":"val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"globalpowerplantdatabasev120/*.csv\")","user":"anonymous","dateUpdated":"2019-12-10T18:33:53+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [country: string, country_long: string ... 22 more fields]\n"}]},"apps":[],"jobName":"paragraph_1575986120376_1104405252","id":"20191210-145520_1389233865","dateCreated":"2019-12-10T14:55:20+0100","dateStarted":"2019-12-10T18:33:53+0100","dateFinished":"2019-12-10T18:33:54+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1607"},{"text":"%md\nAllo scopo di avere una visualizzazione più semplice del dataframe, riduco lo stesso ad un numero di colonne più maneggiabile. Seleziono le colonne contenenti i dati di stato, capacità(MGW), alimentazione, anno di costruzione, produzione di energia stimata (GWh).\nPer farlo uso il metodo **select()**, che agisce sulle colonne.","user":"anonymous","dateUpdated":"2019-12-10T18:14:22+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Allo scopo di avere una visualizzazione più semplice del dataframe, riduco lo stesso ad un numero di colonne più maneggiabile. Seleziono le colonne contenenti i dati di stato, capacità(MGW), alimentazione, anno di costruzione, produzione di energia stimata (GWh).<br/>Per farlo uso il metodo <strong>select()</strong>, che agisce sulle colonne.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575997753853_-915690360","id":"20191210-180913_1264511170","dateCreated":"2019-12-10T18:09:13+0100","dateStarted":"2019-12-10T18:14:22+0100","dateFinished":"2019-12-10T18:14:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1608"},{"text":"val df_reduced = df.select(\"country_long\", \"capacity_mw\", \"primary_fuel\", \"commissioning_year\", \"estimated_generation_gwh\")","user":"anonymous","dateUpdated":"2019-12-10T18:50:54+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_reduced: org.apache.spark.sql.DataFrame = [country_long: string, capacity_mw: double ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1575998033801_-590846949","id":"20191210-181353_632470427","dateCreated":"2019-12-10T18:13:53+0100","dateStarted":"2019-12-10T18:50:54+0100","dateFinished":"2019-12-10T18:50:54+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1609"},{"text":"%md\ncon **filter()** seleziono solo le righe in cui la produzione stimata è superiore a 50000GWh ","user":"anonymous","dateUpdated":"2019-12-10T18:16:26+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>con <strong>filter()</strong> seleziono solo le righe in cui la produzione stimata è superiore a 50000GWh</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575998056843_-2130441077","id":"20191210-181416_307482537","dateCreated":"2019-12-10T18:14:16+0100","dateStarted":"2019-12-10T18:16:26+0100","dateFinished":"2019-12-10T18:16:26+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1610"},{"text":"df_reduced.filter('estimated_generation_gwh > 50000).show()","user":"anonymous","dateUpdated":"2019-12-10T18:22:23+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----------+------------+------------------+------------------------+\n|        country_long|capacity_mw|primary_fuel|commissioning_year|estimated_generation_gwh|\n+--------------------+-----------+------------+------------------+------------------------+\n|               China|    13050.0|       Hydro|              null|       53622.49078855527|\n|               China|    22500.0|       Hydro|            2003.0|        92452.5703250953|\n|               China|    12600.0|       Hydro|            2013.0|      51773.439382053366|\n|United States of ...|      454.3|        Coal|              null|       450562.6923495511|\n+--------------------+-----------+------------+------------------+------------------------+\n\r\n"}]},"apps":[],"jobName":"paragraph_1575997479838_-170245660","id":"20191210-180439_1208136448","dateCreated":"2019-12-10T18:04:39+0100","dateStarted":"2019-12-10T18:22:23+0100","dateFinished":"2019-12-10T18:22:24+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1611"},{"text":"%md\n#### DISTINCT DATASET","user":"anonymous","dateUpdated":"2019-12-12T00:22:35+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>DISTINCT DATASET</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1576106533778_-192297514","id":"20191212-002213_234199614","dateCreated":"2019-12-12T00:22:13+0100","dateStarted":"2019-12-12T00:22:35+0100","dateFinished":"2019-12-12T00:22:35+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1612"},{"text":"%md\nuso **distinct()** per vedere quali sono tutti gli elementi distinti della colonna *country_long* ","user":"anonymous","dateUpdated":"2019-12-10T18:25:13+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>uso <strong>distinct()</strong> per vedere quali sono tutti gli elementi distinti della colonna <em>country_long</em></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575998613849_-1122245355","id":"20191210-182333_246931147","dateCreated":"2019-12-10T18:23:33+0100","dateStarted":"2019-12-10T18:25:13+0100","dateFinished":"2019-12-10T18:25:13+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1613"},{"text":"df_reduced.select('country_long).distinct().show","user":"anonymous","dateUpdated":"2019-12-10T18:23:20+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+\n|  country_long|\n+--------------+\n|      Paraguay|\n|        Russia|\n|         Yemen|\n|       Senegal|\n|        Sweden|\n|        Guyana|\n|       Eritrea|\n|   Philippines|\n|      Djibouti|\n|      Malaysia|\n|     Singapore|\n|          Fiji|\n|        Turkey|\n|        Malawi|\n|Western Sahara|\n|          Iraq|\n|       Germany|\n|   Afghanistan|\n|      Cambodia|\n|        Jordan|\n+--------------+\nonly showing top 20 rows\n\r\n"}]},"apps":[],"jobName":"paragraph_1575998251455_-2122922514","id":"20191210-181731_80430365","dateCreated":"2019-12-10T18:17:31+0100","dateStarted":"2019-12-10T18:23:20+0100","dateFinished":"2019-12-10T18:23:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1614"},{"text":"%md\n#### JOIN","user":"anonymous","dateUpdated":"2019-12-12T00:23:29+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>JOIN</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1576106599338_1464784574","id":"20191212-002319_1412952400","dateCreated":"2019-12-12T00:23:19+0100","dateStarted":"2019-12-12T00:23:29+0100","dateFinished":"2019-12-12T00:23:29+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1615"},{"text":"%md\ndefault è un inner join, cioè vengono usate le chiavi comuni","user":"anonymous","dateUpdated":"2019-12-11T23:31:46+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>default è un inner join, cioè vengono usate le chiavi comuni</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576015272302_-149440235","id":"20191210-230112_1384528109","dateCreated":"2019-12-10T23:01:12+0100","dateStarted":"2019-12-11T23:31:46+0100","dateFinished":"2019-12-11T23:31:46+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1616"},{"text":"val df_1 = Seq(\n    (0, \"zero\"),\n    (1, \"uno\"),\n    (4, \"quattro\")).toDF(\"number\", \"numberString\")\n\nval df_2 = Seq(\n    (1, \"UNO\"),\n    (2, \"due\"),\n    (3, \"tre\")).toDF(\"number\", \"numberString\")\n\n//se ho una chiave comune posso semplicemente indicare la chiave sulla quale fare il join\ndf_2.join(df_1, \"number\").show()","user":"anonymous","dateUpdated":"2019-12-10T23:05:20+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+------------+------------+\n|number|numberString|numberString|\n+------+------------+------------+\n|     1|         UNO|         uno|\n+------+------------+------------+\n\r\ndf_1: org.apache.spark.sql.DataFrame = [number: int, numberString: string]\r\ndf_2: org.apache.spark.sql.DataFrame = [number: int, numberString: string]\n"}]},"apps":[],"jobName":"paragraph_1576014893603_-248154685","id":"20191210-225453_1498492579","dateCreated":"2019-12-10T22:54:53+0100","dateStarted":"2019-12-10T23:00:59+0100","dateFinished":"2019-12-10T23:01:00+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1617"},{"text":"%md\nse non abbiamo una chiave comune devo usare una espressione del tipo \n`$\"key1\"===$\"key2\"`","user":"anonymous","dateUpdated":"2019-12-11T23:31:18+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>se non abbiamo una chiave comune devo usare una espressione del tipo<br/><code>$&quot;key1&quot;===$&quot;key2&quot;</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576015631471_966026247","id":"20191210-230711_944681551","dateCreated":"2019-12-10T23:07:11+0100","dateStarted":"2019-12-11T23:31:18+0100","dateFinished":"2019-12-11T23:31:18+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1618"},{"text":"val df_1 = Seq(\n    (0, \"zero\"),\n    (1, \"uno\"),\n    (4, \"quattro\")).toDF(\"numberKey1\", \"numberString\")\n\nval df_2 = Seq(\n    (1, \"UNO\"),\n    (2, \"due\"),\n    (3, \"tre\")).toDF(\"numberKey2\", \"numberString\")\n\n//se ho una chiave comune posso semplicemente indicare la chiave sulla quale fare il join\ndf_2.join(df_1, $\"numberKey1\"===$\"numberKey2\").show()","user":"anonymous","dateUpdated":"2019-12-10T23:06:50+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------------+----------+------------+\n|numberKey2|numberString|numberKey1|numberString|\n+----------+------------+----------+------------+\n|         1|         UNO|         1|         uno|\n+----------+------------+----------+------------+\n\r\ndf_1: org.apache.spark.sql.DataFrame = [numberKey1: int, numberString: string]\r\ndf_2: org.apache.spark.sql.DataFrame = [numberKey2: int, numberString: string]\n"}]},"apps":[],"jobName":"paragraph_1576015548722_-1750776844","id":"20191210-230548_444257089","dateCreated":"2019-12-10T23:05:48+0100","dateStarted":"2019-12-10T23:06:50+0100","dateFinished":"2019-12-10T23:06:50+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1619"},{"text":"val df_1 = Seq(\n    (0, \"zero\"),\n    (1, \"uno\"),\n    (4, \"quattro\")).toDF(\"number\", \"numberString\")\n\nval df_2 = Seq(\n    (1, \"UNO\"),\n    (2, \"due\"),\n    (3, \"tre\")).toDF(\"number\", \"numberString\")\n\n//se ho una chiave comune posso semplicemente indicare la chiave sulla quale fare il join\ndf_2.join(df_1, Seq(\"number\"), \"fullouter\").show()","user":"anonymous","dateUpdated":"2019-12-10T23:22:13+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+------------+------------+\n|number|numberString|numberString|\n+------+------------+------------+\n|     1|         UNO|         uno|\n|     3|         tre|        null|\n|     4|        null|     quattro|\n|     2|         due|        null|\n|     0|        null|        zero|\n+------+------------+------------+\n\r\ndf_1: org.apache.spark.sql.DataFrame = [number: int, numberString: string]\r\ndf_2: org.apache.spark.sql.DataFrame = [number: int, numberString: string]\n"}]},"apps":[],"jobName":"paragraph_1576015524624_-2130056033","id":"20191210-230524_1703954643","dateCreated":"2019-12-10T23:05:24+0100","dateStarted":"2019-12-10T23:22:13+0100","dateFinished":"2019-12-10T23:22:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1620"},{"text":"%md\nesiste anche una \"left-anti\" join in cui vengono usate le chiavi non comuni e vengono prese solo le colonne del dataframe di sinistra","user":"anonymous","dateUpdated":"2019-12-10T23:27:25+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>esiste anche una &ldquo;left-anti&rdquo; join in cui vengono usate le chiavi non comuni e vengono prese solo le colonne del dataframe di sinistra</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576016570059_-1544750330","id":"20191210-232250_794208794","dateCreated":"2019-12-10T23:22:50+0100","dateStarted":"2019-12-10T23:27:25+0100","dateFinished":"2019-12-10T23:27:25+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1621"},{"text":"val df_1 = Seq(\n    (0, \"zero\"),\n    (1, \"uno\"),\n    (4, \"quattro\")).toDF(\"number\", \"numberString\")\n\nval df_2 = Seq(\n    (1, \"UNO\"),\n    (2, \"due\"),\n    (3, \"tre\")).toDF(\"number\", \"numberString\")\n\n//se ho una chiave comune posso semplicemente indicare la chiave sulla quale fare il join\ndf_2.join(df_1, \"number\").show()\n\ndf_2.join(df_1, Seq(\"number\"), \"leftanti\").show()\n\ndf_2.join(df_1, Seq(\"number\"), \"fullouter\").show()\n\ndf_2.join(df_1, Seq(\"number\"), \"leftouter\").show()\n\ndf_2.join(df_1, Seq(\"number\"), \"leftsemi\").show()\n\ndf_2.join(df_1, Seq(\"number\"), \"rightouter\").show()\n","user":"anonymous","dateUpdated":"2019-12-11T23:35:43+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+------------+------------+\n|number|numberString|numberString|\n+------+------------+------------+\n|     1|         UNO|         uno|\n+------+------------+------------+\n\r\n+------+------------+\n|number|numberString|\n+------+------------+\n|     2|         due|\n|     3|         tre|\n+------+------------+\n\r\n+------+------------+------------+\n|number|numberString|numberString|\n+------+------------+------------+\n|     1|         UNO|         uno|\n|     3|         tre|        null|\n|     4|        null|     quattro|\n|     2|         due|        null|\n|     0|        null|        zero|\n+------+------------+------------+\n\r\n+------+------------+------------+\n|number|numberString|numberString|\n+------+------------+------------+\n|     1|         UNO|         uno|\n|     2|         due|        null|\n|     3|         tre|        null|\n+------+------------+------------+\n\r\n+------+------------+\n|number|numberString|\n+------+------------+\n|     1|         UNO|\n+------+------------+\n\r\n+------+------------+------------+\n|number|numberString|numberString|\n+------+------------+------------+\n|     0|        null|        zero|\n|     1|         UNO|         uno|\n|     4|        null|     quattro|\n+------+------------+------------+\n\r\ndf_1: org.apache.spark.sql.DataFrame = [number: int, numberString: string]\r\ndf_2: org.apache.spark.sql.DataFrame = [number: int, numberString: string]\n"}]},"apps":[],"jobName":"paragraph_1576016859529_1855191508","id":"20191210-232739_1603140433","dateCreated":"2019-12-10T23:27:39+0100","dateStarted":"2019-12-11T23:35:43+0100","dateFinished":"2019-12-11T23:35:45+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1622"},{"text":"%md\n#### PARTINIONING - REPARTITION e COALESCE","user":"anonymous","dateUpdated":"2019-12-12T00:24:50+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>PARTINIONING - REPARTITION e COALESCE</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1576106573718_-1583946594","id":"20191212-002253_416496746","dateCreated":"2019-12-12T00:22:53+0100","dateStarted":"2019-12-12T00:24:50+0100","dateFinished":"2019-12-12T00:24:50+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1623"},{"text":"//un parametro interessante in questo senso è spark.sql.files.maxPartitionBytes, la quantità di spazio su disco max per ogni partizione \nspark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"1000000\")\n\n//ricostruisco il datatframe\nval df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"globalpowerplantdatabasev120/*.csv\")\n\n//poichè ho deciso di fare partizioni da circa 1MB, il file (7.55MB) viene suddiviso in 8 partizioni, come possiamo verificare\nprintln(\"Numero di partizioni di df: \" + df.rdd.getNumPartitions)\n\n//riduco il numero di partizioni a 4\nval df_coalesce = df.coalesce(4)\n\nprintln(\"Numero di partizioni di df_colesce: \" + df_coalesce.rdd.getNumPartitions)\n\n//aumento il numero di partizioni a 12\nval df_repartition = df.repartition(12)\n\nprintln(\"Numero di partizioni di df_reduced_repartition: \" + df_repartition.rdd.getNumPartitions)","user":"anonymous","dateUpdated":"2019-12-10T19:28:21+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":226.8,"optionOpen":false}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575998769596_-1291444881","id":"20191210-182609_462097691","dateCreated":"2019-12-10T18:26:09+0100","dateStarted":"2019-12-10T19:06:39+0100","dateFinished":"2019-12-10T19:06:40+0100","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1624"},{"text":"%md\nin questo momento il dataframe *df_2* è suddiviso tra x partizioni, vediamo esattemente quante sono:","user":"anonymous","dateUpdated":"2019-12-11T23:35:48+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>in questo momento il dataframe <em>df_2</em> è suddiviso tra x partizioni, vediamo esattemente quante sono:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576016967679_-1567596924","id":"20191210-232927_992011959","dateCreated":"2019-12-10T23:29:27+0100","dateStarted":"2019-12-11T23:35:48+0100","dateFinished":"2019-12-11T23:35:48+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1625"},{"text":"df_2.rdd.getNumPartitions","user":"anonymous","dateUpdated":"2019-12-11T23:35:51+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res6: Int = 1\n"}]},"apps":[],"jobName":"paragraph_1576016965179_-1386156713","id":"20191210-232925_1979395053","dateCreated":"2019-12-10T23:29:25+0100","dateStarted":"2019-12-11T23:35:51+0100","dateFinished":"2019-12-11T23:35:51+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1626"},{"text":"%md\nposso decidere di distribuire i dati su 10 partizioni, usando il metodo **repartition()*","user":"anonymous","dateUpdated":"2019-12-11T23:36:57+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>posso decidere di distribuire i dati su 10 partizioni, usando il metodo **repartition()*</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576016963462_-1786736466","id":"20191210-232923_28399345","dateCreated":"2019-12-10T23:29:23+0100","dateStarted":"2019-12-11T23:36:57+0100","dateFinished":"2019-12-11T23:36:57+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1627"},{"text":"val df_3 = df_2.repartition(10)\ndf_3.rdd.getNumPartitions","user":"anonymous","dateUpdated":"2019-12-11T23:37:50+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [number: int, numberString: string]\r\nres9: Int = 10\n"}]},"apps":[],"jobName":"paragraph_1576016960515_1717189830","id":"20191210-232920_807033847","dateCreated":"2019-12-10T23:29:20+0100","dateStarted":"2019-12-11T23:37:50+0100","dateFinished":"2019-12-11T23:37:50+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1628"},{"text":"%md\ncon il metodo **coalesce()** posso ridurre il numero delle partizioni con la garanzia che il minor numero di spostamenti è stato fatto","user":"admin","dateUpdated":"2019-12-12T19:17:28+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>con il metodo <strong>coalesce()</strong> posso ridurre il numero delle partizioni con la garanzia che il minor numero di spostamenti è stato fatto</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1576103885634_1553147367","id":"20191211-233805_1133424068","dateCreated":"2019-12-11T23:38:05+0100","dateStarted":"2019-12-12T19:17:28+0100","dateFinished":"2019-12-12T19:17:28+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1629"},{"text":"val df_4 = df_2.coalesce(2)\ndf_3.rdd.getNumPartitions","user":"anonymous","dateUpdated":"2019-12-11T23:40:40+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_4: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [number: int, numberString: string]\r\nres10: Int = 10\n"}]},"apps":[],"jobName":"paragraph_1576103976400_657956159","id":"20191211-233936_2021273454","dateCreated":"2019-12-11T23:39:36+0100","dateStarted":"2019-12-11T23:40:40+0100","dateFinished":"2019-12-11T23:40:40+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1630"}],"name":"Databricks cert","id":"2EUA7CSBC","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}