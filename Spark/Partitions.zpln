{
  "paragraphs": [
    {
      "text": "%md\n## Cosa sono le partizioni di Apache Spark?\n\nPoiché non sono riuscito a trovare informazioni chiare e definitive su come vengono gestite le partizioni in Apache Spark, ho deciso di investigare un po' per conto mio.\n\n### Cosa ho trovato ?\nCercando su Internet ho trovato diverse informazioni che non so bene come collegare tra loro. Vediamo queste informazioni.\n\nNel memorizzare i dati in partizioni vengono seguite queste regole:\n- Il numero di partizioni che vengono generate dipende dal numero di cores \n- Ogni nodo (executor) del cluster di Spark contiene una o più partizioni\n- Di default il numero di partizioni dovrebbe essere settato pari al numero di cores disponibili nel cluster, purtroppo non si può fissare direttamente il numero di partizioni\n- Ogni partizione è interamente contenuta in un unico worker (o executor???)\n- Spark assegna un task per ogni partizione e di default ogni core può processare un task per volta\n\n\n### Esempio di partitioning\nUn esempio per capire l'importanza del numero di partizioni.\n\nSupponiamo di avere un cluster con 4 core, e che i dati vengano suddivisi in 5 partizioni....\n\nIl numero di partizioni suggerito è pari al numero di cores del cluster. In questo modo ogni nodo (supponendo 1 nodo = 1 core) riceve una partizione, e ho un numero di task pari al numero delle partizioni (core). L'utilizzo del cluster è al 100%, ovvero o tutti i nodi sono impegnati a processare la relativa partizione 👌.\n\nCome esempio, carico un dataset di 7.55MB in un dataframe e controllo subito in quante partizioni è suddiviso il dataframe.\nPer vedere il numero di partizioni uso il metodo **getNumPartitions()** o **df.rdd.pastions.size()**.\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-07T08:55:57+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Cosa sono le partizioni di Apache Spark?</h2>\n<p>Poiché non sono riuscito a trovare informazioni chiare e definitive su come vengono gestite le partizioni in Apache Spark, ho deciso di investigare un po&rsquo; per conto mio.</p>\n<h3>Cosa ho trovato ?</h3>\n<p>Cercando su Internet ho trovato diverse informazioni che non so bene come collegare tra loro. Vediamo queste informazioni.</p>\n<p>Nel memorizzare i dati in partizioni vengono seguite queste regole:</p>\n<ul>\n<li>Il numero di partizioni che vengono generate dipende dal numero di cores</li>\n<li>Ogni nodo (executor) del cluster di Spark contiene una o più partizioni</li>\n<li>Di default il numero di partizioni dovrebbe essere settato pari al numero di cores disponibili nel cluster, purtroppo non si può fissare direttamente il numero di partizioni</li>\n<li>Ogni partizione è interamente contenuta in un unico worker (o executor???)</li>\n<li>Spark assegna un task per ogni partizione e di default ogni core può processare un task per volta</li>\n</ul>\n<h3>Esempio di partitioning</h3>\n<p>Un esempio per capire l&rsquo;importanza del numero di partizioni.</p>\n<p>Supponiamo di avere un cluster con 4 core, e che i dati vengano suddivisi in 5 partizioni&hellip;.</p>\n<p>Il numero di partizioni suggerito è pari al numero di cores del cluster. In questo modo ogni nodo (supponendo 1 nodo = 1 core) riceve una partizione, e ho un numero di task pari al numero delle partizioni (core). L&rsquo;utilizzo del cluster è al 100%, ovvero o tutti i nodi sono impegnati a processare la relativa partizione 👌.</p>\n<p>Come esempio, carico un dataset di 7.55MB in un dataframe e controllo subito in quante partizioni è suddiviso il dataframe.<br />\nPer vedere il numero di partizioni uso il metodo <strong>getNumPartitions()</strong> o <strong>df.rdd.pastions.size()</strong>.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076003_1532229198",
      "id": "20191211-111114_1565692295",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-07T08:55:57+0000",
      "dateFinished": "2021-06-07T08:55:57+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:181"
    },
    {
      "text": "val df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"notebook/myfiles/globalpowerplantdatabasev120/*.csv\")\nprintln(f\"Numero di partizioni del dataframe: ${df.rdd.partitions.size}\")",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T16:53:56+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Numero di partizioni del dataframe: 2\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076012_1968820985",
      "id": "20191211-113930_2105717002",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T16:53:56+0000",
      "dateFinished": "2021-06-06T16:54:02+0000",
      "status": "FINISHED",
      "$$hashKey": "object:182"
    },
    {
      "text": "%md\nPerché 2 partizioni soltanto?\nPerché Spark parte di default con due **executors** e un core per ogni executor. Per settare un numero di executors (cores) più alto si può usare **--num-executors** (e **--executor-cores**) al momento di lanciare il cluster.\n\nInvece di --num-executors si può settare la proprietà **spark.executor.instances**.\n\nMa prima devo stoppare la SparkSession per lanciare una nuova SparkSession con settaggi diversi. ",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T16:54:35+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Perché 2 partizioni soltanto?<br />\nPerché Spark parte di default con due <strong>executors</strong> e un core per ogni executor. Per settare un numero di executors (cores) più alto si può usare <strong>&ndash;num-executors</strong> (e <strong>&ndash;executor-cores</strong>) al momento di lanciare il cluster.</p>\n<p>Invece di &ndash;num-executors si può settare la proprietà <strong>spark.executor.instances</strong>.</p>\n<p>Ma prima devo stoppare la SparkSession per lanciare una nuova SparkSession con settaggi diversi.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076013_453309642",
      "id": "20200105-162802_1539673061",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T16:54:35+0000",
      "dateFinished": "2021-06-06T16:54:36+0000",
      "status": "FINISHED",
      "$$hashKey": "object:183"
    },
    {
      "text": "spark.stop",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T15:03:07+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076014_2119689357",
      "id": "20200105-180314_546314396",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T15:03:07+0000",
      "dateFinished": "2021-06-06T15:03:08+0000",
      "status": "FINISHED",
      "$$hashKey": "object:184"
    },
    {
      "text": "%md\nE questa è la nuova SparkSession con le nuove impostazioni.",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T16:55:13+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>E questa è la nuova SparkSession con le nuove impostazioni.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622998486027_1947179632",
      "id": "paragraph_1622998486027_1947179632",
      "dateCreated": "2021-06-06T16:54:46+0000",
      "dateStarted": "2021-06-06T16:55:13+0000",
      "dateFinished": "2021-06-06T16:55:13+0000",
      "status": "FINISHED",
      "$$hashKey": "object:185"
    },
    {
      "text": "import org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder.master(\"local[4]\").config(\"spark.executor.instances\", 4)\n                                                   .config(\"spark.default.parallelism\", 4).getOrCreate()\n\n\nval numExecs = spark.conf.get(\"spark.executor.instances\")\nprintln(f\"Numero di executors ${numExecs}\" )",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T16:55:28+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Numero di executors 4\nimport org.apache.spark.sql.SparkSession\n\u001b[1m\u001b[34mspark\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.SparkSession\u001b[0m = org.apache.spark.sql.SparkSession@470e03d7\n\u001b[1m\u001b[34mnumExecs\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 4\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076015_969627801",
      "id": "20200105-172541_599899694",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T16:55:28+0000",
      "dateFinished": "2021-06-06T16:55:30+0000",
      "status": "FINISHED",
      "$$hashKey": "object:186"
    },
    {
      "text": "%md\n\nIl setting `sc.defaultMinPartitions` definisce il numero minimo di partizioni quando il dataframe è creato.\n\nQuesto parametro è definito internamente da Spark come il minimo tra defaultParallelism e 2, ciò significa che defaultMinPartitions non può essere più piccolo di 2.\n\n**defaultParallelism** viene settato di default pari al numero di cores usato. Posso settare questo in una nuova **Sparkession** con il metodo **master(\"local[x]\")**.\n\nSi noti che anche allocando un numero alto di executor, il numero minimo di partizioni create è 2.\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T20:55:37+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Il setting <code>sc.defaultMinPartitions</code> definisce il numero minimo di partizioni quando il dataframe è creato.</p>\n<p>Questo parametro è definito internamente da Spark come il minimo tra defaultParallelism e 2, ciò significa che defaultMinPartitions non può essere più piccolo di 2.</p>\n<p><strong>defaultParallelism</strong> viene settato di default pari al numero di cores usato. Posso settare questo in una nuova <strong>Sparkession</strong> con il metodo <strong>master(&ldquo;local[x]&rdquo;)</strong>.</p>\n<p>Si noti che anche allocando un numero alto di executor, il numero minimo di partizioni create è 2.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076016_1496070029",
      "id": "20191211-123655_801279717",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T20:55:37+0000",
      "dateFinished": "2021-06-06T20:55:37+0000",
      "status": "FINISHED",
      "$$hashKey": "object:187"
    },
    {
      "text": "println(f\"sc.defaultParallelism: ${spark.sparkContext.defaultParallelism}\")\nprintln(f\"Numero minimo di partizioni: ${spark.sparkContext.defaultMinPartitions}\")",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T15:09:09+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "sc.defaultParallelism: 4\nNumero minimo di partizioni: 2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076017_1452784900",
      "id": "20191211-121001_507255553",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T15:09:09+0000",
      "dateFinished": "2021-06-06T15:09:11+0000",
      "status": "FINISHED",
      "$$hashKey": "object:188"
    },
    {
      "text": "%md\nDiversi settaggi influiscono sul valore di **sc.defaultParallelism**:\n- se uso **master(\"local[x]\")** per usare x cores, defaultParallelism viene settato a x\n- si può definire nello SparkSession, per es.:\n```val spark = SparkSession.builder().appName(\"TestPartitionNums\").master(\"local\").config(\"spark.default.parallelism\", 20).getOrCreate()```\n- si può settare nel file *spark-defaults.conf* con il settaggio: `spark.default.parallelism=20`\n- in Apache Zeppelin si può anche settare nella GUI tra i parametri dell'interprete Spark, creando un nuovo parametro con `spark.default.parallelism=20`\n\nVediamo un esempio, ma prima dobbiamo stoppare la SparkSession.\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T17:01:31+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Diversi settaggi influiscono sul valore di <strong>sc.defaultParallelism</strong>:</p>\n<ul>\n<li>se uso <strong>master(&ldquo;local[x]&rdquo;)</strong> per usare x cores, defaultParallelism viene settato a x</li>\n<li>si può definire nello SparkSession, per es.:<br />\n<code>val spark = SparkSession.builder().appName(&quot;TestPartitionNums&quot;).master(&quot;local&quot;).config(&quot;spark.default.parallelism&quot;, 20).getOrCreate()</code></li>\n<li>si può settare nel file <em>spark-defaults.conf</em> con il settaggio: <code>spark.default.parallelism=20</code></li>\n<li>in Apache Zeppelin si può anche settare nella GUI tra i parametri dell&rsquo;interprete Spark, creando un nuovo parametro con <code>spark.default.parallelism=20</code></li>\n</ul>\n<p>Vediamo un esempio, ma prima dobbiamo stoppare la SparkSession.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076018_1393951366",
      "id": "20191211-124201_1141603656",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T17:01:31+0000",
      "dateFinished": "2021-06-06T17:01:31+0000",
      "status": "FINISHED",
      "$$hashKey": "object:189"
    },
    {
      "text": "spark.stop",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T17:01:36+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076021_1151712393",
      "id": "20191211-120018_821827504",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T17:01:36+0000",
      "dateFinished": "2021-06-06T17:01:37+0000",
      "status": "FINISHED",
      "$$hashKey": "object:190"
    },
    {
      "text": "%md\nSettando `master(\"local[*]\")` la SparkSession userà tutti e 4 i cores del mio laptop. ",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T17:04:38+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Settando <code>master(&quot;local[*]&quot;)</code> la SparkSession userà tutti e 4 i cores del mio laptop.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622999014108_1348010350",
      "id": "paragraph_1622999014108_1348010350",
      "dateCreated": "2021-06-06T17:03:34+0000",
      "dateStarted": "2021-06-06T17:04:38+0000",
      "dateFinished": "2021-06-06T17:04:38+0000",
      "status": "FINISHED",
      "$$hashKey": "object:191"
    },
    {
      "text": "import org.apache.spark.sql.SparkSession\nval spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\nval sc = spark.sparkContext\n\nval df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"notebook/myfiles/globalpowerplantdatabasev120/*.csv\")\nprintln(f\"Numero di partizioni: ${df.rdd.getNumPartitions}\")\nprintln(f\"sc.defaultParallelism: ${sc.defaultParallelism}\")\nprintln(f\"Numero minimo di partizioni: ${sc.defaultMinPartitions}\")",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T21:35:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Numero di partizioni: 8\nsc.defaultParallelism: 4\nNumero minimo di partizioni: 2\nimport org.apache.spark.sql.SparkSession\n\u001b[1m\u001b[34mspark\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.SparkSession\u001b[0m = org.apache.spark.sql.SparkSession@7f59fe\n\u001b[1m\u001b[34msc\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.SparkContext\u001b[0m = org.apache.spark.SparkContext@3beb7dfc\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076022_1682177523",
      "id": "20191211-114655_2070483524",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T21:35:20+0000",
      "dateFinished": "2021-06-06T21:35:27+0000",
      "status": "FINISHED",
      "$$hashKey": "object:192"
    },
    {
      "text": "%md\nQuindi 4 cores ha disposizione ma solo 2 partizioni, significa che 2 cores saranno impegnati a processare 2 partizioni, e altri 2 cores saranno non utilizzati. Uno spreco di risorse e anche di tempo, visto che il tempo di elaborazione sarà più lungo.\n\nVediamo come risolvere il problema.\n\nIl parametro fondamentale per capire quante partizioni vengono generate è\n`spark.sql.files.maxPartitionBytes`\nCome si evince dal nome, questo parametro definisce il massimo spazio su disco per ogni partizione.\nSetto questo parametro a 1M (il default è 128MB), con un file dati da 7.55MB, implica che vengono create **ceil(7.55M/1M) = 8** partizioni.\nSi noti che così viene rispettata la condizione **defaultMinPartitions** = 2.\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T21:20:08+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Quindi 4 cores ha disposizione ma solo 2 partizioni, significa che 2 cores saranno impegnati a processare 2 partizioni, e altri 2 cores saranno non utilizzati. Uno spreco di risorse e anche di tempo, visto che il tempo di elaborazione sarà più lungo.</p>\n<p>Vediamo come risolvere il problema.</p>\n<p>Il parametro fondamentale per capire quante partizioni vengono generate è<br />\n<code>spark.sql.files.maxPartitionBytes</code><br />\nCome si evince dal nome, questo parametro definisce il massimo spazio su disco per ogni partizione.<br />\nSetto questo parametro a 1M (il default è 128MB), con un file dati da 7.55MB, implica che vengono create <strong>ceil(7.55M/1M) = 8</strong> partizioni.<br />\nSi noti che così viene rispettata la condizione <strong>defaultMinPartitions</strong> = 2.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076024_2053264956",
      "id": "20191211-120627_1269901082",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T21:20:08+0000",
      "dateFinished": "2021-06-06T21:20:08+0000",
      "status": "FINISHED",
      "$$hashKey": "object:193"
    },
    {
      "text": "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"1000000\")\nval df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"notebook/myfiles/globalpowerplantdatabasev120/*.csv\")\n\nprintln(f\"sc.defaultParallelism: ${sc.defaultParallelism}\")\nprintln(f\"Numero minimo di partizioni: ${sc.defaultMinPartitions}\")\nprintln(f\"Numero di partizioni: ${df.rdd.getNumPartitions}\")",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T21:36:57+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "sc.defaultParallelism: 4\nNumero minimo di partizioni: 2\nNumero di partizioni: 8\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076025_148251544",
      "id": "20191211-121123_739577557",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T21:36:57+0000",
      "dateFinished": "2021-06-06T21:37:01+0000",
      "status": "FINISHED",
      "$$hashKey": "object:194"
    },
    {
      "text": "%md\nCome volevasi dimostrare ho 8 partizioni.\n\nDunque, per uno sfruttamento ottimale del cluster (con 4 cores) dovremmo settare **spark.sql.files.maxPartitionBytes** in modo tale da avere un numero di partizioni pari a 4  (o multipli di 4). Ad esempio, se il file è di 7.55MB posso settare:\n`spark.sql.files.maxPartitionBytes = 2M`\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T21:37:07+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Come volevasi dimostrare ho 8 partizioni.</p>\n<p>Dunque, per uno sfruttamento ottimale del cluster (con 4 cores) dovremmo settare <strong>spark.sql.files.maxPartitionBytes</strong> in modo tale da avere un numero di partizioni pari a 4  (o multipli di 4). Ad esempio, se il file è di 7.55MB posso settare:<br />\n<code>spark.sql.files.maxPartitionBytes = 2M</code></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076030_2055609908",
      "id": "20200105-033821_837933988",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T21:37:07+0000",
      "dateFinished": "2021-06-06T21:37:07+0000",
      "status": "FINISHED",
      "$$hashKey": "object:195"
    },
    {
      "text": "%md\n### Caricare dati da più file\nIl caso visto sopra funziona quando c'è solo un file da aprire. Di regola una grossa quantità di dati è momorizzata in più file.\n\nVediamo come funziona il partitioning nel caso i ati siano memorizzati in più file.\n\nAd esempio i miei dati sono in una cartella, i files sono nominati come segue:\n- block_1.csv\n- block_2.csv\n...\n- block_10.csv\n\nPer caricare i dati devo usare il **dataFrame reader** indicando semplicemente la cartella dove si trovano i dati.\n\nIn questo caso i dati sono 10 files da 25MB ciascuno, vediamo cosa succede e in quante partizioni vengono spalmati i dati. Usiamo l'impostazione di default `maxPartitionBytes = 128MB`.",
      "user": "anonymous",
      "dateUpdated": "2021-06-07T08:38:28+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Caricare dati da più file</h3>\n<p>Il caso visto sopra funziona quando c&rsquo;è solo un file da aprire. Di regola una grossa quantità di dati è momorizzata in più file.</p>\n<p>Vediamo come funziona il partitioning nel caso i ati siano memorizzati in più file.</p>\n<p>Ad esempio i miei dati sono in una cartella, i files sono nominati come segue:</p>\n<ul>\n<li>block_1.csv</li>\n<li>block_2.csv<br />\n&hellip;</li>\n<li>block_10.csv</li>\n</ul>\n<p>Per caricare i dati devo usare il <strong>dataFrame reader</strong> indicando semplicemente la cartella dove si trovano i dati.</p>\n<p>In questo caso i dati sono 10 files da 25MB ciascuno, vediamo cosa succede e in quante partizioni vengono spalmati i dati. Usiamo l&rsquo;impostazione di default <code>maxPartitionBytes = 128MB</code>.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623014737334_772416573",
      "id": "paragraph_1623014737334_772416573",
      "dateCreated": "2021-06-06T21:25:37+0000",
      "dateStarted": "2021-06-07T08:38:28+0000",
      "dateFinished": "2021-06-07T08:38:28+0000",
      "status": "FINISHED",
      "$$hashKey": "object:196"
    },
    {
      "text": "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 128*1024*1024)\n\nval df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"notebook/myfiles/blocks\")\n\nprintln(f\"sc.defaultParallelism: ${sc.defaultParallelism}\")\nprintln(f\"Numero minimo di partizioni: ${sc.defaultMinPartitions}\")\nprintln(f\"Dimensione massima della partizione (in Bytes): ${spark.conf.get(\"spark.sql.files.maxPartitionBytes\")}\")\nprintln(f\"Numero di partizioni: ${df.rdd.getNumPartitions}\")\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-07T08:39:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "sc.defaultParallelism: 4\nNumero minimo di partizioni: 2\nDimensione massima della partizione (in Bytes): 134217728\nNumero di partizioni: 5\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id_1: int, id_2: int ... 10 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623054425001_464384250",
      "id": "paragraph_1623054425001_464384250",
      "dateCreated": "2021-06-07T08:27:05+0000",
      "dateStarted": "2021-06-07T08:39:12+0000",
      "dateFinished": "2021-06-07T08:39:53+0000",
      "status": "FINISHED",
      "$$hashKey": "object:197"
    },
    {
      "text": "%md\nOttengo che i 250MB sono suddivisi in 5 partizioni. Non è una suddivisione ottimale avendo 4 cores. Vuol dire che mentre i primi 4 tasks saranno finiti quasi contemporaneamente, poi rimarrà il quinto task da eseguire, quindi un solo core impegnato a lavorare e altri 3 in idle! \n\nPerché ho questa suddivisione? Perché Spark calcola una dimensione massima della partizione usando il codice sotto:\n\n\n```\nval defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\nval openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\nval defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)\nval files = listStatus(context).asScala\nval totalBytes = files.filterNot(_.isDirectory).map(_.getLen + openCostInBytes).sum\nval bytesPerCore = totalBytes / defaultParallelism\nval maxSplitSize = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))\n```\n\nNel nostro caso ho:\n\ndefaultMaxSplitBytes = 128M <- default\nopenCostInBytes = 4M        <- default\ndefaultParallelism = 4\ntotalBytes = 10*(25M+4M) = 290M\nbytesPerCore = 250M/4 = 72.5M\nmaxSplitSize = min(128M, max(4M, 72.5M)) = 72.5M\n\nQuindi la dimensione massima della partizione non è 128M ma 72.5M.\n\nMa c'è ancora un dettaglio su come viene effettuata la partizione dei dati.\n\n- Il primo file viene caricato nella prima partizione, la dimensione dell partizione è 25M, il \"costo\" di aprire un altro file è 4M, quindi viene calcolato che la partizione è (25+4)M:\n`dimensione_corrente_della_partizione=29M`\n\n- Se la somma dimensione_corrente_della_partizione + dimensione_prossimo_file è maggiore del maxSplitSize, allora il prossimo file verrà caricato in una nuova partizione. Altimenti il file fiene caricato nella partizione corrente. Iterando questa logica vengono caricati tutti i file\n- Si noti che per **ogni** file caricato nella partizione viene aggiunto un openCostInBytes\n\nAlla fine le partizioni sono occupate come sotto:\n\n|---|---|---|---|---|\n|Partizione 1 => | file1= 25MB |openCostInBytes = 4M| file2= 25MB |openCostInBytes = 4M|\n|Partizione 2 => | file3= 25MB |openCostInBytes = 4M| file4= 25MB |openCostInBytes = 4M| \n|Partizione 3 => | file5= 25MB |openCostInBytes = 4M| file6= 25MB |openCostInBytes = 4M|\n|Partizione 4 => | file7= 25MB |openCostInBytes = 4M| file8= 25MB |openCostInBytes = 4M|\n|Partizione 5 => | file9= 25MB |openCostInBytes = 4M| file10= 25MB |openCostInBytes = 4M|\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-07T11:48:34+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Ottengo che i 250MB sono suddivisi in 5 partizioni. Non è una suddivisione ottimale avendo 4 cores. Vuol dire che mentre i primi 4 tasks saranno finiti quasi contemporaneamente, poi rimarrà il quinto task da eseguire, quindi un solo core impegnato a lavorare e altri 3 in idle!</p>\n<p>Perché ho questa suddivisione? Perché Spark calcola una dimensione massima della partizione usando il codice sotto:</p>\n<pre><code>val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)\nval openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)\nval defaultParallelism = Math.max(sc.defaultParallelism, minPartitions)\nval files = listStatus(context).asScala\nval totalBytes = files.filterNot(_.isDirectory).map(_.getLen + openCostInBytes).sum\nval bytesPerCore = totalBytes / defaultParallelism\nval maxSplitSize = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))\n</code></pre>\n<p>Nel nostro caso ho:</p>\n<p>defaultMaxSplitBytes = 128M &lt;- default<br />\nopenCostInBytes = 4M        &lt;- default<br />\ndefaultParallelism = 4<br />\ntotalBytes = 10*(25M+4M) = 290M<br />\nbytesPerCore = 250M/4 = 72.5M<br />\nmaxSplitSize = min(128M, max(4M, 72.5M)) = 72.5M</p>\n<p>Quindi la dimensione massima della partizione non è 128M ma 72.5M.</p>\n<p>Ma c&rsquo;è ancora un dettaglio su come viene effettuata la partizione dei dati.</p>\n<ul>\n<li>\n<p>Il primo file viene caricato nella prima partizione, la dimensione dell partizione è 25M, il &ldquo;costo&rdquo; di aprire un altro file è 4M, quindi viene calcolato che la partizione è (25+4)M:<br />\n<code>dimensione_corrente_della_partizione=29M</code></p>\n</li>\n<li>\n<p>Se la somma dimensione_corrente_della_partizione + dimensione_prossimo_file è maggiore del maxSplitSize, allora il prossimo file verrà caricato in una nuova partizione. Altimenti il file fiene caricato nella partizione corrente. Iterando questa logica vengono caricati tutti i file</p>\n</li>\n<li>\n<p>Si noti che per <strong>ogni</strong> file caricato nella partizione viene aggiunto un openCostInBytes</p>\n</li>\n</ul>\n<p>Alla fine le partizioni sono occupate come sotto:</p>\n<table>\n<thead></thead>\n<tbody>\n<tr><td>Partizione 1 =&gt;</td><td>file1= 25MB</td><td>openCostInBytes = 4M</td><td>file2= 25MB</td><td>openCostInBytes = 4M</td></tr>\n<tr><td>Partizione 2 =&gt;</td><td>file3= 25MB</td><td>openCostInBytes = 4M</td><td>file4= 25MB</td><td>openCostInBytes = 4M</td></tr>\n<tr><td>Partizione 3 =&gt;</td><td>file5= 25MB</td><td>openCostInBytes = 4M</td><td>file6= 25MB</td><td>openCostInBytes = 4M</td></tr>\n<tr><td>Partizione 4 =&gt;</td><td>file7= 25MB</td><td>openCostInBytes = 4M</td><td>file8= 25MB</td><td>openCostInBytes = 4M</td></tr>\n<tr><td>Partizione 5 =&gt;</td><td>file9= 25MB</td><td>openCostInBytes = 4M</td><td>file10= 25MB</td><td>openCostInBytes = 4M</td></tr>\n</tbody>\n</table>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623060822308_1659401874",
      "id": "paragraph_1623060822308_1659401874",
      "dateCreated": "2021-06-07T10:13:42+0000",
      "dateStarted": "2021-06-07T11:48:34+0000",
      "dateFinished": "2021-06-07T11:48:34+0000",
      "status": "FINISHED",
      "$$hashKey": "object:198"
    },
    {
      "text": "%md\n![apache-spark-partitions/](https://www.1week4.com/wp-content/uploads/2021/06/apache-spark-partitions.jpg)",
      "user": "anonymous",
      "dateUpdated": "2021-06-07T10:13:19+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><img src=\"https://www.1week4.com/wp-content/uploads/2021/06/apache-spark-partitions.jpg\" alt=\"apache-spark-partitions/\" /></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623015499108_893863790",
      "id": "paragraph_1623015499108_893863790",
      "dateCreated": "2021-06-06T21:38:19+0000",
      "dateStarted": "2021-06-07T10:13:19+0000",
      "dateFinished": "2021-06-07T10:13:19+0000",
      "status": "FINISHED",
      "$$hashKey": "object:199"
    },
    {
      "text": "%md\n### Shuffle dei dati\nAbbiamo visto come fissare il numero delle partizioni quando una struttura di **dati in ingresso viene generata**.\n\nMa il numero delle partizioni non è fisso dall'inizio alla fine della applicazione, infatti esso cambia se triggeriamo uno shuffle dei dati, ovvero se utilizziamo una trasformazione che implica uno shuffle (**wide transformation**).\n\nCome si può vedere sotto il numero delle partizioni dopo le operazioni **groupBy()** e **max()** è diventato 200. Perchè?",
      "user": "anonymous",
      "dateUpdated": "2021-06-07T11:48:20+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Shuffle dei dati</h3>\n<p>Abbiamo visto come fissare il numero delle partizioni quando una struttura di <strong>dati in ingresso viene generata</strong>.</p>\n<p>Ma il numero delle partizioni non è fisso dall&rsquo;inizio alla fine della applicazione, infatti esso cambia se triggeriamo uno shuffle dei dati, ovvero se utilizziamo una trasformazione che implica uno shuffle (<strong>wide transformation</strong>).</p>\n<p>Come si può vedere sotto il numero delle partizioni dopo le operazioni <strong>groupBy()</strong> e <strong>max()</strong> è diventato 200. Perchè?</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623014714688_560224217",
      "id": "paragraph_1623014714688_560224217",
      "dateCreated": "2021-06-06T21:25:14+0000",
      "dateStarted": "2021-06-07T11:48:20+0000",
      "dateFinished": "2021-06-07T11:48:20+0000",
      "status": "FINISHED",
      "$$hashKey": "object:203"
    },
    {
      "text": "val newdf = df.groupBy(\"country\").max(\"estimated_generation_gwh\")\nnewdf.rdd.getNumPartitions",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T14:51:16+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "newdf: org.apache.spark.sql.DataFrame = [country: string, max(estimated_generation_gwh): double]\r\nres31: Int = 200\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076037_1701120640",
      "id": "20191211-145314_1450041834",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "status": "READY",
      "$$hashKey": "object:204"
    },
    {
      "text": "%md\nCome si vede sopra dopo le operazioni **groupBy()** e **aggregate** ho 200 partizioni, questo perchè il parametro **spark.sql.shuffle.partitions** è settato a 200. Ricordiamo che quest'ultimo fissa il numero di partizioni create da un'operazione che1 implica uno shuffle, come ad esempio **groupBy**.\n\nNella UI di Spark si può vedere come è cambiato il numero di task e conseguentemente di partizioni, da 1 a 200.\n![spark ui stages](https://www.1week4.com/wp-content/uploads/2019/12/spark-ui-stages1.jpg)",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T20:55:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Come si vede sopra dopo le operazioni <strong>groupBy()</strong> e <strong>aggregate</strong> ho 200 partizioni, questo perchè il parametro <strong>spark.sql.shuffle.partitions</strong> è settato a 200. Ricordiamo che quest&rsquo;ultimo fissa il numero di partizioni create da un&rsquo;operazione che1 implica uno shuffle, come ad esempio <strong>groupBy</strong>.</p>\n<p>Nella UI di Spark si può vedere come è cambiato il numero di task e conseguentemente di partizioni, da 1 a 200.<br />\n<img src=\"https://www.1week4.com/wp-content/uploads/2019/12/spark-ui-stages1.jpg\" alt=\"spark ui stages\" /></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076039_1418405409",
      "id": "20191211-155312_1902930156",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T20:55:51+0000",
      "dateFinished": "2021-06-06T20:55:51+0000",
      "status": "FINISHED",
      "$$hashKey": "object:205"
    },
    {
      "text": "%md\nI parametri interessanti sono riassunti nella tabella\n\n\n| settaggio | default | spiegazione |\n|---------|---------|-------------|\n|spark.driver.cores|1|setta il numero di cores usati dal driver|\n|spark.executor.cores|1 in modo YARN. Tutti i cores disponibili in modo Spark standalone e con Mesos | il numero di cores per ogni executor in fase di lancio del nodo|\n|spark.task.cpu | 1 | numero di task da allocare per ciascun core|\n|spark.cores.max| non settato | il massimo numero di cores del cluster (totale, non per ogni executor) da assegnare **all'applicazione**. Solo per Spark standalone e Mesos \"coarse-grained\" |\n|spark.deploy.defaultCores| illimitato | numero default di cores del cluster da assegnare in Spark standalone se non è assegnato spark.cores.max. Si può usare per limitare il numero di cores da assegnare a diversi user di un cluster condiviso|\n|spark.default.parallelism <br>sc.defaultParallelism| numero di cores | numero di default delle partizioni generate da operazioni quali *join* e *reduceByKey* e *parallelize*|\n|spark.sql.files.maxPartitionBytes|128M|dimensione massima della singola partizione|\n\n#### Esempio\nSupponendo di essere in **cluster mode**, settando: \n- spark.cores.max = 5\n- spark.driver.cores = 1\n- spark.executor.cores = 2\n\navrò un numero di executors pari a\n\n`executors = (spark.cores.max - spark.driver.cores)/spark.executor.cores = 2`\n\n\nInfine, per controllare i settaggi del cluster Spark:",
      "user": "anonymous",
      "dateUpdated": "2021-06-07T10:16:32+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>I parametri interessanti sono riassunti nella tabella</p>\n<table>\n<thead>\n<tr><th>settaggio</th><th>default</th><th>spiegazione</th></tr>\n</thead>\n<tbody>\n<tr><td>spark.driver.cores</td><td>1</td><td>setta il numero di cores usati dal driver</td></tr>\n<tr><td>spark.executor.cores</td><td>1 in modo YARN. Tutti i cores disponibili in modo Spark standalone e con Mesos</td><td>il numero di cores per ogni executor in fase di lancio del nodo</td></tr>\n<tr><td>spark.task.cpu</td><td>1</td><td>numero di task da allocare per ciascun core</td></tr>\n<tr><td>spark.cores.max</td><td>non settato</td><td>il massimo numero di cores del cluster (totale, non per ogni executor) da assegnare <strong>all&rsquo;applicazione</strong>. Solo per Spark standalone e Mesos &ldquo;coarse-grained&rdquo;</td></tr>\n<tr><td>spark.deploy.defaultCores</td><td>illimitato</td><td>numero default di cores del cluster da assegnare in Spark standalone se non è assegnato spark.cores.max. Si può usare per limitare il numero di cores da assegnare a diversi user di un cluster condiviso</td></tr>\n<tr><td>spark.default.parallelism <br>sc.defaultParallelism</td><td>numero di cores</td><td>numero di default delle partizioni generate da operazioni quali <em>join</em> e <em>reduceByKey</em> e <em>parallelize</em></td></tr>\n<tr><td>spark.sql.files.maxPartitionBytes</td><td>128M</td><td>dimensione massima della singola partizione</td></tr>\n</tbody>\n</table>\n<h4>Esempio</h4>\n<p>Supponendo di essere in <strong>cluster mode</strong>, settando:</p>\n<ul>\n<li>spark.cores.max = 5</li>\n<li>spark.driver.cores = 1</li>\n<li>spark.executor.cores = 2</li>\n</ul>\n<p>avrò un numero di executors pari a</p>\n<p><code>executors = (spark.cores.max - spark.driver.cores)/spark.executor.cores = 2</code></p>\n<p>Infine, per controllare i settaggi del cluster Spark:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076041_219773557",
      "id": "20200104-234415_320317775",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-07T10:16:32+0000",
      "dateFinished": "2021-06-07T10:16:32+0000",
      "status": "FINISHED",
      "$$hashKey": "object:206"
    },
    {
      "text": "spark.conf.getAll.foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T21:04:39+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(spark.driver.host,c1801f224934)\n(spark.driver.port,45177)\n(spark.app.name,c9595f99-da7b-4d42-a424-1f3fb172f850)\n(spark.driver.memory,1g)\n(spark.executor.instances,2)\n(spark.executor.id,driver)\n(spark.driver.cores,1)\n(spark.webui.yarn.useProxy,false)\n(spark.master,local[*])\n(spark.executor.memory,1g)\n(spark.executor.cores,1)\n(spark.app.id,local-1622999305790)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076044_741384797",
      "id": "20191211-130103_1686291439",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T21:04:39+0000",
      "dateFinished": "2021-06-06T21:04:41+0000",
      "status": "FINISHED",
      "$$hashKey": "object:207"
    },
    {
      "text": "%md\nRiferimenti\n1. [Spark Github](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L799)\n2. https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297",
      "user": "anonymous",
      "dateUpdated": "2021-06-06T21:06:15+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Riferimenti</p>\n<ol>\n<li><a href=\"https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L799\">Spark Github</a></li>\n<li><a href=\"https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297\">https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297</a></li>\n</ol>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1622991076042_1484912063",
      "id": "20191211-130043_314232691",
      "dateCreated": "2021-06-06T14:51:16+0000",
      "dateStarted": "2021-06-06T21:06:15+0000",
      "dateFinished": "2021-06-06T21:06:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:208"
    }
  ],
  "name": "Partitions",
  "id": "2G7C1H2V9",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Partitions"
}