{"paragraphs":[{"text":"%md\n## Configurazione del cluster\nImportiamo la SparkSesssion, il punto di accesso al cluster per tutte le attività di Spark.","user":"anonymous","dateUpdated":"2019-11-27T14:12:58+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Configurazione del cluster</h2>\n<p>Importiamo la SparkSesssion, il punto di accesso al cluster per tutte le attività di Spark.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574674191825_-1307089363","id":"20191125-102951_96032273","dateCreated":"2019-11-25T10:29:51+0100","dateStarted":"2019-11-27T14:12:58+0100","dateFinished":"2019-11-27T14:13:04+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:368"},{"text":"import org.apache.spark.sql.SparkSession","user":"anonymous","dateUpdated":"2019-11-27T14:13:04+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.SparkSession\n"}]},"apps":[],"jobName":"paragraph_1574674146818_2070972959","id":"20191125-102906_1597767286","dateCreated":"2019-11-25T10:29:06+0100","dateStarted":"2019-11-27T14:13:04+0100","dateFinished":"2019-11-27T14:13:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:369"},{"text":"%md\nDefinisco e configuro una SparkSession, ho bisogno del metodo **builder**, assegno un nome alla applicazione Spark usando **appName**, definisco che il cluster funziona in modo locale. Infine uso getOrCreate, cioè crea una nuova SparkSession o usa una già esistente.  ","user":"anonymous","dateUpdated":"2019-11-27T14:13:15+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Definisco e configuro una SparkSession, ho bisogno del metodo <strong>builder</strong>, assegno un nome alla applicazione Spark usando <strong>appName</strong>, definisco che il cluster funziona in modo locale. Infine uso getOrCreate, cioè crea una nuova SparkSession o usa una già esistente.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574674841771_1876021451","id":"20191125-104041_1369871670","dateCreated":"2019-11-25T10:40:41+0100","dateStarted":"2019-11-27T14:13:15+0100","dateFinished":"2019-11-27T14:13:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:370"},{"text":"val spark = SparkSession.builder.appName(\"MyApp1\").master(\"local\").getOrCreate","user":"anonymous","dateUpdated":"2019-11-27T14:13:16+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@22161fb9\n"}]},"apps":[],"jobName":"paragraph_1574674187077_273060964","id":"20191125-102947_64480312","dateCreated":"2019-11-25T10:29:47+0100","dateStarted":"2019-11-27T14:13:16+0100","dateFinished":"2019-11-27T14:13:17+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:371"},{"text":"%md\nSe voglio vedere tutte le proprietà di configurazione posso usare **spark.conf.getAll** che restituisce un **map** di coppie key,value, ciscuna coppia contenente una proprietà della configurazione del cluster.","user":"anonymous","dateUpdated":"2019-11-27T14:13:17+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Se voglio vedere tutte le proprietà di configurazione posso usare <strong>spark.conf.getAll</strong> che restituisce un <strong>map</strong> di coppie key,value, ciscuna coppia contenente una proprietà della configurazione del cluster.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574675047297_-619468556","id":"20191125-104407_1554911329","dateCreated":"2019-11-25T10:44:07+0100","dateStarted":"2019-11-27T14:13:17+0100","dateFinished":"2019-11-27T14:13:17+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:372"},{"text":"spark.conf.getAll","user":"anonymous","dateUpdated":"2019-11-27T14:13:17+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res1: Map[String,String] = Map(zeppelin.pyspark.python -> python, spark.driver.host -> 10.0.75.1, zeppelin.dep.localrepo -> local-repo, zeppelin.spark.sql.stacktrace -> false, spark.driver.port -> 53386, master -> local[*], spark.repl.class.uri -> spark://10.0.75.1:53386/classes, zeppelin.spark.useHiveContext -> true, spark.repl.class.outputDir -> C:\\Users\\home\\AppData\\Local\\Temp\\spark8113731899773902274, zeppelin.spark.sql.interpolation -> false, zeppelin.spark.importImplicit -> true, zeppelin.interpreter.output.limit -> 102400, spark.app.name -> MyApp1, zeppelin.R.cmd -> R, zeppelin.spark.maxResult -> 1000, zeppelin.pyspark.useIPython -> true, zeppelin.spark.concurrentSQL -> false, zeppelin.spark.enableSupportedVersionCheck -> true, zeppelin.spark.printREPLOutput -> true, zeppelin.dep..."}]},"apps":[],"jobName":"paragraph_1574680426740_-576525551","id":"20191125-121346_1893066719","dateCreated":"2019-11-25T12:13:46+0100","dateStarted":"2019-11-27T14:13:17+0100","dateFinished":"2019-11-27T14:13:17+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:373"},{"text":"%md\nPer visualizzare meglio i field del **map**, posso usare la funzione **foreach** con argomento la funzione **println**.\nDa ricordare che in **Scala** esistono le funzioni di ordine superiore, cioè funzioni che accettano come argomento un altra funzione.","user":"anonymous","dateUpdated":"2019-11-27T14:13:17+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Per visualizzare meglio i field del <strong>map</strong>, posso usare la funzione <strong>foreach</strong> con argomento la funzione <strong>println</strong>.<br/>Da ricordare che in <strong>Scala</strong> esistono le funzioni di ordine superiore, cioè funzioni che accettano come argomento un altra funzione.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574680458046_-1385260751","id":"20191125-121418_1278282582","dateCreated":"2019-11-25T12:14:18+0100","dateStarted":"2019-11-27T14:13:18+0100","dateFinished":"2019-11-27T14:13:18+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:374"},{"text":"spark.conf.getAll.foreach(println)","user":"anonymous","dateUpdated":"2019-11-27T14:13:18+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(zeppelin.pyspark.python,python)\r\n(spark.driver.host,10.0.75.1)\r\n(zeppelin.dep.localrepo,local-repo)\r\n(zeppelin.spark.sql.stacktrace,false)\r\n(spark.driver.port,53386)\r\n(master,local[*])\r\n(spark.repl.class.uri,spark://10.0.75.1:53386/classes)\r\n(zeppelin.spark.useHiveContext,true)\r\n(spark.repl.class.outputDir,C:\\Users\\home\\AppData\\Local\\Temp\\spark8113731899773902274)\r\n(zeppelin.spark.sql.interpolation,false)\r\n(zeppelin.spark.importImplicit,true)\r\n(zeppelin.interpreter.output.limit,102400)\r\n(spark.app.name,MyApp1)\r\n(zeppelin.R.cmd,R)\r\n(zeppelin.spark.maxResult,1000)\r\n(zeppelin.pyspark.useIPython,true)\r\n(zeppelin.spark.concurrentSQL,false)\r\n(zeppelin.spark.enableSupportedVersionCheck,true)\r\n(zeppelin.spark.printREPLOutput,true)\r\n(zeppelin.dep.additionalRemoteRepository,spark-packages,http://dl.bintray.com/spark-packages/maven,false;)\r\n(spark.executor.id,driver)\r\n(zeppelin.spark.useNew,true)\r\n(spark.useHiveContext,true)\r\n(spark.master,local)\r\n(zeppelin.R.image.width,100%)\r\n(zeppelin.spark.ui.hidden,false)\r\n(zeppelin.interpreter.localRepo,C:\\zeppelin-0.8.2-bin-all/local-repo/spark)\r\n(zeppelin.R.render.options,out.format = 'html', comment = NA, echo = FALSE, results = 'asis', message = F, warning = F, fig.retina = 2)\r\n(zeppelin.interpreter.max.poolsize,10)\r\n(spark.app.id,local-1574860392257)\r\n(zeppelin.R.knitr,true)\r\n"}]},"apps":[],"jobName":"paragraph_1574674480175_1740071417","id":"20191125-103440_299176729","dateCreated":"2019-11-25T10:34:40+0100","dateStarted":"2019-11-27T14:13:18+0100","dateFinished":"2019-11-27T14:13:18+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:375"},{"text":"%md\n## Importazione dei dati\n\nI dati sono in un file csv. Questi possono essere importati comodamente in un **DataFrame** o in un **RDD**, dipendentemente a quale struttura voglio usare.\n\nPoichè sono dati numerici in formato tabellare è sicuramente più comodo usare i DataFrame\nPrima importo il package Dataframe, e poi importiamo il file csv in un Dataframe nuovo di zecca.\n\nIl DataFrame di Spark è molto simile al DataFrame di Numpy per Python. Si tratta di un insieme di dati in formato tabella con le colonne aventi dei nomi per identificarle .\nCon l'opzione **(\"Header\",\"true\")** comunico che la prima riga ha la funzione di *Header* per cui le colonne del DataFrame verranno nominate usando l'header.\nIn alternativa spark avrebbe assegnato i nomi generici _c0, _c1, _c2... e la prima riga sarebbe finita tra i dati.","user":"anonymous","dateUpdated":"2019-11-28T12:42:02+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Importazione dei dati</h2>\n<p>I dati sono in un file csv. Questi possono essere importati comodamente in un <strong>DataFrame</strong> o in un <strong>RDD</strong>, dipendentemente a quale struttura voglio usare.</p>\n<p>Poichè sono dati numerici in formato tabellare è sicuramente più comodo usare i DataFrame<br/>Prima importo il package Dataframe, e poi importiamo il file csv in un Dataframe nuovo di zecca.</p>\n<p>Il DataFrame di Spark è molto simile al DataFrame di Numpy per Python. Si tratta di un insieme di dati in formato tabella con le colonne aventi dei nomi per identificarle .<br/>Con l&rsquo;opzione <strong>(&ldquo;Header&rdquo;,&ldquo;true&rdquo;)</strong> comunico che la prima riga ha la funzione di <em>Header</em> per cui le colonne del DataFrame verranno nominate usando l&rsquo;header.<br/>In alternativa spark avrebbe assegnato i nomi generici _c0, _c1, _c2&hellip; e la prima riga sarebbe finita tra i dati.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574674480043_-832882968","id":"20191125-103440_1520293990","dateCreated":"2019-11-25T10:34:40+0100","dateStarted":"2019-11-28T12:42:02+0100","dateFinished":"2019-11-28T12:42:08+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:376"},{"text":"import org.apache.spark.sql.DataFrame\n\nval df = spark.read.option(\"Header\",\"true\").csv(\"exampl1.csv\")","user":"anonymous","dateUpdated":"2019-11-27T14:13:18+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.DataFrame\r\ndf: org.apache.spark.sql.DataFrame = [eta: string, amici: string]\n"}]},"apps":[],"jobName":"paragraph_1574683388711_-629324192","id":"20191125-130308_110147485","dateCreated":"2019-11-25T13:03:08+0100","dateStarted":"2019-11-27T14:13:18+0100","dateFinished":"2019-11-27T14:13:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:377"},{"text":"%md\nCome possiamo vedere sopra Spark mi dice che l'oggetto DataFrame ha 2 colonne di nome rispettivamente *eta* e *amici*, e che tutti e due i campi contengono stringhe.\nPoiché in realtà i campi del file sono valori interi, posso fare in modo che nell'importazione i valori siano interpretati correttamente come valori numerici interi.\nPer fare ciò uso l'opzione (\"inferSchema\",\"true\") in cui comunico al sistema di inferire il tipo dei valori nelle colonne.","user":"anonymous","dateUpdated":"2019-11-27T14:13:20+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Come possiamo vedere sopra Spark mi dice che l&rsquo;oggetto DataFrame ha 2 colonne di nome rispettivamente <em>eta</em> e <em>amici</em>, e che tutti e due i campi contengono stringhe.<br/>Poiché in realtà i campi del file sono valori interi, posso fare in modo che nell&rsquo;importazione i valori siano interpretati correttamente come valori numerici interi.<br/>Per fare ciò uso l&rsquo;opzione (&ldquo;inferSchema&rdquo;,&ldquo;true&rdquo;) in cui comunico al sistema di inferire il tipo dei valori nelle colonne.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574683416989_41940561","id":"20191125-130336_1924810457","dateCreated":"2019-11-25T13:03:36+0100","dateStarted":"2019-11-27T14:13:20+0100","dateFinished":"2019-11-27T14:13:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:378"},{"text":"val df = spark.read.option(\"Header\",\"true\").option(\"inferSchema\",\"true\").csv(\"exampl1.csv\")","user":"anonymous","dateUpdated":"2019-11-27T14:13:21+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [eta: int, amici: int]\n"}]},"apps":[],"jobName":"paragraph_1574684257996_1456929141","id":"20191125-131737_1111301496","dateCreated":"2019-11-25T13:17:37+0100","dateStarted":"2019-11-27T14:13:21+0100","dateFinished":"2019-11-27T14:13:21+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:379"},{"text":"%md\nI nomi delle colonne li conosco, ma se voglio verificare posso usare il metodo **columns**","user":"anonymous","dateUpdated":"2019-11-28T12:44:01+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>I nomi delle colonne li conosco, ma se voglio verificare posso usare il metodo <strong>columns</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574683918819_432639864","id":"20191125-131158_1574211598","dateCreated":"2019-11-25T13:11:58+0100","dateStarted":"2019-11-28T12:44:01+0100","dateFinished":"2019-11-28T12:44:01+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:380"},{"text":"df.columns","user":"anonymous","dateUpdated":"2019-11-27T14:13:21+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res3: Array[String] = Array(eta, amici)\n"}]},"apps":[],"jobName":"paragraph_1574683974350_-2047343485","id":"20191125-131254_1091874570","dateCreated":"2019-11-25T13:12:54+0100","dateStarted":"2019-11-27T14:13:21+0100","dateFinished":"2019-11-27T14:13:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:381"},{"text":"%md\nAdesso che i dati sono correttamente interpretati come numerici vediamo la prime 5 righe del Dataframe, con il metodo **head**","user":"anonymous","dateUpdated":"2019-11-27T14:13:22+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Adesso che i dati sono correttamente interpretati come numerici vediamo la prime 5 righe del Dataframe, con il metodo <strong>head</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574684252628_-1037328671","id":"20191125-131732_633881925","dateCreated":"2019-11-25T13:17:32+0100","dateStarted":"2019-11-27T14:13:22+0100","dateFinished":"2019-11-27T14:13:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:382"},{"text":"df.head(5)","user":"anonymous","dateUpdated":"2019-11-27T14:13:22+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res4: Array[org.apache.spark.sql.Row] = Array([44,11], [5,190], [44,123], [9,111], [18,238])\n"}]},"apps":[],"jobName":"paragraph_1574683904555_-2103924770","id":"20191125-131144_161534002","dateCreated":"2019-11-25T13:11:44+0100","dateStarted":"2019-11-27T14:13:22+0100","dateFinished":"2019-11-27T14:13:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:383"},{"text":"%md\nVisualizziamo lo schema del DataFrame con **printSchema**, le informazioni che ottengo sono il nome e il tipo di ciascun campo:","user":"anonymous","dateUpdated":"2019-11-28T12:46:28+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Visualizziamo lo schema del DataFrame con <strong>printSchema</strong>, le informazioni che ottengo sono il nome e il tipo di ciascun campo:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574683971968_-1817435565","id":"20191125-131251_465970265","dateCreated":"2019-11-25T13:12:51+0100","dateStarted":"2019-11-28T12:46:28+0100","dateFinished":"2019-11-28T12:46:28+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:384"},{"text":"df.printSchema","user":"anonymous","dateUpdated":"2019-11-27T14:13:22+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- eta: integer (nullable = true)\n |-- amici: integer (nullable = true)\n\r\n"}]},"apps":[],"jobName":"paragraph_1574684617896_-327163030","id":"20191125-132337_910905477","dateCreated":"2019-11-25T13:23:37+0100","dateStarted":"2019-11-27T14:13:22+0100","dateFinished":"2019-11-27T14:13:23+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:385"},{"text":"%md\nIl Dataframe ha un numero di elementi pari a","user":"anonymous","dateUpdated":"2019-11-28T12:47:30+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Il Dataframe ha un numero di elementi pari a</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574684852959_616214020","id":"20191125-132732_1328688072","dateCreated":"2019-11-25T13:27:32+0100","dateStarted":"2019-11-28T12:47:30+0100","dateFinished":"2019-11-28T12:47:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:386"},{"text":"df.count","user":"anonymous","dateUpdated":"2019-11-27T14:13:23+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res6: Long = 499\n"}]},"apps":[],"jobName":"paragraph_1574684879292_-459614457","id":"20191125-132759_1528300593","dateCreated":"2019-11-25T13:27:59+0100","dateStarted":"2019-11-27T14:13:23+0100","dateFinished":"2019-11-27T14:13:23+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:387"},{"text":"%md\n## Statistica descrittiva\nCon questo termine si indica una analisi statistica da fare sui dati a disposizione.\nPer il momento usiamo semplicemente la funzione **describe()**","user":"anonymous","dateUpdated":"2019-11-28T12:48:38+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Statistica descrittiva</h2>\n<p>Con questo termine si indica una analisi statistica da fare sui dati a disposizione.<br/>Per il momento usiamo semplicemente la funzione <strong>describe()</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574689906709_1579955579","id":"20191125-145146_1128384758","dateCreated":"2019-11-25T14:51:46+0100","dateStarted":"2019-11-28T12:48:38+0100","dateFinished":"2019-11-28T12:48:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:388"},{"text":"df.describe().show","user":"anonymous","dateUpdated":"2019-11-27T14:13:23+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+------------------+------------------+\n|summary|               eta|             amici|\n+-------+------------------+------------------+\n|  count|               499|               499|\n|   mean| 34.78156312625251| 258.3627254509018|\n| stddev|20.532332734289348|136.55329326307273|\n|    min|                 0|                 1|\n|    max|                69|               498|\n+-------+------------------+------------------+\n\r\n"}]},"apps":[],"jobName":"paragraph_1574690284293_-959025520","id":"20191125-145804_1966092883","dateCreated":"2019-11-25T14:58:04+0100","dateStarted":"2019-11-27T14:13:24+0100","dateFinished":"2019-11-27T14:13:24+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:389"},{"text":"%md\n## Analisi\nIl problema è vedere quanti amici hanno in media le persone di un'età definita x. E questo per ogni età x.\n\nFacciamo un'aggregazione dei dati usando come chiave l'età.\n\nPer questo ci viene in aiuto la funzione **groupBy**","user":"anonymous","dateUpdated":"2019-11-27T14:13:24+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574684635650_-1634697721","id":"20191125-132355_452999482","dateCreated":"2019-11-25T13:23:55+0100","dateStarted":"2019-11-27T14:13:24+0100","dateFinished":"2019-11-27T14:13:24+0100","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:390"},{"text":"df.groupBy(\"eta\")","user":"anonymous","dateUpdated":"2019-11-27T14:13:24+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res8: org.apache.spark.sql.RelationalGroupedDataset = org.apache.spark.sql.RelationalGroupedDataset@727e4c7d\n"}]},"apps":[],"jobName":"paragraph_1574684972834_1923654664","id":"20191125-132932_1629992967","dateCreated":"2019-11-25T13:29:32+0100","dateStarted":"2019-11-27T14:13:24+0100","dateFinished":"2019-11-27T14:13:25+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:391"},{"text":"%md\nIl metodo groupBy fornisce come risultato un **RelationalGroupedDataset**, dopodiché per utilizzarlo devo specificare come aggregare i valori corrispondenti ad ogni *key*. Per scoprire quali sono i metodi che possiamo applicare a questa entità vado sulla pagina della documentazione di Spark relativa al [RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.RelationalGroupedDataset). \nScopro che posso usare diverse funzioni, tra cui **agg**, **max**, **min**, **pivot**. Proviamone alcune.\n\nLa funzione min è così definita:\n    def min(colNames: String*): DataFrame","user":"anonymous","dateUpdated":"2019-11-27T14:13:25+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Il metodo groupBy fornisce come risultato un <strong>RelationalGroupedDataset</strong>, dopodiché per utilizzarlo devo specificare come aggregare i valori corrispondenti ad ogni <em>key</em>. Per scoprire quali sono i metodi che possiamo applicare a questa entità vado sulla pagina della documentazione di Spark relativa al <a href=\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.RelationalGroupedDataset\">RelationalGroupedDataset</a>.<br/>Scopro che posso usare diverse funzioni, tra cui <strong>agg</strong>, <strong>max</strong>, <strong>min</strong>, <strong>pivot</strong>. Proviamone alcune.</p>\n<p>La funzione min è così definita:<br/> def min(colNames: String*): DataFrame</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574684984419_-762815662","id":"20191125-132944_1883418827","dateCreated":"2019-11-25T13:29:44+0100","dateStarted":"2019-11-27T14:13:25+0100","dateFinished":"2019-11-27T14:13:25+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:392"},{"text":"df.groupBy(\"eta\").min(\"amici\").show(5)","user":"anonymous","dateUpdated":"2019-11-27T14:13:25+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----------+\n|eta|min(amici)|\n+---+----------+\n| 31|        73|\n| 65|       141|\n| 53|        42|\n| 34|        53|\n| 28|        36|\n+---+----------+\nonly showing top 5 rows\n\r\n"}]},"apps":[],"jobName":"paragraph_1574685360866_507111509","id":"20191125-133600_323730237","dateCreated":"2019-11-25T13:36:00+0100","dateStarted":"2019-11-27T14:13:25+0100","dateFinished":"2019-11-27T14:13:26+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:393"},{"text":"%md\nAdesso voglio contare quanti elementi ci sono nel DataFrame con la stessa key, così scopro per esempio che ci sono esattamente 5 31enni.","user":"anonymous","dateUpdated":"2019-11-27T14:13:26+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Adesso voglio contare quanti elementi ci sono nel DataFrame con la stessa key, così scopro per esempio che ci sono esattamente 5 31enni.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574719568507_1632380477","id":"20191125-230608_1242145458","dateCreated":"2019-11-25T23:06:08+0100","dateStarted":"2019-11-27T14:13:26+0100","dateFinished":"2019-11-27T14:13:26+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:394"},{"text":"df.groupBy(\"eta\").count().show(5)","user":"anonymous","dateUpdated":"2019-11-27T14:13:26+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|eta|count|\n+---+-----+\n| 31|    5|\n| 65|    9|\n| 53|    8|\n| 34|   10|\n| 28|    8|\n+---+-----+\nonly showing top 5 rows\n\r\n"}]},"apps":[],"jobName":"paragraph_1574685373281_123197630","id":"20191125-133613_413840384","dateCreated":"2019-11-25T13:36:13+0100","dateStarted":"2019-11-27T14:13:26+0100","dateFinished":"2019-11-27T14:13:27+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:395"},{"text":"%md\nPer la funzione **agg** devo specificare come argomento/i la/e funzione/i e su quale colonna ciascuna funzione deve essere applicata","user":"anonymous","dateUpdated":"2019-11-27T14:13:27+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Per la funzione <strong>agg</strong> devo specificare come argomento/i la/e funzione/i e su quale colonna ciascuna funzione deve essere applicata</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574687171387_2080140370","id":"20191125-140611_954079549","dateCreated":"2019-11-25T14:06:11+0100","dateStarted":"2019-11-27T14:13:27+0100","dateFinished":"2019-11-27T14:13:27+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:396"},{"text":"df.groupBy(\"eta\").agg(count(\"amici\"),min(\"amici\"), max(\"amici\"), mean(\"amici\")).show(5)","user":"anonymous","dateUpdated":"2019-11-27T14:13:27+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+------------+----------+----------+------------------+\n|eta|count(amici)|min(amici)|max(amici)|        avg(amici)|\n+---+------------+----------+----------+------------------+\n| 31|           5|        73|       419|             279.6|\n| 65|           9|       141|       469|328.22222222222223|\n| 53|           8|        42|       489|            322.25|\n| 34|          10|        53|       478|             278.7|\n| 28|           8|        36|       437|             249.0|\n+---+------------+----------+----------+------------------+\nonly showing top 5 rows\n\r\n"}]},"apps":[],"jobName":"paragraph_1574686876191_-1030045581","id":"20191125-140116_2079582200","dateCreated":"2019-11-25T14:01:16+0100","dateStarted":"2019-11-27T14:13:28+0100","dateFinished":"2019-11-27T14:13:29+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:397"},{"text":"%md\nSe volessimo sapere i dati relativi ai 30-enni potrei filtrare preventivamente usando la funzione **filter**","user":"anonymous","dateUpdated":"2019-11-27T14:13:29+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Se volessimo sapere i dati relativi ai 30-enni potrei filtrare preventivamente usando la funzione <strong>filter</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574687023957_2050611993","id":"20191125-140343_1552712998","dateCreated":"2019-11-25T14:03:43+0100","dateStarted":"2019-11-27T14:13:29+0100","dateFinished":"2019-11-27T14:13:29+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:398"},{"text":"df.filter(\"eta == 30\").show","user":"anonymous","dateUpdated":"2019-11-27T14:13:29+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|eta|amici|\n+---+-----+\n| 30|  148|\n| 30|  374|\n| 30|  153|\n| 30|  247|\n| 30|   56|\n| 30|   46|\n| 30|  352|\n+---+-----+\n\r\n"}]},"apps":[],"jobName":"paragraph_1574690584486_-1097583384","id":"20191125-150304_609743018","dateCreated":"2019-11-25T15:03:04+0100","dateStarted":"2019-11-27T14:13:29+0100","dateFinished":"2019-11-27T14:13:29+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:399"},{"text":"%md\nLa funzione **filter()** è del tutto equivalente alla funzione **where()**","user":"anonymous","dateUpdated":"2019-11-28T13:16:20+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>La funzione <strong>filter()</strong> è del tutto equivalente alla funzione <strong>where()</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574690784980_-1550947089","id":"20191125-150624_390466358","dateCreated":"2019-11-25T15:06:24+0100","dateStarted":"2019-11-28T13:16:20+0100","dateFinished":"2019-11-28T13:16:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:400"},{"text":"df.where(\"eta == 30\").show","user":"anonymous","dateUpdated":"2019-11-27T14:13:30+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|eta|amici|\n+---+-----+\n| 30|  148|\n| 30|  374|\n| 30|  153|\n| 30|  247|\n| 30|   56|\n| 30|   46|\n| 30|  352|\n+---+-----+\n\r\n"}]},"apps":[],"jobName":"paragraph_1574690603189_661259164","id":"20191125-150323_1777409381","dateCreated":"2019-11-25T15:03:23+0100","dateStarted":"2019-11-27T14:13:30+0100","dateFinished":"2019-11-27T14:13:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:401"},{"text":"%md\nSe voglio aggregare i dati relativi ai 30 posso usare ancora la funzione **agg**\n","user":"anonymous","dateUpdated":"2019-11-28T13:18:34+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Se voglio aggregare i dati relativi ai 30 posso usare ancora la funzione <strong>agg</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574690670332_-1102619730","id":"20191125-150430_1097001603","dateCreated":"2019-11-25T15:04:30+0100","dateStarted":"2019-11-28T13:18:34+0100","dateFinished":"2019-11-28T13:18:34+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:402"},{"text":"df.filter(\"eta == 30\").agg(count(\"amici\"),min(\"amici\"), max(\"amici\"), mean(\"amici\")).show","user":"anonymous","dateUpdated":"2019-11-27T14:13:30+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+----------+----------+------------------+\n|count(amici)|min(amici)|max(amici)|        avg(amici)|\n+------------+----------+----------+------------------+\n|           7|        46|       374|196.57142857142858|\n+------------+----------+----------+------------------+\n\r\n"}]},"apps":[],"jobName":"paragraph_1574690899003_-351923275","id":"20191125-150819_1600950523","dateCreated":"2019-11-25T15:08:19+0100","dateStarted":"2019-11-27T14:13:30+0100","dateFinished":"2019-11-27T14:13:31+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:403"},{"text":"%md\nCome argomento di filter posso usare gli operatori logici di SQL","user":"anonymous","dateUpdated":"2019-11-28T13:19:39+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Come argomento di filter posso usare gli operatori logici di SQL</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574690921240_-2055593804","id":"20191125-150841_31200003","dateCreated":"2019-11-25T15:08:41+0100","dateStarted":"2019-11-28T13:19:39+0100","dateFinished":"2019-11-28T13:19:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:404"},{"text":"df.filter(\"eta>=30 AND eta<=32\").agg(count(\"amici\"),min(\"amici\"), max(\"amici\"), mean(\"amici\")).show","user":"anonymous","dateUpdated":"2019-11-27T14:13:31+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+----------+----------+------------------+\n|count(amici)|min(amici)|max(amici)|        avg(amici)|\n+------------+----------+----------+------------------+\n|          13|        46|       419|232.84615384615384|\n+------------+----------+----------+------------------+\n\r\n"}]},"apps":[],"jobName":"paragraph_1574691035951_-361650596","id":"20191125-151035_686521832","dateCreated":"2019-11-25T15:10:35+0100","dateStarted":"2019-11-27T14:13:31+0100","dateFinished":"2019-11-27T14:13:31+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:405"},{"text":"%md\nVediamo di calcolare per ogni età il delta tra il numero massimo di amici e il numero minimo.\nNel codice sotto, ho fatto un groupBy(\"eta\"), con funzioni aggreganti **min** e **max**.\n\nDopodiché con il metodo **withColumnRenamed** ho rinominato le due colonne poiché non riesco ad usare i nomi di default (min(amici) e max(amici)).\n\nInfine con la funzione **select** ho selezionato le colonne che mi interessano e ho definito una nuova colonna come ('maxAmici - 'minAmici). \n\nSi noti che ho usato due diverse notazioni possibili per indicare il nome della colonna: **'nome-colonna**  oppure  **\"nome-colonna\"**.","user":"anonymous","dateUpdated":"2019-11-27T14:13:31+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Vediamo di calcolare per ogni età il delta tra il numero massimo di amici e il numero minimo.<br/>Nel codice sotto, ho fatto un groupBy(&ldquo;eta&rdquo;), con funzioni aggreganti <strong>min</strong> e <strong>max</strong>.</p>\n<p>Dopodiché con il metodo <strong>withColumnRenamed</strong> ho rinominato le due colonne poiché non riesco ad usare i nomi di default (min(amici) e max(amici)).</p>\n<p>Infine con la funzione <strong>select</strong> ho selezionato le colonne che mi interessano e ho definito una nuova colonna come (&rsquo;maxAmici - &rsquo;minAmici). </p>\n<p>Si noti che ho usato due diverse notazioni possibili per indicare il nome della colonna: <strong>&rsquo;nome-colonna</strong> oppure <strong>&ldquo;nome-colonna&rdquo;</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574692870970_-1722637644","id":"20191125-154110_117346726","dateCreated":"2019-11-25T15:41:10+0100","dateStarted":"2019-11-27T14:13:31+0100","dateFinished":"2019-11-27T14:13:31+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:406"},{"text":"val df2 = df.groupBy(\"eta\").agg(min('amici), max('amici))\n\nval df3 = df2.withColumnRenamed(\"min(amici)\", \"minAmici\").withColumnRenamed(\"max(amici)\", \"maxAmici\")\n\ndf3.select('eta, 'minAmici, 'maxAmici, ('maxAmici - 'minAmici) as (\"delta\")).show(5)","user":"anonymous","dateUpdated":"2019-11-27T14:13:32+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+--------+-----+\n|eta|minAmici|maxAmici|delta|\n+---+--------+--------+-----+\n| 31|      73|     419|  346|\n| 65|     141|     469|  328|\n| 53|      42|     489|  447|\n| 34|      53|     478|  425|\n| 28|      36|     437|  401|\n+---+--------+--------+-----+\nonly showing top 5 rows\n\r\ndf2: org.apache.spark.sql.DataFrame = [eta: int, min(amici): int ... 1 more field]\r\ndf3: org.apache.spark.sql.DataFrame = [eta: int, minAmici: int ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1574696411492_826493343","id":"20191125-164011_184046025","dateCreated":"2019-11-25T16:40:11+0100","dateStarted":"2019-11-27T14:13:32+0100","dateFinished":"2019-11-27T14:13:33+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:407"},{"text":"%md\nUn altro modo per esprimere l'ultima riga del codice sopra è usando il metodo **selectExpr**.\nCon l'argomento **\"*\"** indico che voglio tutte le righe del DataFrame originario.\nE poi definisco tra virgolette la nuova colonna come differenze tra le atre due colonne maxAmici e minAmici.\n\nLa grossa differenza con select è che selectExpr consente di usare espressioni SQL.","user":"anonymous","dateUpdated":"2019-11-27T14:13:33+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Un altro modo per esprimere l&rsquo;ultima riga del codice sopra è usando il metodo <strong>selectExpr</strong>.<br/>Con l&rsquo;argomento <strong>&quot;*&quot;</strong> indico che voglio tutte le righe del DataFrame originario.<br/>E poi definisco tra virgolette la nuova colonna come differenze tra le atre due colonne maxAmici e minAmici.</p>\n<p>La grossa differenza con select è che selectExpr consente di usare espressioni SQL.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574697960185_-466029341","id":"20191125-170600_1839550777","dateCreated":"2019-11-25T17:06:00+0100","dateStarted":"2019-11-27T14:13:33+0100","dateFinished":"2019-11-27T14:13:33+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:408"},{"text":"df3.selectExpr( \"*\", \"(maxAmici - minAmici) as delta\" ).show(5)","user":"anonymous","dateUpdated":"2019-11-27T14:13:33+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+--------+-----+\n|eta|minAmici|maxAmici|delta|\n+---+--------+--------+-----+\n| 31|      73|     419|  346|\n| 65|     141|     469|  328|\n| 53|      42|     489|  447|\n| 34|      53|     478|  425|\n| 28|      36|     437|  401|\n+---+--------+--------+-----+\nonly showing top 5 rows\n\r\n"}]},"apps":[],"jobName":"paragraph_1574697505646_-2043094092","id":"20191125-165825_1385459195","dateCreated":"2019-11-25T16:58:25+0100","dateStarted":"2019-11-27T14:13:33+0100","dateFinished":"2019-11-27T14:13:33+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:409"},{"text":"%md\nA quale età si hanno in media più amici? Utilizziamo la funzione **sort()** con la specifica **.desc**","user":"anonymous","dateUpdated":"2019-11-27T14:13:33+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>A quale età si hanno in media più amici? Utilizziamo la funzione <strong>sort()</strong> con la specifica <strong>.desc</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574697503471_1491429497","id":"20191125-165823_548995766","dateCreated":"2019-11-25T16:58:23+0100","dateStarted":"2019-11-27T14:13:33+0100","dateFinished":"2019-11-27T14:13:33+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:410"},{"text":"df.groupBy(\"eta\").mean(\"amici\").withColumnRenamed(\"avg(amici)\", \"avgAmici\").sort('avgAmici.desc).show(5)","user":"anonymous","dateUpdated":"2019-11-27T14:13:34+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----------------+\n|eta|         avgAmici|\n+---+-----------------+\n|  6|394.3333333333333|\n| 56|366.3333333333333|\n| 40|           344.25|\n| 12|339.6666666666667|\n| 21|            336.9|\n+---+-----------------+\nonly showing top 5 rows\n\r\n"}]},"apps":[],"jobName":"paragraph_1574697502477_-1354601554","id":"20191125-165822_142643538","dateCreated":"2019-11-25T16:58:22+0100","dateStarted":"2019-11-27T14:13:34+0100","dateFinished":"2019-11-27T14:13:35+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:411"},{"text":"%md\nSe voglio creare un nuovo DataFrame con le prime N righe dopo avere ordinato la colonna in ordine decrescente uso la funzione **limit(N)**","user":"anonymous","dateUpdated":"2019-11-27T14:13:35+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Se voglio creare un nuovo DataFrame con le prime N righe dopo avere ordinato la colonna in ordine decrescente uso la funzione <strong>limit(N)</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574696035991_930948805","id":"20191125-163355_1989865449","dateCreated":"2019-11-25T16:33:55+0100","dateStarted":"2019-11-27T14:13:35+0100","dateFinished":"2019-11-27T14:13:35+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:412"},{"text":"df.groupBy(\"eta\").mean(\"amici\").withColumnRenamed(\"avg(amici)\", \"avgAmici\").sort('avgAmici.desc).limit(10).show()","user":"anonymous","dateUpdated":"2019-11-27T14:13:35+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+------------------+\n|eta|          avgAmici|\n+---+------------------+\n|  6| 394.3333333333333|\n| 56| 366.3333333333333|\n| 40|            344.25|\n| 12| 339.6666666666667|\n| 21|             336.9|\n| 50|             332.4|\n| 65|328.22222222222223|\n| 53|            322.25|\n|  1|             321.6|\n| 37|             320.6|\n+---+------------------+\n\r\n"}]},"apps":[],"jobName":"paragraph_1574696224717_426264717","id":"20191125-163704_1449211607","dateCreated":"2019-11-25T16:37:04+0100","dateStarted":"2019-11-27T14:13:35+0100","dateFinished":"2019-11-27T14:13:37+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:413"},{"text":"%md\nAggiungere una riga al DataFrame non è immediato, in quanto occorre creare un nuovo DataFrame con una riga e successivamente fare l'unione dei due DataFrame.\n\nPer creare il nuovo DataFrame uso il metodo della SparkSession **createDataFrame**, che prende come argomenti un RDD composto da Row, e lo *schema* da seguire, per questo uso lo schema di df.\n","user":"anonymous","dateUpdated":"2019-11-27T14:13:37+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Aggiungere una riga al DataFrame non è immediato, in quanto occorre creare un nuovo DataFrame con una riga e successivamente fare l&rsquo;unione dei due DataFrame.</p>\n<p>Per creare il nuovo DataFrame uso il metodo della SparkSession <strong>createDataFrame</strong>, che prende come argomenti un RDD composto da Row, e lo <em>schema</em> da seguire, per questo uso lo schema di df.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1574707530775_346237677","id":"20191125-194530_851574778","dateCreated":"2019-11-25T19:45:30+0100","dateStarted":"2019-11-27T14:13:37+0100","dateFinished":"2019-11-27T14:13:37+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:414"},{"text":"//filtriamo gli elementi con età 31 anni\nval df2 = df.filter(\"eta == 31\")\ndf2.show\n//importo il tipo Row\nimport org.apache.spark.sql.Row\n//creo un RDD di Row\nval dataRDD = spark.sparkContext.parallelize(Seq(Row(31,100)))\n//creo il nuovo DataFrame\nval newdf = spark.createDataFrame(dataRDD, df.schema)\nval df_31 = df2.union(newdf)\ndf_31.show","user":"anonymous","dateUpdated":"2019-11-27T14:13:37+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|eta|amici|\n+---+-----+\n| 31|  172|\n| 31|  408|\n| 31|   73|\n| 31|  326|\n| 31|  419|\n+---+-----+\n\r\n+---+-----+\n|eta|amici|\n+---+-----+\n| 31|  172|\n| 31|  408|\n| 31|   73|\n| 31|  326|\n| 31|  419|\n| 31|  100|\n+---+-----+\n\r\ndf2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [eta: int, amici: int]\r\nimport org.apache.spark.sql.Row\r\ndataRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[96] at parallelize at <console>:37\r\nnewdf: org.apache.spark.sql.DataFrame = [eta: int, amici: int]\r\ndf_31: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [eta: int, amici: int]\n"}]},"apps":[],"jobName":"paragraph_1574707969498_-1703321839","id":"20191125-195249_1016619002","dateCreated":"2019-11-25T19:52:49+0100","dateStarted":"2019-11-27T14:13:37+0100","dateFinished":"2019-11-27T14:13:37+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:415"}],"name":"Spark Basic 1","id":"2ET6N23G8","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}