{"paragraphs":[{"text":"%md\nScarichiamo il dataset sulle centrali elettriche mondiali dal sito [datasets.wri.org](http://datasets.wri.org/dataset/globalpowerplantdatabase).\n\nPer leggere i dati dal file csv possiamo importare i dati in un RDD, e poi convertire il RDD in un DataFrame.","user":"anonymous","dateUpdated":"2019-12-05T08:21:35+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Scarichiamo il dataset sulle centrali elettriche mondiali dal sito <a href=\"http://datasets.wri.org/dataset/globalpowerplantdatabase\">datasets.wri.org</a>.</p>\n<p>Per leggere i dati dal file csv possiamo importare i dati in un RDD, e poi convertire il RDD in un DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575233195023_385007770","id":"20191201-214635_79899124","dateCreated":"2019-12-01T21:46:35+0100","dateStarted":"2019-12-05T08:21:35+0100","dateFinished":"2019-12-05T08:21:35+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:38676"},{"text":"val data = sc.textFile(\"globalpowerplantdatabasev120/global_power_plant_database.csv\")","user":"anonymous","dateUpdated":"2019-12-05T06:14:26+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data: org.apache.spark.rdd.RDD[String] = globalpowerplantdatabasev120/global_power_plant_database.csv MapPartitionsRDD[1] at textFile at <console>:25\n"}]},"apps":[],"jobName":"paragraph_1575233048855_1460439028","id":"20191201-214408_249173080","dateCreated":"2019-12-01T21:44:08+0100","dateStarted":"2019-12-05T06:14:26+0100","dateFinished":"2019-12-05T06:14:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38677"},{"text":"%md\nIl dataset consta di 29911 righe","user":"anonymous","dateUpdated":"2019-12-05T08:21:58+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Il dataset consta di 29911 righe</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575233172735_1160193289","id":"20191201-214612_1874979361","dateCreated":"2019-12-01T21:46:12+0100","dateStarted":"2019-12-05T08:21:58+0100","dateFinished":"2019-12-05T08:21:58+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38678"},{"text":"data.count()","user":"anonymous","dateUpdated":"2019-12-05T06:14:41+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res2: Long = 29911\n"}]},"apps":[],"jobName":"paragraph_1575242930477_1103952607","id":"20191202-002850_271206334","dateCreated":"2019-12-02T00:28:50+0100","dateStarted":"2019-12-05T06:14:41+0100","dateFinished":"2019-12-05T06:14:42+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38679"},{"text":"%md\nStampiamo i primi 5 elementi del RDD","user":"anonymous","dateUpdated":"2019-12-05T08:23:12+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Stampiamo i primi 5 elementi del RDD</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575530554146_1431520260","id":"20191205-082234_760779531","dateCreated":"2019-12-05T08:22:34+0100","dateStarted":"2019-12-05T08:23:12+0100","dateFinished":"2019-12-05T08:23:12+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38680"},{"text":"data.take(5).foreach(println)","user":"anonymous","dateUpdated":"2019-12-05T08:22:18+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"country,country_long,name,gppd_idnr,capacity_mw,latitude,longitude,primary_fuel,other_fuel1,other_fuel2,other_fuel3,commissioning_year,owner,source,url,geolocation_source,wepp_id,year_of_capacity_data,generation_gwh_2013,generation_gwh_2014,generation_gwh_2015,generation_gwh_2016,generation_gwh_2017,estimated_generation_gwh\r\nAFG,Afghanistan,Kajaki Hydroelectric Power Plant Afghanistan,GEODB0040538,33.0,32.3220,65.1190,Hydro,,,,,,GEODB,http://globalenergyobservatory.org,GEODB,1009793,2017,,,,,,\r\nAFG,Afghanistan,Mahipar Hydroelectric Power Plant Afghanistan,GEODB0040541,66.0,34.5560,69.4787,Hydro,,,,,,GEODB,http://globalenergyobservatory.org,GEODB,1009795,2017,,,,,,\r\nAFG,Afghanistan,Naghlu Dam Hydroelectric Power Plant Afghanistan,GEODB0040534,100.0,34.6410,69.7170,Hydro,,,,,,GEODB,http://globalenergyobservatory.org,GEODB,1009797,2017,,,,,,\r\nAFG,Afghanistan,Nangarhar (Darunta) Hydroelectric Power Plant Afghanistan,GEODB0040536,11.55,34.4847,70.3633,Hydro,,,,,,GEODB,http://globalenergyobservatory.org,GEODB,1009787,2017,,,,,,\r\n"}]},"apps":[],"jobName":"paragraph_1575234017621_-1708517116","id":"20191201-220017_1290540198","dateCreated":"2019-12-01T22:00:17+0100","dateStarted":"2019-12-05T08:22:18+0100","dateFinished":"2019-12-05T08:22:19+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38681"},{"text":"%md\nCome si vede la prima riga contiene l'header del file csv, ovvero i nomi di ciascuna colonna del file csv.\n\nSiccome vogliamo un RDD di soli dati numerici devo eliminare la prima colonna.\n\nPer fare ciò uso il metodo **filter()**, definendo un filtro che cattura tutti gli elementi tranne la prima stringa contenuta nella prima riga.\n\nInizio col definire una variabile contenente la stringa della prima riga usando il metodo **first()**","user":"anonymous","dateUpdated":"2019-12-05T06:14:43+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Come si vede la prima riga contiene l&rsquo;header del file csv, ovvero i nomi di ciascuna colonna del file csv.</p>\n<p>Siccome vogliamo un RDD di soli dati numerici devo eliminare la prima colonna.</p>\n<p>Per fare ciò uso il metodo <strong>filter()</strong>, definendo un filtro che cattura tutti gli elementi tranne la prima stringa contenuta nella prima riga.</p>\n<p>Inizio col definire una variabile contenente la stringa della prima riga usando il metodo <strong>first()</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575293346261_-387994335","id":"20191202-142906_1740898142","dateCreated":"2019-12-02T14:29:06+0100","dateStarted":"2019-12-05T06:14:43+0100","dateFinished":"2019-12-05T06:14:43+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38682"},{"text":"val dataheader = data.first()","user":"anonymous","dateUpdated":"2019-12-05T06:14:43+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dataheader: String = country,country_long,name,gppd_idnr,capacity_mw,latitude,longitude,primary_fuel,other_fuel1,other_fuel2,other_fuel3,commissioning_year,owner,source,url,geolocation_source,wepp_id,year_of_capacity_data,generation_gwh_2013,generation_gwh_2014,generation_gwh_2015,generation_gwh_2016,generation_gwh_2017,estimated_generation_gwh\n"}]},"apps":[],"jobName":"paragraph_1575234056651_886482691","id":"20191201-220056_2105282125","dateCreated":"2019-12-01T22:00:56+0100","dateStarted":"2019-12-05T06:14:43+0100","dateFinished":"2019-12-05T06:14:44+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38683"},{"text":"%md\nAdesso filtro tutti gli elementi che soddisfano la condizione **x != dataheader**, questi elementi sono inclusi nel nuovo RDD","user":"anonymous","dateUpdated":"2019-12-05T06:14:44+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Adesso filtro tutti gli elementi che soddisfano la condizione <strong>x != dataheader</strong>, questi elementi sono inclusi nel nuovo RDD</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575293342692_708678822","id":"20191202-142902_2096090107","dateCreated":"2019-12-02T14:29:02+0100","dateStarted":"2019-12-05T06:14:44+0100","dateFinished":"2019-12-05T06:14:44+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38684"},{"text":"val data_no_header = data.filter(x => (x != dataheader))","user":"anonymous","dateUpdated":"2019-12-05T06:14:44+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data_no_header: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:29\n"}]},"apps":[],"jobName":"paragraph_1575235783115_923865695","id":"20191201-222943_546130858","dateCreated":"2019-12-01T22:29:43+0100","dateStarted":"2019-12-05T06:14:44+0100","dateFinished":"2019-12-05T06:14:45+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38685"},{"text":"%md\nCome possiamo vedere il primo elemento del nuovo RDD non contiene più l'header ma i dati del dataset","user":"anonymous","dateUpdated":"2019-12-05T06:14:45+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Come possiamo vedere il primo elemento del nuovo RDD non contiene più l&rsquo;header ma i dati del dataset</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575294342301_1470470052","id":"20191202-144542_1289691035","dateCreated":"2019-12-02T14:45:42+0100","dateStarted":"2019-12-05T06:14:45+0100","dateFinished":"2019-12-05T06:14:45+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38686"},{"text":"data_no_header.first()","user":"anonymous","dateUpdated":"2019-12-05T06:14:45+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res4: String = AFG,Afghanistan,Kajaki Hydroelectric Power Plant Afghanistan,GEODB0040538,33.0,32.3220,65.1190,Hydro,,,,,,GEODB,http://globalenergyobservatory.org,GEODB,1009793,2017,,,,,,\n"}]},"apps":[],"jobName":"paragraph_1575235817311_214166964","id":"20191201-223017_2113288087","dateCreated":"2019-12-01T22:30:17+0100","dateStarted":"2019-12-05T06:14:45+0100","dateFinished":"2019-12-05T06:14:45+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38687"},{"text":"data_no_header.take(1)","user":"anonymous","dateUpdated":"2019-12-05T06:14:46+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res5: Array[String] = Array(AFG,Afghanistan,Kajaki Hydroelectric Power Plant Afghanistan,GEODB0040538,33.0,32.3220,65.1190,Hydro,,,,,,GEODB,http://globalenergyobservatory.org,GEODB,1009793,2017,,,,,,)\n"}]},"apps":[],"jobName":"paragraph_1575243078184_-812457598","id":"20191202-003118_1919192121","dateCreated":"2019-12-02T00:31:18+0100","dateStarted":"2019-12-05T06:14:46+0100","dateFinished":"2019-12-05T06:14:46+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38688"},{"text":"%md\nSi può vedere sotto che ciascun elemento del RDD è una lunga stringa, contenente i dati separati da una virgola.","user":"anonymous","dateUpdated":"2019-12-05T08:25:51+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Si può vedere sotto che ciascun elemento del RDD è una lunga stringa, contenente i dati separati da una virgola.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575243170683_1282653005","id":"20191202-003250_1797908175","dateCreated":"2019-12-02T00:32:50+0100","dateStarted":"2019-12-05T08:25:51+0100","dateFinished":"2019-12-05T08:25:51+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38689"},{"text":"data_no_header.take(1)","user":"anonymous","dateUpdated":"2019-12-05T06:14:47+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res7: Array[String] = Array(AFG,Afghanistan,Kajaki Hydroelectric Power Plant Afghanistan,GEODB0040538,33.0,32.3220,65.1190,Hydro,,,,,,GEODB,http://globalenergyobservatory.org,GEODB,1009793,2017,,,,,,)\n"}]},"apps":[],"jobName":"paragraph_1575295363829_-1115014261","id":"20191202-150243_1143496609","dateCreated":"2019-12-02T15:02:43+0100","dateStarted":"2019-12-05T06:14:47+0100","dateFinished":"2019-12-05T06:14:47+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38690"},{"text":"%md\nAdesso voglio separare i dati contenuti nelle stringhe. Inizio con l'usare il metodo **split()** su ciascuna stringa. Il risultato è che ogni stringa è convertita in un Array di stringhe.","user":"anonymous","dateUpdated":"2019-12-05T08:27:01+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":53,"optionOpen":false}}},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Adesso voglio separare i dati contenuti nelle stringhe. Inizio con l&rsquo;usare il metodo <strong>split()</strong> su ciascuna stringa. Il risultato è che ogni stringa è convertita in un Array di stringhe.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575295374530_-1862793636","id":"20191202-150254_548823022","dateCreated":"2019-12-02T15:02:54+0100","dateStarted":"2019-12-05T08:27:01+0100","dateFinished":"2019-12-05T08:27:01+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38691"},{"text":"val data_array = data_no_header.map(x => x.split(','))\ndata_array.take(1)","user":"anonymous","dateUpdated":"2019-12-05T06:14:48+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data_array: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[9] at map at <console>:31\r\nres8: Array[Array[String]] = Array(Array(AFG, Afghanistan, Kajaki Hydroelectric Power Plant Afghanistan, GEODB0040538, 33.0, 32.3220, 65.1190, Hydro, \"\", \"\", \"\", \"\", \"\", GEODB, http://globalenergyobservatory.org, GEODB, 1009793, 2017))\n"}]},"apps":[],"jobName":"paragraph_1575295814363_-891429458","id":"20191202-151014_1208019031","dateCreated":"2019-12-02T15:10:14+0100","dateStarted":"2019-12-05T06:14:48+0100","dateFinished":"2019-12-05T06:14:48+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38692"},{"text":"%md\n## Problema\n\nSupponiamo di volere calcolare il dato della produzione totale annuale di energia in Italia.\n\nPer sapere cosa sono i dati nell'array posso fare riferimento al valore precedentemente definito *dataheader*.\n\nScopriamo che i dati che ci interessano sono nel primo campo ( *country*), nell'ottavo ( *primary_fuel*) e nel 24-esimo ( *estimated_generation_gwh*). Si ricordi comunque che in Scala il primo elemento dell'Array ha posizione 0.","user":"anonymous","dateUpdated":"2019-12-05T10:35:37+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Problema</h2>\n<p>Supponiamo di volere calcolare il dato della produzione totale annuale di energia in Italia.</p>\n<p>Per sapere cosa sono i dati nell&rsquo;array posso fare riferimento al valore precedentemente definito <em>dataheader</em>.</p>\n<p>Scopriamo che i dati che ci interessano sono nel primo campo ( <em>country</em>), nell&rsquo;ottavo ( <em>primary_fuel</em>) e nel 24-esimo ( <em>estimated_generation_gwh</em>). Si ricordi comunque che in Scala il primo elemento dell&rsquo;Array ha posizione 0.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575296011560_-254763528","id":"20191202-151331_1486030913","dateCreated":"2019-12-02T15:13:31+0100","dateStarted":"2019-12-05T10:35:37+0100","dateFinished":"2019-12-05T10:35:37+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38693"},{"text":"dataheader","user":"anonymous","dateUpdated":"2019-12-05T06:14:49+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res9: String = country,country_long,name,gppd_idnr,capacity_mw,latitude,longitude,primary_fuel,other_fuel1,other_fuel2,other_fuel3,commissioning_year,owner,source,url,geolocation_source,wepp_id,year_of_capacity_data,generation_gwh_2013,generation_gwh_2014,generation_gwh_2015,generation_gwh_2016,generation_gwh_2017,estimated_generation_gwh\n"}]},"apps":[],"jobName":"paragraph_1575298038249_569442068","id":"20191202-154718_1435942342","dateCreated":"2019-12-02T15:47:18+0100","dateStarted":"2019-12-05T06:14:49+0100","dateFinished":"2019-12-05T06:14:49+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38694"},{"text":"%md\nInizio con il filtrare tutte e solo le righe in cui il primo elemento dell'array contiene il codice country='ITA'. ","user":"anonymous","dateUpdated":"2019-12-05T10:34:37+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Inizio con il filtrare tutte e solo le righe in cui il primo elemento dell&rsquo;array contiene il codice country=&lsquo;ITA&rsquo;.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575298080313_-1209878337","id":"20191202-154800_1847115384","dateCreated":"2019-12-02T15:48:00+0100","dateStarted":"2019-12-05T10:34:37+0100","dateFinished":"2019-12-05T10:34:37+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38695"},{"text":"val data_ita = data_array.filter(x => x(0) == \"ITA\")\ndata_ita.take(2)","user":"anonymous","dateUpdated":"2019-12-05T06:14:49+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data_ita: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[10] at filter at <console>:33\r\nres10: Array[Array[String]] = Array(Array(ITA, Italy, ACCEGLIO, WRI1021706, 19.0, 44.4742, 7.0183, Hydro, \"\", \"\", \"\", \"\", \"\", ENTSOE, https://transparency.entsoe.eu/generation/r2/installedCapacityPerProductionUnit/show, WRI, 1015906, \"\", \"\", \"\", \"\", \"\", \"\", 85.53859855441274), Array(ITA, Italy, ACERRA, WRI1021322, 72.0, 40.9319, 14.3850, Other, \"\", \"\", \"\", \"\", \"\", ENTSOE, https://transparency.entsoe.eu/generation/r2/installedCapacityPerProductionUnit/show, GEODB, 1053275|1067259, \"\", \"\", \"\", \"\", \"\", \"\", 123.74414976599063))\n"}]},"apps":[],"jobName":"paragraph_1575296829770_-1379226443","id":"20191202-152709_1578261837","dateCreated":"2019-12-02T15:27:09+0100","dateStarted":"2019-12-05T06:14:49+0100","dateFinished":"2019-12-05T06:14:49+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38696"},{"text":"%md\nSi noti che il motivo per cui ho filtrato i dati sopra è quello di ridurre il dataset al minimo indispensabile, cioè ai dati relativi solo all'Italia e ai campi che ci servono. In questo modo il lavoro del cluster è semplificato e velocizzato, poiché ci sono meno dati da spostare nella rete. \n\nAdesso posso selezionare solamente la colonna di dati che mi interessa quella con i valori della produzione energetica, colonna numero 24","user":"anonymous","dateUpdated":"2019-12-05T10:36:09+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Si noti che il motivo per cui ho filtrato i dati sopra è quello di ridurre il dataset al minimo indispensabile, cioè ai dati relativi solo all&rsquo;Italia e ai campi che ci servono. In questo modo il lavoro del cluster è semplificato e velocizzato, poiché ci sono meno dati da spostare nella rete. </p>\n<p>Adesso posso selezionare solamente la colonna di dati che mi interessa quella con i valori della produzione energetica, colonna numero 24</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575301602128_1339838706","id":"20191202-164642_860431816","dateCreated":"2019-12-02T16:46:42+0100","dateStarted":"2019-12-05T10:36:09+0100","dateFinished":"2019-12-05T10:36:09+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38697"},{"text":"val data_ita_keyvalue = data_ita.map(x => (x(0), x(23).toDouble))","user":"anonymous","dateUpdated":"2019-12-05T06:14:50+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data_ita_keyvalue: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[11] at map at <console>:35\n"}]},"apps":[],"jobName":"paragraph_1575301566937_1911746762","id":"20191202-164606_2052055759","dateCreated":"2019-12-02T16:46:06+0100","dateStarted":"2019-12-05T06:14:50+0100","dateFinished":"2019-12-05T06:14:50+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38698"},{"text":"%md\nIl risultato della mappatura precedente è un RDD di coppie (chiave, valore) o *(key, value)*. Un RDD di coppie *(key, value)* ha una serie di mtodi dedicati, tra cui **reduceByKey()**.\n\nAdesso applico il metodo **reduceByKey()** sulle coppie *(chiave, valore)* del RDD. Questo metodo seleziona tutti gli elementi aventi la stessa chiave e sui corrispondenti valori applica la funzione che indico come argomento del metodo. Nel codice sotto **a** e **b** sono i valori di due diverse coppie (chiave, valore), quindi **a** e **b** sono scalari che posso sommare tra loro.","user":"anonymous","dateUpdated":"2019-12-05T10:39:13+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Il risultato della mappatura precedente è un RDD di coppie (chiave, valore) o <em>(key, value)</em>. Un RDD di coppie <em>(key, value)</em> ha una serie di mtodi dedicati, tra cui <strong>reduceByKey()</strong>.</p>\n<p>Adesso applico il metodo <strong>reduceByKey()</strong> sulle coppie <em>(chiave, valore)</em> del RDD. Questo metodo seleziona tutti gli elementi aventi la stessa chiave e sui corrispondenti valori applica la funzione che indico come argomento del metodo. Nel codice sotto <strong>a</strong> e <strong>b</strong> sono i valori di due diverse coppie (chiave, valore), quindi <strong>a</strong> e <strong>b</strong> sono scalari che posso sommare tra loro.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575301167130_-1332456002","id":"20191202-163927_2001901954","dateCreated":"2019-12-02T16:39:27+0100","dateStarted":"2019-12-05T10:39:13+0100","dateFinished":"2019-12-05T10:39:13+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38699"},{"text":"val somma_produzione = data_ita_keyvalue.reduceByKey( (a,b) => a+b ).collect()","user":"anonymous","dateUpdated":"2019-12-05T06:14:51+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"somma_produzione: Array[(String, Double)] = Array((ITA,258640.99999999994))\n"}]},"apps":[],"jobName":"paragraph_1575296900128_-1531188754","id":"20191202-152820_1744700337","dateCreated":"2019-12-02T15:28:20+0100","dateStarted":"2019-12-05T06:14:51+0100","dateFinished":"2019-12-05T06:14:52+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38700"},{"text":"%md\n### Domanda\nMa quanto è la produzione di energia in Italia?","user":"anonymous","dateUpdated":"2019-12-05T10:39:32+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Domanda</h3>\n<p>Ma quanto è la produzione di energia in Italia?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575305972135_-1516100070","id":"20191202-175932_1000417631","dateCreated":"2019-12-02T17:59:32+0100","dateStarted":"2019-12-05T10:39:32+0100","dateFinished":"2019-12-05T10:39:32+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38701"},{"text":"val somma = somma_produzione(0)._2\nprintln(f\"La produzione stimata di energia elettrica totale in Italia è di $somma%.2f GWh\")","user":"anonymous","dateUpdated":"2019-12-05T10:39:50+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"La produzione stimata di energia elettrica totale in Italia è di 258641.00 GWh\r\nsomma: Double = 258640.99999999994\n"}]},"apps":[],"jobName":"paragraph_1575299484556_-1344950499","id":"20191202-161124_438345999","dateCreated":"2019-12-02T16:11:24+0100","dateStarted":"2019-12-05T10:39:50+0100","dateFinished":"2019-12-05T10:39:51+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38702"},{"text":"%md\n\n## Aggregazione dei dati\n\nNaturalmente avrei potuto decidere di lavorare su tutto il dataset senza restringere il RDD ai dati relativi all'Italia. In questo modo posso avere il dato aggregato della produzione di energia per ogni stato del database. Vediamo come procedere.\n\nInnanzitutto con una **map()** seleziono solo i dati delle colonne *country* e *estimated_generation_gwh*  e al contempo converto le stringhe di x(23) in double usando il metodo **toDouble()**. Il risulato sarà un RDD di coppie (chiave, valore).","user":"anonymous","dateUpdated":"2019-12-05T10:42:22+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Aggregazione dei dati</h2>\n<p>Naturalmente avrei potuto decidere di lavorare su tutto il dataset senza restringere il RDD ai dati relativi all&rsquo;Italia. In questo modo posso avere il dato aggregato della produzione di energia per ogni stato del database. Vediamo come procedere.</p>\n<p>Innanzitutto con una <strong>map()</strong> seleziono solo i dati delle colonne <em>country</em> e <em>estimated_generation_gwh</em> e al contempo converto le stringhe di x(23) in double usando il metodo <strong>toDouble()</strong>. Il risulato sarà un RDD di coppie (chiave, valore).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575304843805_859949676","id":"20191202-174043_937491198","dateCreated":"2019-12-02T17:40:43+0100","dateStarted":"2019-12-05T10:42:22+0100","dateFinished":"2019-12-05T10:42:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38703"},{"text":"val data_kv = data_array.map(x => (x(0), x(23).toDouble))","user":"anonymous","dateUpdated":"2019-12-05T10:42:27+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data_kv: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[56] at map at <console>:33\n"}]},"apps":[],"jobName":"paragraph_1575305626722_742830955","id":"20191202-175346_1866297325","dateCreated":"2019-12-02T17:53:46+0100","dateStarted":"2019-12-05T10:42:27+0100","dateFinished":"2019-12-05T10:42:27+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38704"},{"text":"%md\nVediamo la prima coppia *(chiave, valore)* cosa contiene","user":"anonymous","dateUpdated":"2019-12-05T10:42:36+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Vediamo la prima coppia <em>(chiave, valore)</em> cosa contiene</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575325219462_-1515166214","id":"20191202-232019_1992745906","dateCreated":"2019-12-02T23:20:19+0100","dateStarted":"2019-12-05T10:42:36+0100","dateFinished":"2019-12-05T10:42:36+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38705"},{"text":"data_kv.take(1)","user":"anonymous","dateUpdated":"2019-12-05T10:42:39+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":144,"optionOpen":false}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 131.0 failed 1 times, most recent failure: Lost task 0.0 in stage 131.0 (TID 79, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 23\r\n\tat $anonfun$1.apply(<console>:33)\r\n\tat $anonfun$1.apply(<console>:33)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:393)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\r\nDriver stacktrace:\r\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n  at scala.Option.foreach(Option.scala:257)\r\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\r\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1327)\r\n  ... 47 elided\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 23\r\n  at $anonfun$1.apply(<console>:33)\r\n  at $anonfun$1.apply(<console>:33)\r\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n  at scala.collection.Iterator$$anon$10.next(Iterator.scala:393)\r\n  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n  at scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n  at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\r\n  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\r\n  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n  at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1575315303452_-1118030315","id":"20191202-203503_1806272136","dateCreated":"2019-12-02T20:35:03+0100","dateStarted":"2019-12-05T10:42:39+0100","dateFinished":"2019-12-05T10:42:40+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:38706"},{"text":"%md\nCome si vede ottengo un errore, precisamente **java.lang.ArrayIndexOutOfBoundsException**. Stranamente non avevo ricevuto lo stesso errore quando ho usato la stessa procedura per i dati dell'Italia.\n\nDopo un po di investigazioni mi accorgo che il problema sta nel fatto che l'ultimo campo ( x(23) ) alla fine di alcune righe è vuoto perché la funzione **split()**, che abbiamo usato per convertire la stringa singola letta dal file, non converte i campi vuoti alla fine della stringa. Ovvero:\n\n`split(\"AFG, 100, 12500,,,,\")`\n\nviene convertito in\n\n`Array[String] = [\"AFG\", \"100\", \"12500\"]`\n\nPer fare in modo che la conversione contenga anche i campi vuoti devo usare un secondo parametro in **split()** ovvero:\n\n`split(\"AFG, 100, 12500,,,,\", -1)`\n\nviene convertito in \n\n`Array[String] = [\"AFG\", \"100\", \"12500\", \"\", \"\", \"\", \"\"]`\n\nesattamente ciò di cui abbiamo bisogno per evitare l'errore **ArrayIndexOutOfBoundsException**.","user":"anonymous","dateUpdated":"2019-12-05T10:44:03+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Come si vede ottengo un errore, precisamente <strong>java.lang.ArrayIndexOutOfBoundsException</strong>. Stranamente non avevo ricevuto lo stesso errore quando ho usato la stessa procedura per i dati dell&rsquo;Italia.</p>\n<p>Dopo un po di investigazioni mi accorgo che il problema sta nel fatto che l&rsquo;ultimo campo ( x(23) ) alla fine di alcune righe è vuoto perché la funzione <strong>split()</strong>, che abbiamo usato per convertire la stringa singola letta dal file, non converte i campi vuoti alla fine della stringa. Ovvero:</p>\n<p><code>split(&quot;AFG, 100, 12500,,,,&quot;)</code></p>\n<p>viene convertito in</p>\n<p><code>Array[String] = [&quot;AFG&quot;, &quot;100&quot;, &quot;12500&quot;]</code></p>\n<p>Per fare in modo che la conversione contenga anche i campi vuoti devo usare un secondo parametro in <strong>split()</strong> ovvero:</p>\n<p><code>split(&quot;AFG, 100, 12500,,,,&quot;, -1)</code></p>\n<p>viene convertito in </p>\n<p><code>Array[String] = [&quot;AFG&quot;, &quot;100&quot;, &quot;12500&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;]</code></p>\n<p>esattamente ciò di cui abbiamo bisogno per evitare l&rsquo;errore <strong>ArrayIndexOutOfBoundsException</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575325295942_1879823511","id":"20191202-232135_1079055390","dateCreated":"2019-12-02T23:21:35+0100","dateStarted":"2019-12-05T10:44:03+0100","dateFinished":"2019-12-05T10:44:03+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38707"},{"text":"data_no_header.map(x => x.split(\",\", -1)).map(x => (x(0), x(23))).collect()","user":"anonymous","dateUpdated":"2019-12-05T10:44:43+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":144,"optionOpen":false}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res110: Array[(String, String)] = Array((AFG,\"\"), (AFG,\"\"), (AFG,\"\"), (AFG,\"\"), (AFG,\"\"), (AFG,\"\"), (AFG,\"\"), (ALB,89.13207547169812), (ALB,1650.5939902166317), (ALB,1980.712788259958), (ALB,16.50593990216632), (ALB,79.22851153039832), (ALB,82.52969951083159), (ALB,825.2969951083159), (ALB,0.0), (DZA,2152.249818828645), (DZA,293.8648791092957), (DZA,2317.807497200079), (DZA,413.8941959285856), (DZA,1862.523881678635), (DZA,1862.523881678635), (DZA,1208.57105211147), (DZA,4966.730351143026), (DZA,1730.0777389814875), (DZA,298.0038210685816), (DZA,2483.365175571513), (DZA,827.7883918571712), (DZA,2036.3594439686408), (DZA,254.0), (DZA,2433.697872060083), (DZA,1427.93497595362), (DZA,4966.730351143026), (DZA,1639.0210158771988), (DZA,3476.711245800119), (DZA,761.5653205085974), (DZA,4056.1..."}]},"apps":[],"jobName":"paragraph_1575311626159_827971120","id":"20191202-193346_738072400","dateCreated":"2019-12-02T19:33:46+0100","dateStarted":"2019-12-05T10:44:43+0100","dateFinished":"2019-12-05T10:44:43+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38708"},{"text":"%md\nCome si vede ottengo un array di coppie (stringa, stringa), ma se voglio fare operazioni matematiche sul secondo elemento della coppia devo convertire quest'ultimo in un valore numerico.\n\nPer la conversione della stringa normalmente basterebbe usare il metodo **String.toDouble()**. \nMa a causa del fatto che ci sono stringhe vuote, il metodo toDouble() sulla stringa vuota darebbe errore.\n\nAllora definiamo una funzione che riceve in ingresso una stringa, converte il suo contenuto in Double, usando il metodo **toDouble()**, e in caso la stringa sia vuota ritorna il valore 0.","user":"anonymous","dateUpdated":"2019-12-05T10:47:06+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Come si vede ottengo un array di coppie (stringa, stringa), ma se voglio fare operazioni matematiche sul secondo elemento della coppia devo convertire quest&rsquo;ultimo in un valore numerico.</p>\n<p>Per la conversione della stringa normalmente basterebbe usare il metodo <strong>String.toDouble()</strong>.<br/>Ma a causa del fatto che ci sono stringhe vuote, il metodo toDouble() sulla stringa vuota darebbe errore.</p>\n<p>Allora definiamo una funzione che riceve in ingresso una stringa, converte il suo contenuto in Double, usando il metodo <strong>toDouble()</strong>, e in caso la stringa sia vuota ritorna il valore 0.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575415879539_-245195480","id":"20191204-003119_1042729388","dateCreated":"2019-12-04T00:31:19+0100","dateStarted":"2019-12-05T10:47:06+0100","dateFinished":"2019-12-05T10:47:06+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38709"},{"text":"def convert_to_Double(x: String): Double = {\n    if (x != \"\") {\n        x.toDouble\n    } else {\n        0\n    }\n}","user":"anonymous","dateUpdated":"2019-12-05T10:47:19+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"convert_to_Double: (x: String)Double\n"}]},"apps":[],"jobName":"paragraph_1575324465223_-2077640588","id":"20191202-230745_1088654845","dateCreated":"2019-12-02T23:07:45+0100","dateStarted":"2019-12-05T10:47:19+0100","dateFinished":"2019-12-05T10:47:19+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38710"},{"text":"%md\nVerifichiamo che funzioni:","user":"anonymous","dateUpdated":"2019-12-05T10:47:22+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Verifichiamo che funzioni:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575311799568_861121229","id":"20191202-193639_1958040466","dateCreated":"2019-12-02T19:36:39+0100","dateStarted":"2019-12-05T10:47:22+0100","dateFinished":"2019-12-05T10:47:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38711"},{"text":"print(convert_to_Double(\"25\"))\nprint(\"\\n\")\nprint(convert_to_Double(\"\"))","user":"anonymous","dateUpdated":"2019-12-05T10:47:24+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"25.0\n0.0"}]},"apps":[],"jobName":"paragraph_1575329700532_-306527629","id":"20191203-003500_1040363346","dateCreated":"2019-12-03T00:35:00+0100","dateStarted":"2019-12-05T10:47:24+0100","dateFinished":"2019-12-05T10:47:25+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38712"},{"text":"%md\nOvviamente lo svantaggio di questo metodo è che non so se un risultato pari a zero corrisponde ad una produzione nulla o ad una mancanza di dati. \n\nAdesso procedo con la stessa analisi fatta in predecenza, siccome conosco i vari passaggi posso eseguirli tutti in una singola riga di codice, questi sono i singoli passaggi:\n- dato un RDD composto da stringhe\n- splitto ogni stringa nei singoli campi con **split(\",\")**\n- mappo ogni riga in modo da avere solo il campo *country_code* e *estimated_production*, ottenendo un RDD di coppie *(key, value)*\n- con **reduceByKey()** aggrego tutte le righe aventi la stessa chiave\n- **collect()** e **println()**","user":"anonymous","dateUpdated":"2019-12-05T10:49:58+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Ovviamente lo svantaggio di questo metodo è che non so se un risultato pari a zero corrisponde ad una produzione nulla o ad una mancanza di dati. </p>\n<p>Adesso procedo con la stessa analisi fatta in predecenza, siccome conosco i vari passaggi posso eseguirli tutti in una singola riga di codice, questi sono i singoli passaggi:<br/>- dato un RDD composto da stringhe<br/>- splitto ogni stringa nei singoli campi con <strong>split(&ldquo;,&rdquo;)</strong><br/>- mappo ogni riga in modo da avere solo il campo <em>country_code</em> e <em>estimated_production</em>, ottenendo un RDD di coppie <em>(key, value)</em><br/>- con <strong>reduceByKey()</strong> aggrego tutte le righe aventi la stessa chiave<br/>- <strong>collect()</strong> e <strong>println()</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575474193088_-1285746384","id":"20191204-164313_182392174","dateCreated":"2019-12-04T16:43:13+0100","dateStarted":"2019-12-05T10:49:58+0100","dateFinished":"2019-12-05T10:49:58+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38713"},{"text":"val data_agg = data_no_header.map(x => x.split(\",\", -1)).map(x => (x(0), convert_to_Double(x(23)) )).reduceByKey((a,b) => a+b)\ndata_agg.collect().foreach(println)","user":"anonymous","dateUpdated":"2019-12-05T10:52:00+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(SVN,16869.0)\r\n(FIN,67265.99999999999)\r\n(BLR,34041.99999999999)\r\n(VNM,140913.00000000006)\r\n(ROU,63069.0)\r\n(FJI,0.0)\r\n(GHA,0.0)\r\n(TUR,250530.99999999994)\r\n(HND,7996.999999999999)\r\n(NER,686.0000000000001)\r\n(BHR,27252.999999999996)\r\n(ATA,0.0)\r\n(BRN,4461.0)\r\n(CHL,73428.0)\r\n(COL,67719.0)\r\n(GIN,0.0)\r\n(IDN,227586.99999999994)\r\n(JOR,18153.999999999996)\r\n(CYP,4033.0)\r\n(NGA,30390.000000000007)\r\n(MOZ,16175.000000000002)\r\n(SWE,150760.00000000006)\r\n(BEN,183.0)\r\n(ESH,0.0)\r\n(MKD,4944.0)\r\n(ZMB,14452.0)\r\n(MDA,5322.0)\r\n(CMR,6850.0)\r\n(NLD,93830.00000000004)\r\n(TJK,16000.0)\r\n(RUS,984212.0)\r\n(GEO,10371.0)\r\n(GNB,0.0)\r\n(VEN,109826.99999999999)\r\n(ISR,60439.000000000015)\r\n(SVK,24935.999999999993)\r\n(PRY,55276.0)\r\n(LAO,0.0)\r\n(MWI,0.0)\r\n(IRQ,67767.99999999997)\r\n(LVA,4331.0)\r\n(SAU,311806.0)\r\n(HRV,12623.999999999996)\r\n(NPL,3788.9999999999995)\r\n(SDN,11376.0)\r\n(KWT,21984.000000000004)\r\n(BRA,590631.9999999984)\r\n(SGP,48647.0)\r\n(IRL,26180.000000000007)\r\n(JAM,3976.0)\r\n(GUY,0.0)\r\n(POL,158866.99999999997)\r\n(SRB,33783.00000000001)\r\n(ARE,108506.0)\r\n(SLE,0.0)\r\n(TKM,20400.0)\r\n(UGA,0.0)\r\n(GTM,10725.000000000002)\r\n(ZWE,9828.0)\r\n(CUB,18605.0)\r\n(ERI,368.0)\r\n(BWA,2263.0)\r\n(BDI,0.0)\r\n(PHL,77196.0)\r\n(ECU,23812.0)\r\n(MEX,301418.99999999977)\r\n(ISL,18111.000000000004)\r\n(BIH,16085.999999999996)\r\n(TTO,9891.0)\r\n(SLV,5784.000000000001)\r\n(ALB,4724.0)\r\n(CIV,7708.0)\r\n(COD,8831.0)\r\n(AUT,54075.0)\r\n(BFA,0.0)\r\n(COG,1740.0)\r\n(EGY,45324.0)\r\n(IRN,274032.0)\r\n(CHN,5621543.999999972)\r\n(KOS,5270.0)\r\n(TWN,255790.99999999997)\r\n(IND,316056.4701018557)\r\n(GUF,0.0)\r\n(AGO,9479.999999999998)\r\n(CAN,655943.9999999995)\r\n(KHM,3055.9999999999995)\r\n(TGO,137.0)\r\n(EST,716.0000000000001)\r\n(KAZ,104031.0)\r\n(SWZ,0.0)\r\n(MMR,14092.0)\r\n(PRT,51438.00000000008)\r\n(MRT,0.0)\r\n(SEN,3440.0)\r\n(LBR,0.0)\r\n(UKR,180910.0)\r\n(USA,760222.293761999)\r\n(NIC,3106.0000000000005)\r\n(DZA,63079.0)\r\n(ARM,7746.0)\r\n(BEL,69406.0)\r\n(DOM,18541.0)\r\n(GAB,2354.9999999999995)\r\n(DJI,0.0)\r\n(HUN,26061.0)\r\n(AFG,0.0)\r\n(PAK,105304.99999999997)\r\n(THA,171591.99999999988)\r\n(OMN,28371.0)\r\n(BTN,0.0)\r\n(KOR,549440.0000000001)\r\n(ITA,258640.99999999994)\r\n(CAF,0.0)\r\n(ETH,9606.0)\r\n(GRC,50254.0)\r\n(JPN,1011747.9999999998)\r\n(RWA,0.0)\r\n(SYR,21725.999999999996)\r\n(PAN,9139.0)\r\n(ZAF,252387.99999999997)\r\n(AUS,61441.04416666662)\r\n(MUS,2930.999999999999)\r\n(GBR,338923.0000000005)\r\n(LTU,2837.0000000000005)\r\n(ARG,138805.99999999997)\r\n(KGZ,14455.0)\r\n(DEU,627697.0000000007)\r\n(GNQ,0.0)\r\n(FRA,557930.0000000016)\r\n(MNE,3174.0)\r\n(URY,13011.0)\r\n(LBY,37731.0)\r\n(DNK,30559.0)\r\n(LUX,2621.0)\r\n(LKA,12419.999999999998)\r\n(PER,43676.0)\r\n(QAT,38692.0)\r\n(BOL,8379.0)\r\n(MLI,0.0)\r\n(UZB,55399.99999999999)\r\n(KEN,9120.999999999998)\r\n(PRK,17322.0)\r\n(NOR,141453.00000000006)\r\n(BGD,55696.00000000002)\r\n(MAR,29141.999999999996)\r\n(MNG,5134.0)\r\n(TZA,6179.0)\r\n(CZE,0.0)\r\n(CPV,0.0)\r\n(CRI,9298.000000000004)\r\n(GMB,0.0)\r\n(YEM,7646.0)\r\n(PNG,0.0)\r\n(BGR,43666.00000000001)\r\n(TUN,18483.000000000004)\r\n(AZE,24549.0)\r\n(NAM,1498.0)\r\n(LSO,0.0)\r\n(LBN,17759.0)\r\n(ESP,278749.9999999994)\r\n(MDG,0.0)\r\n(NZL,42867.99999999999)\r\n(MYS,147468.99999999997)\r\n(CHE,67258.00000000001)\r\ndata_agg: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[63] at reduceByKey at <console>:33\n"}]},"apps":[],"jobName":"paragraph_1575323179086_1859091287","id":"20191202-224619_511166840","dateCreated":"2019-12-02T22:46:19+0100","dateStarted":"2019-12-05T10:52:00+0100","dateFinished":"2019-12-05T10:52:01+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38714"},{"text":"%md\nSe voglio ordinare in ordine descrescente per vedere quali sono le nazioni con la più alta produzione di energia elettrica posso usare il metodo **sortBy()**. Come ogni altro metodo per lavorare sugli RDD, questi si aspetta come parametro una funzione che restituisce un valore numerico per ogni valore del RDD.\n\nInoltre l'ordinamento viene fatto in direzione crescente per default, per invertire la direzione dobbiamo moltiplicare per **-1**.\n\nPer indicare che voglio ordinare sul secondo elemento della coppia *(chiave, valore)* devo usare la notazione x._2. Contrariamente agli array, dove il primo elemento ha posizione 0, nei tuple il primo elemento ha posizione 1.  ","user":"anonymous","dateUpdated":"2019-12-04T17:42:19+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Se voglio ordinare in ordine descrescente per vedere quali sono le nazioni con la più alta produzione di energia elettrica posso usare il metodo <strong>sortBy()</strong>. Come ogni altro metodo per lavorare sugli RDD, questi si aspetta come parametro una funzione che restituisce un valore numerico per ogni valore del RDD.</p>\n<p>Inoltre l&rsquo;ordinamento viene fatto in direzione crescente per default, per invertire la direzione dobbiamo moltiplicare per <strong>-1</strong>.</p>\n<p>Per indicare che voglio ordinare sul secondo elemento della coppia <em>(chiave, valore)</em> devo usare la notazione x._2. Contrariamente agli array, dove il primo elemento ha posizione 0, nei tuple il primo elemento ha posizione 1.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575477025345_-34930543","id":"20191204-173025_83324272","dateCreated":"2019-12-04T17:30:25+0100","dateStarted":"2019-12-04T17:42:19+0100","dateFinished":"2019-12-04T17:42:19+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38715"},{"text":"data_agg.sortBy( x => (x._2 * -1) ).take(10)","user":"anonymous","dateUpdated":"2019-12-05T10:52:07+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res114: Array[(String, Double)] = Array((CHN,5621543.999999972), (JPN,1011747.9999999998), (RUS,984212.0), (USA,760222.293761999), (CAN,655943.9999999995), (DEU,627697.0000000007), (BRA,590631.9999999984), (FRA,557930.0000000016), (KOR,549440.0000000001), (GBR,338923.0000000005))\n"}]},"apps":[],"jobName":"paragraph_1575474671121_-2093908626","id":"20191204-165111_1478604904","dateCreated":"2019-12-04T16:51:11+0100","dateStarted":"2019-12-05T10:52:07+0100","dateFinished":"2019-12-05T10:52:08+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38716"},{"text":"%md\nNon è una sorpresa scoprire che il primo produttore di energia elettrica è la Cina, mentre il secondo è il Giappone.","user":"anonymous","dateUpdated":"2019-12-05T10:52:16+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Non è una sorpresa scoprire che il primo produttore di energia elettrica è la Cina, mentre il secondo è il Giappone.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575477405958_131603824","id":"20191204-173645_1509429523","dateCreated":"2019-12-04T17:36:45+0100","dateStarted":"2019-12-05T10:52:16+0100","dateFinished":"2019-12-05T10:52:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38717"},{"text":"%md\nNon trattandosi di un RDD di coppie *(key, value)* non ho a disposizione il metodo reduceByKey(), quindi dobbiamo inventarci qualcosa.\n\nVediamo di passare ad un problema più complicato.\n\n### Problema\n\nVoglio sapere quanto è la produzione di energia di ogni stato suddivisa per ogni fonte di energia.\n\nIniziamo con il ridurre il dataset ai soli dati che mi interessano, in questo caso sono i campi *country*, *primary_fuel* e *estimated_generation_gwh*. Come in precedenza il valore di potenza generata viene convertito in Double. \n\nLe operazioni String -> Array e selezione dei campi possono essere fatte in un solo comando.","user":"anonymous","dateUpdated":"2019-12-05T12:12:01+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Non trattandosi di un RDD di coppie <em>(key, value)</em> non ho a disposizione il metodo reduceByKey(), quindi dobbiamo inventarci qualcosa.</p>\n<p>Vediamo di passare ad un problema più complicato.</p>\n<h3>Problema</h3>\n<p>Voglio sapere quanto è la produzione di energia di ogni stato suddivisa per ogni fonte di energia.</p>\n<p>Iniziamo con il ridurre il dataset ai soli dati che mi interessano, in questo caso sono i campi <em>country</em>, <em>primary_fuel</em> e <em>estimated_generation_gwh</em>. Come in precedenza il valore di potenza generata viene convertito in Double. </p>\n<p>Le operazioni String -&gt; Array e selezione dei campi possono essere fatte in un solo comando.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575480743924_88701771","id":"20191204-183223_2065344229","dateCreated":"2019-12-04T18:32:23+0100","dateStarted":"2019-12-05T12:12:01+0100","dateFinished":"2019-12-05T12:12:01+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38718"},{"text":"val data_reduced = data_no_header.map( x=>x.split(\",\",-1)).map(x => (x(0), x(7), convert_to_Double(x(23))))\ndata_reduced.take(5)","user":"anonymous","dateUpdated":"2019-12-05T12:12:07+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data_reduced: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[68] at map at <console>:37\r\nres116: Array[(String, String, Double)] = Array((AFG,Hydro,0.0), (AFG,Hydro,0.0), (AFG,Hydro,0.0), (AFG,Hydro,0.0), (AFG,Gas,0.0))\n"}]},"apps":[],"jobName":"paragraph_1575477828485_552309804","id":"20191204-174348_762450845","dateCreated":"2019-12-04T17:43:48+0100","dateStarted":"2019-12-05T12:12:07+0100","dateFinished":"2019-12-05T12:12:07+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38719"},{"text":"%md\nIl RDD data_reduced è costituito da tuple a 3 elementi *(String, String, Double)*.\nAdesso raggruppo usando **groupBy()** e indicando come chiavi il primo e il secondo elemento del tuple, cioè *country* e *primary_fuel*. \nIl risultato sarà un tuple in cui il primo elemento è a sua volta un tuple *(country, primary_fuel)* e il secondo elemento una Sequenza di numeri indicanti la produzioni delle varie centrali corrispondenti alla combinazione *(country, primary_fuel)*.\n\nCome esempio si una operazione di groupby su un RDD si veda la figura sotto.\n![spark groupby](https://www.1week4.com/wp-content/uploads/2019/12/apache-spark-groupby-operation1-e1575612354692.png)","user":"anonymous","dateUpdated":"2019-12-06T07:06:40+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Il RDD data_reduced è costituito da tuple a 3 elementi <em>(String, String, Double)</em>.<br/>Adesso raggruppo usando <strong>groupBy()</strong> e indicando come chiavi il primo e il secondo elemento del tuple, cioè <em>country</em> e <em>primary_fuel</em>.<br/>Il risultato sarà un tuple in cui il primo elemento è a sua volta un tuple <em>(country, primary_fuel)</em> e il secondo elemento una Sequenza di numeri indicanti la produzioni delle varie centrali corrispondenti alla combinazione <em>(country, primary_fuel)</em>.</p>\n<p>Come esempio si una operazione di groupby su un RDD si veda la figura sotto.<br/><img src=\"https://www.1week4.com/wp-content/uploads/2019/12/apache-spark-groupby-operation1-e1575612354692.png\" alt=\"spark groupby\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575527085688_-1436382928","id":"20191205-072445_534884089","dateCreated":"2019-12-05T07:24:45+0100","dateStarted":"2019-12-06T07:06:40+0100","dateFinished":"2019-12-06T07:06:40+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38720"},{"text":"val data_grouped = data_reduced.groupBy( x => (x._1, x._2))\ndata_grouped.take(1)","user":"anonymous","dateUpdated":"2019-12-05T08:04:34+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data_grouped: org.apache.spark.rdd.RDD[((String, String), Iterable[(String, String, Double)])] = ShuffledRDD[47] at groupBy at <console>:37\r\nres94: Array[((String, String), Iterable[(String, String, Double)])] = Array(((SVN,Nuclear),CompactBuffer((SVN,Nuclear,6370.0))))\n"}]},"apps":[],"jobName":"paragraph_1575524570854_372473152","id":"20191205-064250_1311600600","dateCreated":"2019-12-05T06:42:50+0100","dateStarted":"2019-12-05T08:04:34+0100","dateFinished":"2019-12-05T08:04:35+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38721"},{"text":"%md\nCome si può vedere sopra gli elementi del RDD sono ((String, String), Iterable[(String, String, Double)]), più specificatamente si può scrivere ((country, primary_fuel), Iterable[(country, primary_fuel, estimated_generation_gwh)]).\nVoglio ridurre questo tuple (mostruso) in **(String, String, Double)** ovvero (country, primary_fuel, sum(estimated_generation_gwh).\n\nPer farlo scrivo una funzione che riceve in ingresso un tuple ((String, String), Iterable[(String, String, Double)]) e fa la somma dei valori di energia generata.\nSi noti che il tipo **Iterable** è per definizione iterabile con un for loop, quindi mi basta iterare sugli elementi del Iterable e accumulare il valore in una variabile.\n\nLa dichiarazione della funzione è piuttosto elaborata:\n    `def proc_group2(input: ((String, String), Iterable[(String, String, Double)])): (String, String, Double)`","user":"anonymous","dateUpdated":"2019-12-05T08:04:12+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Come si può vedere sopra gli elementi del RDD sono ((String, String), Iterable[(String, String, Double)]), più specificatamente si può scrivere ((country, primary_fuel), Iterable[(country, primary_fuel, estimated_generation_gwh)]).<br/>Voglio ridurre questo tuple (mostruso) in <strong>(String, String, Double)</strong> ovvero (country, primary_fuel, sum(estimated_generation_gwh).</p>\n<p>Per farlo scrivo una funzione che riceve in ingresso un tuple ((String, String), Iterable[(String, String, Double)]) e fa la somma dei valori di energia generata.<br/>Si noti che il tipo <strong>Iterable</strong> è per definizione iterabile con un for loop, quindi mi basta iterare sugli elementi del Iterable e accumulare il valore in una variabile.</p>\n<p>La dichiarazione della funzione è piuttosto elaborata:<br/> <code>def proc_group2(input: ((String, String), Iterable[(String, String, Double)])): (String, String, Double)</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575528178864_617465833","id":"20191205-074258_148150483","dateCreated":"2019-12-05T07:42:58+0100","dateStarted":"2019-12-05T08:04:12+0100","dateFinished":"2019-12-05T08:04:12+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38722"},{"text":"def proc_group(input: ((String, String), Iterable[(String, String, Double)])): (String, String, Double) = {\n    //definisco una variabile (var) in quanto sum deve cambiare valore\n    var sum = 0.0\n    //facciamo un loop sull'elemento Iterable del tuple\n    for (elem <- input._2){\n        // println(\"summing \"+ elem._3)\n        sum += elem._3\n        // println(\"sum is \" + sum)\n    }\n    //l'ultima riga della funzione è quello che la funzione ritorna\n    (input._1._1, input._1._2, sum)\n}","user":"anonymous","dateUpdated":"2019-12-05T08:09:29+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"proc_group: (input: ((String, String), Iterable[(String, String, Double)]))(String, String, Double)\n"}]},"apps":[],"jobName":"paragraph_1575525134692_-785930951","id":"20191205-065214_1522049686","dateCreated":"2019-12-05T06:52:14+0100","dateStarted":"2019-12-05T08:09:29+0100","dateFinished":"2019-12-05T08:09:29+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38723"},{"text":"%md\nVerifichiamo che funzioni, prendiamo un elemento a caso e lo foniamo come input della funzione:","user":"anonymous","dateUpdated":"2019-12-05T08:06:40+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Verifichiamo che funzioni, prendiamo un elemento a caso e lo foniamo come input della funzione:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575529029948_-647876473","id":"20191205-075709_111118344","dateCreated":"2019-12-05T07:57:09+0100","dateStarted":"2019-12-05T08:06:40+0100","dateFinished":"2019-12-05T08:06:40+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38724"},{"text":"val input = data_grouped.take(5)(4)\nprintln(\"Produzione di \" + input._1._1 + '\\n')\nval prodAggregata = proc_group(input)\nprintln(f\"La produzione di energia di ${prodAggregata._1} mediante centrali di tipo ${prodAggregata._2} è di ${prodAggregata._3}\\n\")","user":"anonymous","dateUpdated":"2019-12-05T08:10:09+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Produzione di DOM\n\r\nLa produzione di energia di DOM mediante centrali di tipo Oil è di 9642.0\n\r\ninput: ((String, String), Iterable[(String, String, Double)]) = ((DOM,Oil),CompactBuffer((DOM,Oil,2117.3465558194775), (DOM,Oil,480.95486935866984), (DOM,Oil,1146.2757719714964), (DOM,Oil,4924.061757719715), (DOM,Oil,973.3610451306414)))\r\nprodAggregata: (String, String, Double) = (DOM,Oil,9642.0)\n"}]},"apps":[],"jobName":"paragraph_1575526520498_958239178","id":"20191205-071520_381723656","dateCreated":"2019-12-05T07:15:20+0100","dateStarted":"2019-12-05T08:10:09+0100","dateFinished":"2019-12-05T08:10:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38725"},{"text":"%md\nPer finire mappo il RDD data_grouped con la funzione proc_group","user":"anonymous","dateUpdated":"2019-12-05T08:10:15+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Per finire mappo il RDD data_grouped con la funzione proc_group</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575529713899_-43600893","id":"20191205-080833_1602052677","dateCreated":"2019-12-05T08:08:33+0100","dateStarted":"2019-12-05T08:10:15+0100","dateFinished":"2019-12-05T08:10:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38726"},{"text":"val data_aggregated = data_grouped.map(proc_group)\ndata_aggregated.take(5).foreach(println)","user":"anonymous","dateUpdated":"2019-12-05T21:58:17+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(SVN,Nuclear,6370.0)\r\n(GBR,Wind,32015.000000000025)\r\n(USA,Wind,4317.620910000014)\r\n(ARM,Nuclear,2465.0)\r\n(DOM,Oil,9642.0)\r\ndata_aggregated: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[51] at map at <console>:45\n"}]},"apps":[],"jobName":"paragraph_1575525211060_-509662256","id":"20191205-065331_103919707","dateCreated":"2019-12-05T06:53:31+0100","dateStarted":"2019-12-05T08:11:27+0100","dateFinished":"2019-12-05T08:11:28+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38727"},{"text":"%md\nAdesso se voglio posso filtrare tutti i dati relativi agli USA per esempio","user":"anonymous","dateUpdated":"2019-12-05T08:12:37+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Adesso se voglio posso filtrare tutti i dati relativi agli USA per esempio</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575529895618_-236650015","id":"20191205-081135_688233774","dateCreated":"2019-12-05T08:11:35+0100","dateStarted":"2019-12-05T08:12:37+0100","dateFinished":"2019-12-05T08:12:37+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38728"},{"text":"data_aggregated.filter( x => x._1==\"USA\").collect().foreach(println)","user":"anonymous","dateUpdated":"2019-12-05T08:13:31+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(USA,Wind,4317.620910000014)\r\n(USA,Petcoke,0.0)\r\n(USA,Solar,7617.67086999997)\r\n(USA,Storage,0.0)\r\n(USA,Geothermal,3120.476999999997)\r\n(USA,Waste,0.0)\r\n(USA,Oil,28942.813017000062)\r\n(USA,Gas,159085.28519300214)\r\n(USA,Nuclear,0.0)\r\n(USA,Other,3226.2118600000013)\r\n(USA,Cogeneration,0.0)\r\n(USA,Coal,484977.23213499994)\r\n(USA,Hydro,29065.432300000262)\r\n(USA,Biomass,39869.550477000004)\r\n"}]},"apps":[],"jobName":"paragraph_1575525859255_1722884412","id":"20191205-070419_1942159088","dateCreated":"2019-12-05T07:04:19+0100","dateStarted":"2019-12-05T08:13:31+0100","dateFinished":"2019-12-05T08:13:32+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38729"},{"text":"%md\nOppure posso selezionare i dati relativi alla produzione mediante \"Waste\" nel mondo","user":"anonymous","dateUpdated":"2019-12-05T21:57:50+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Oppure posso selezionare i dati relativi alla produzione mediante &ldquo;Waste&rdquo; nel mondo</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575525933943_188989579","id":"20191205-070533_1951415848","dateCreated":"2019-12-05T07:05:33+0100","dateStarted":"2019-12-05T21:57:50+0100","dateFinished":"2019-12-05T21:57:50+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38730"},{"text":"data_aggregated.filter( x => x._2==\"Waste\").collect().foreach(println)","user":"anonymous","dateUpdated":"2019-12-05T08:15:16+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(HND,Waste,0.0)\r\n(GTM,Waste,0.0)\r\n(RWA,Waste,0.0)\r\n(USA,Waste,0.0)\r\n(DEU,Waste,13503.0)\r\n(BEL,Waste,2083.0)\r\n(ESP,Waste,1371.9999999999998)\r\n(AUS,Waste,0.0)\r\n(JPN,Waste,6595.0)\r\n(KHM,Waste,0.0)\r\n(KOR,Waste,696.0)\r\n(GRC,Waste,100.0)\r\n(SGP,Waste,1260.0)\r\n(NIC,Waste,0.0)\r\n(PRT,Waste,489.00000000000006)\r\n(GBR,Waste,4038.0000000000014)\r\n(BRA,Waste,0.0)\r\n(ZAF,Waste,0.0)\r\n"}]},"apps":[],"jobName":"paragraph_1575530073782_-1386714531","id":"20191205-081433_265485026","dateCreated":"2019-12-05T08:14:33+0100","dateStarted":"2019-12-05T08:15:16+0100","dateFinished":"2019-12-05T08:15:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38731"},{"text":"%md\nE scopriamo che lo stato dove si produce maggiormente energia bruciando l'immondizia è l'industrializzatissima Germania! \nProprio a questo serve fare questo tipo di analisi: a leggere fatti che sono nascosti in una marea di dati. ","user":"anonymous","dateUpdated":"2019-12-05T08:17:55+0100","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>E scopriamo che lo stato dove si produce maggiormente energia bruciando l&rsquo;immondizia è l&rsquo;industrializzatissima Germania!<br/>Proprio a questo serve fare questo tipo di analisi: a leggere fatti che sono nascosti in una marea di dati.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575530116029_-1427817580","id":"20191205-081516_41290319","dateCreated":"2019-12-05T08:15:16+0100","dateStarted":"2019-12-05T08:17:55+0100","dateFinished":"2019-12-05T08:17:55+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38732"}],"name":"RDD API example2","id":"2EUHSREGV","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"simple","personalizedMode":"false"},"info":{}}