{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('data/csv/GlobalLandTemperaturesByMajorCity.csv.zip')\n",
    "zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('config.cfg'))\n",
    "\n",
    "KEY     = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "SECRET  = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "S3_BUCKET   = config['S3']['S3_bucket']\n",
    "S3_LAKE_RAWDATA = config['S3']['RAW_DATA']\n",
    "\n",
    "I94_DATA = config['S3']['I94_DATA']\n",
    "SHARED_STORAGE = config['S3']['SHARED_STORAGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering file:  ../../data/I94_SAS_Labels_Descriptions.SAS raw/I94_SAS_Labels_Descriptions.SAS \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat raw/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy I94 data into S3\n",
    "filepath = \"../../data/\"\n",
    "\n",
    "if os.path.isdir(filepath):\n",
    "    filelist = os.walk(filepath)\n",
    "\n",
    "    for root, subFolders, files in filelist:\n",
    "        for file in files:\n",
    "            origin_file_path_name = os.path.join(root, file)\n",
    "            bucket_file_path_name = os.path.join(S3_LAKE_RAWDATA, origin_file_path_name.replace(SHARED_STORAGE, ''))\n",
    "\n",
    "            print('Transfering file: ', origin_file_path_name, bucket_file_path_name, '\\n')\n",
    "\n",
    "#             s3.meta.client.upload_file(origin_file_path_name, S3_BUCKET, bucket_file_path_name)\n",
    "elif os.path.isfile(filepath):\n",
    "    origin_file_path_name = filepath\n",
    "    bucket_file_path_name = os.path.join(S3_LAKE_RAWDATA)\n",
    "    print('Transfering file: ', origin_file_path_name, bucket_file_path_name, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fileTransferToS3(origin_filepath, destination_folder, destination_file=None):\n",
    "    \"\"\"\n",
    "    transfer folder or file to S3 bucket\n",
    "    :origin_filepath, a folder or a file. If a folder is given, all the files inside are transferred to the remote destination_folder.\n",
    "    :destination_folder, folder in the S3 bucket, where to copy the files\n",
    "    :destination_file, it is valid only when origin_filepath is a file. If None, the name of the file in the origin_filepath will be used\n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    #if a directory is passed as origin_filepath parameter all files are copied into the destination_folder\n",
    "    if os.path.isdir(origin_filepath):\n",
    "        filelist = os.walk(origin_filepath)\n",
    "\n",
    "        for root, subFolders, files in filelist:\n",
    "            for file in files:\n",
    "                origin_file_path_name = os.path.join(root, file)\n",
    "                bucket_file_path_name = os.path.join(destination_folder, origin_file_path_name.replace(origin_filepath, ''))\n",
    "\n",
    "                print('Transfering file: ', origin_file_path_name, bucket_file_path_name, '\\n')\n",
    "\n",
    "                s3.meta.client.upload_file(origin_file_path_name, S3_BUCKET, bucket_file_path_name)\n",
    "    \n",
    "    #if a file is passed as origin_filepath\n",
    "    elif os.path.isfile(origin_filepath):\n",
    "        origin_file_path_name = origin_filepath\n",
    "        if destination_file == None:\n",
    "            filename = re.match('([^\\/]*$)', origin_filepath)[0]\n",
    "            bucket_file_path_name = os.path.join(destination_folder, filename)\n",
    "        else:\n",
    "            bucket_file_path_name = os.path.join(destination_folder, destination_file)\n",
    "        \n",
    "        print('Transfering file: ', origin_file_path_name, bucket_file_path_name, '\\n')\n",
    "        s3.meta.client.upload_file(origin_file_path_name, S3_BUCKET, bucket_file_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i94_apr16_sub.sas7bdat'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.findall('([^\\/]*$)', '../../data/i94_apr16_sub.sas7bdat')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering file:  ../../data/I94_SAS_Labels_Descriptions.SAS ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/I94_SAS_Labels_Descriptions.SAS \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileTransferToS3('../../data/', '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat raw/i94_apr16_sub.sas7bdat \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileTransferToS3('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', S3_LAKE_RAWDATA, 'i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def addMetadataRaw(json_filepath, origin_filepath, notes = '', access_granted_to = 'anyone', expire_date = '5y'):\n",
    "    \n",
    "    \"\"\"\n",
    "    :notes individual notes for each file\n",
    "    :access_granted_to, possibly values are secret, admin, anyone, policy_name\n",
    "    \"\"\"\n",
    "    \n",
    "    import pathlib\n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    fname = pathlib.Path(origin_filepath)\n",
    "    \n",
    "    if os.path.isfile(json_filepath):\n",
    "        with open(json_filepath) as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = {}\n",
    "    \n",
    "    metadata['rawData'].append({\n",
    "        'filename': re.findall('([^\\/]*$)', origin_filepath)[0],\n",
    "        'added_by': KEY,\n",
    "        'date_added': datetime.datetime.timestamp(datetime.datetime.now()),\n",
    "        'modified_on': fname.stat().st_mtime,\n",
    "        'notes': notes,\n",
    "        'access_granted_to': access_granted_to,\n",
    "        'expire_date': expire_date})\n",
    "    \n",
    "    with open(json_filepath, 'w') as outfile:\n",
    "        json.dump(metadata, outfile)\n",
    "    \n",
    "addMetadataRaw('./data/metadata/rawDataMetadata.json', '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', access_granted_to='I94_policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/metadata/rawDataMetadata.json') as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawData': [{'filename': 'i94_apr16_sub.sas7bdat',\n",
       "   'added_by': 'AKIARPUGMYML52GPKEOR',\n",
       "   'date_added': 1629627062.301982,\n",
       "   'modified_on': 1527772804.0,\n",
       "   'notes': '',\n",
       "   'access_granted_to': 'I94_policy',\n",
       "   'expire_date': '5y'}]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-e035dbb1cc59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/metadata/rawDataMetadata.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \"\"\"\n\u001b[0;32m--> 296\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "json.load('./data/metadata/rawDataMetadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1629626188.022611"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.timestamp(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i94_apr16_sub.sas7bdat'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Naive metadata system:\n",
    "- file name: file being processed\n",
    "- added by: user logged as | aws access id\n",
    "- date added: date of processing\n",
    "- modified on: fname.stat()\n",
    "- notes\n",
    "- access granted to (roles): I94 access policy | weather data access policy | \n",
    "- expire date: 5 years (default)\n",
    "\n",
    "save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "def creation_date(path_to_file):\n",
    "    \"\"\"\n",
    "    Try to get the date that a file was created, falling back to when it was\n",
    "    last modified if that isn't possible.\n",
    "    See http://stackoverflow.com/a/39501288/1709587 for explanation.\n",
    "    \"\"\"\n",
    "    if platform.system() == 'Windows':\n",
    "        return os.path.getctime(path_to_file)\n",
    "    else:\n",
    "        stat = os.stat(path_to_file)\n",
    "        try:\n",
    "            return stat.st_birthtime\n",
    "        except AttributeError:\n",
    "            # We're probably on Linux. No easy way to get creation dates here,\n",
    "            # so we'll settle for when its content was last modified.\n",
    "            return stat.st_mtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1629623271.9847949"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creation_date('Untitled1.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-27 17:45:11\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import datetime\n",
    "fname = pathlib.Path('spark_4_emr_I94_processing.py')\n",
    "mtime = datetime.datetime.fromtimestamp(fname.stat().st_mtime)\n",
    "print(mtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname.stat().st_uid # if owned by root user equals 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 25] Inappropriate ioctl for device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0f8aa747d9ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 25] Inappropriate ioctl for device"
     ]
    }
   ],
   "source": [
    "os.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 8, 22, 9, 31, 52, 281595)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lit, explode, split, regexp_extract, col, isnan, isnull, desc, when, sum, to_date, desc, regexp_replace, count, to_timestamp\n",
    "from pyspark.sql.types import IntegerType, TimestampType\n",
    "\n",
    "import boto3\n",
    "import configparser\n",
    "\n",
    "#import custom module\n",
    "from lib import emr_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.access.key\", KEY)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['query_0'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "data['query_0'].append({\n",
    "    'nome':'uno',\n",
    "    'cognome': 'due',\n",
    "    'via': 'suc'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_0': [{'nome': 'uno', 'cognome': 'due', 'via': 'suc'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bb4ab90a6e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    346\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    347\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    346\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    347\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lit, explode, split, regexp_extract, col, isnan, isnull, desc, when, sum, to_date, desc, regexp_replace, count, to_timestamp\n",
    "from pyspark.sql.types import IntegerType, TimestampType\n",
    "\n",
    "import boto3\n",
    "import configparser\n",
    "\n",
    "#import custom module\n",
    "from lib import emr_cluster\n",
    "\n",
    "import logging\n",
    "\n",
    "# Logging to the terminal\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "\n",
    "class sparkJob():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        logger.info('Loading configuration')\n",
    "        \n",
    "        # parsing configuration parameters\n",
    "        config = configparser.ConfigParser()\n",
    "\n",
    "        config.read_file(open('config.cfg'))\n",
    "\n",
    "        self.S3_BUCKET         = config['S3']['S3_bucket']\n",
    "        self.S3_LAKE_RAWDATA   = config['S3']['RAW_DATA']\n",
    "        self.S3_PROC_DATA      = config['S3']['PROC_DATA']\n",
    "        self.S3_PROC_DATA_JSON = config['S3']['PROC_DATA_JSON']\n",
    "        self.S3_I94_DATA       = config['S3']['I94_DATA']\n",
    "\n",
    "        self.S3_bucket_I94 = os.path.join('s3a://', self.S3_BUCKET, self.S3_PROC_DATA, self.S3_I94_DATA)\n",
    "\n",
    "        self.AWS_ACCESS_KEY_ID     = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "        self.AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "    \n",
    "    def new_sparkSession(self):\n",
    "        \"\"\"\n",
    "        instantiate a new sparkSession, loading the package needed (parso, spark.sas, hadoop-aws)\n",
    "        configure for s3a access\n",
    "        \"\"\"\n",
    "        logger.info('Instantiating new sparkSession')\n",
    "        \n",
    "        self.spark = SparkSession.builder\\\n",
    "            .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        hadoopConf = self.spark.sparkContext._jsc.hadoopConfiguration()\n",
    "        hadoopConf.set(\"fs.s3a.access.key\", self.AWS_ACCESS_KEY_ID)\n",
    "        hadoopConf.set(\"fs.s3a.secret.key\", self.AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "    def save_query(self, query_name, filename, first_day, last_day, query_description=''):\n",
    "        \n",
    "        \n",
    "        logger.info('Saving query results')\n",
    "        \n",
    "        from datetime import datetime\n",
    "        \n",
    "        if os.path.isfile(filename):\n",
    "            with open(filename) as f:\n",
    "                query_results = json.load(f)\n",
    "        else:\n",
    "            query_results = {}\n",
    "        \n",
    "        query_name = query_name+'-'+str(int(datetime.timestamp(datetime.now())*1e6))\n",
    "        query_results[query_name] = []\n",
    "        \n",
    "        query_results[query_name].append({\n",
    "            'name': query_name,\n",
    "            'description': query_description,\n",
    "            'start_date': first_day,\n",
    "            'end_date': last_day,\n",
    "            'third_dimension': self.z,\n",
    "            'x': self.x,\n",
    "            'y': self.y\n",
    "        })\n",
    "\n",
    "        with open(filename, 'w') as outfile:\n",
    "                json.dump(query_results, outfile)\n",
    "        \n",
    "        \n",
    "        uploadToS3(filename)\n",
    "        \n",
    "        \n",
    "                \n",
    "    def uploadToS3(self, filename):\n",
    "        '''\n",
    "        upload query_result file to S3\n",
    "        '''\n",
    "        s3 = boto3.resource('s3',\n",
    "                    region_name=\"us-west-2\",\n",
    "                    aws_access_key_id     = self.AWS_ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key = self.AWS_SECRET_ACCESS_KEY)\n",
    "        s3.meta.client.upload_file(filename, self.S3_BUCKET, os.path.join('queries', filename))     \n",
    "        \n",
    "        os.remove(filename)\n",
    "                \n",
    "\n",
    "class sparkQuery(sparkJob):\n",
    "    \n",
    "    def query0(self, query_name, first_day, last_day):\n",
    "\n",
    "        logger.info(f'Executing the query: {query_name}')\n",
    "\n",
    "        df_I94 = self.spark.read.format(\"parquet\").load(self.S3_bucket_I94).where((col('arrdate')>=first_day) & (col('arrdate')<=last_day))\n",
    "\n",
    "        df_query = df_I94.groupBy('arrdate').sum('count').orderBy('arrdate').toPandas()\n",
    "        \n",
    "        self.x = df_query['arrdate'].apply(lambda x: time.mktime(x.timetuple())).values.tolist() #modify: date to timestamp to save in json file\n",
    "        self.y = df_query['sum(count)'].values.tolist()\n",
    "        self.z = ''\n",
    "    \n",
    "   \n",
    "    def query1(self, query_name, first_day, last_day):\n",
    "        \n",
    "        logger.info(f'Executing the query: {query_name}')\n",
    "        \n",
    "        df_I94 = self.spark.read.format(\"parquet\").load(self.S3_bucket_I94).where((col('arrdate')>=first_day) & (col('arrdate')<=last_day))\n",
    "        \n",
    "        retain_columns = ['i94port', 'arrdate','count']\n",
    "        df_I94_port_vs_date = df_I94.select(retain_columns).groupBy('i94port').pivot('arrdate').sum('count')\n",
    "        df_I94_port_vs_date = df_I94_port_vs_date.na.fill(0)\n",
    "        \n",
    "        self.x = df_I94_port_vs_date.toPandas().values.tolist()\n",
    "        self.y = ''\n",
    "        self.z = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:09:20,671 - __main__ - INFO - Loading configuration\n",
      "2021-08-23 17:09:20,677 - __main__ - INFO - Instantiating new sparkSession\n",
      "2021-08-23 17:09:20,688 - __main__ - INFO - Executing the query: query_0\n"
     ]
    }
   ],
   "source": [
    "#query0\n",
    "first_day = '2016-04-30'\n",
    "last_day  = '2016-04-30'\n",
    "query_name = 'query_0'\n",
    "query_description = 'How are distribuited the arrivals in USA, in a given period?'\n",
    "emrQuery = sparkQuery()\n",
    "emrQuery.new_sparkSession()\n",
    "emrQuery.query0(query_name, first_day, last_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1450 (char 1449)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8e42300e9573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'query_results.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mquery_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1450 (char 1449)"
     ]
    }
   ],
   "source": [
    "with open('query_results.json') as f:\n",
    "    query_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-23 17:52:43,190 - __main__ - INFO - Saving query results\n"
     ]
    }
   ],
   "source": [
    "emrQuery.save_query(query_name, 'query_results.json', first_day, last_day, query_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#query1\n",
    "first_day = '2016-04-30'\n",
    "last_day  = '2016-04-30'\n",
    "query_name = 'query_1'\n",
    "query_description = 'For each port, in a given period, how many arrivals by day there are?'\n",
    "emrQuery.query1(query_name, first_day, last_day)\n",
    "emrQuery.save_query(query_name, 'query_results.json', first_day, last_day, query_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.remove('query_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
