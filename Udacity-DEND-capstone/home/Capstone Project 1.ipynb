{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "The Organization for Tourism Development (**OTD**) want to analyze migration flux in USA, in order to find insights to significantly and sustainably develop the tourism in USA. \n",
    "\n",
    "To support their core idea they have identified a set of analysis/queries they want to run on the raw data available.\n",
    "\n",
    "The project deals with building a data pipeline, to go from raw data to the data insights on the migration flux.\n",
    "\n",
    "The raw data are gathered from different sources, saved in files and made available for download.\n",
    "\n",
    "The project shows the execution and decisional flow, specifically:\n",
    "\n",
    "* Chapter 1: **Scope the Project and Gather Data**. In Step 1 we give more details on the scope of the project. \n",
    "    - Describe the data and how they have been obtained \n",
    "    - Answer the question \"how to achieve the target?\"\n",
    "    - What infrastructure (storage, computation, communication) has been used and why \n",
    "* Chapter 2: **Explore and Assess the Data**.  A first look at the data.\n",
    "    - Explore the data\n",
    "    - Check the data for issues, for example null, NaN, or other inconsistencies\n",
    "* Chapter 3: **Define the Data Model**. How to organize the data to satisfy the analytical needs.\n",
    "    - Why this data model has been chosen\n",
    "    - How it is implemented\n",
    "* Chapter 4: **Run ELT to Model the Data**. \n",
    "    - Load the data from S3 into the SQL database, if any\n",
    "    - Perform quality checks on the database\n",
    "    - Perform example queries\n",
    "* Chapter 5: **Complete Project Write Up**\n",
    "    - Documentation of the project\n",
    "    - Possible scenario extensions \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> p { max-width:90% !important; } h1 {font-size:2rem!important } h2 {font-size:1.6rem!important } \n",
       "h3 {font-size:1.4rem!important } h4 {font-size:1.3rem!important }h5 {font-size:1.2rem!important }h6 {font-size:1.1rem!important }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lit, explode, split, regexp_extract, col, isnan, isnull, desc, when, sum, to_date, desc, regexp_replace, count, to_timestamp\n",
    "from pyspark.sql.types import IntegerType, TimestampType\n",
    "\n",
    "import boto3\n",
    "import configparser\n",
    "\n",
    "#import custom module\n",
    "from lib import emr_cluster\n",
    "\n",
    "#setting visualization options\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "\n",
    "# modify visualization of the notebook, for easier view\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"\"\"<style> p { max-width:90% !important; } h1 {font-size:2rem!important } h2 {font-size:1.6rem!important } \n",
    "h3 {font-size:1.4rem!important } h4 {font-size:1.3rem!important }h5 {font-size:1.2rem!important }h6 {font-size:1.1rem!important }</style>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "## Index\n",
    "\n",
    "- <a href=\"#1.-Scope-of-the-Project\">1. Scope of the Project</a>\n",
    "- 1.1 What data\n",
    "- 1.2 What tools\n",
    "- <a href=\"#1.3-The-I94-immigration-data\">1.3 The I94 immigration data</a>\n",
    "- 1.3.1 What is an I94?\n",
    "- 1.3.2 The I94 dataset\n",
    "- 1.3.3 The SAS date format\n",
    "- 1.3.4 Loading I94 SAS data\n",
    "- <a href=\"#1.4-World-temperature-data\">1.4 World Temperature Data</a>\n",
    "- 1.5 Airport Code Table\n",
    "- 1.6 U.S. City Demographic Data\n",
    "- <a href=\"#2.-Data-Exploration\">2. Data Exploration</a>\n",
    "- 2.1 The I94 dataset\n",
    "- 2.2 I94 SAS data load\n",
    "- 2.3 Explore I94 data\n",
    "- 2.4 Cleaning the I94 dataset\n",
    "- 2.5 Store I94 data as parquet\n",
    "- 2.6 Airport codes dataset: load, clean, save\n",
    "- <a href=\"#3.-The-Data-Model\">3. The Data Model</a>\n",
    "- 3.1 Mapping Out Data Pipelines\n",
    "- <a href=\"#4.-Run-Pipelines-to-Model-the-Data\">4. Run Pipelines to Model the Data</a>\n",
    "- <a href=\"#4.1-Provision-the-AWS-S3-infrastructure\">4.1 Provision the AWS S3 infrastructure</a>\n",
    "- <a href=\"#4.2-Transfer-raw-data-to-S3-bucket\">4.2 Transfer raw data to S3 bucket</a>\n",
    "- <a href=\"#4.3-EMR-cluster-on-EC2\">4.3 EMR cluster on EC2</a>\n",
    "- 4.3.1 Provision the EMR cluster\n",
    "- 4.3.2 Coded fields: I94CIT and I94RES\n",
    "- 4.3.3 Coded field: I94PORT\n",
    "- 4.3.4 Data cleaning\n",
    "- <a href=\"#4.3.5-Save-clean-data-(parquet/json)-to-S3\">4.3.5 Save clean data (parquet/json) to S3</a>\n",
    "- 4.3.6 Loading, cleaning and saving airport codes\n",
    "- <a href=\"#4.4-Querying-data-on-the-fly\">4.4 Querying data on-the-fly</a>\n",
    "- 4.5 Querying data using the SQL querying style\n",
    "- 4.6 Data Quality Checks\n",
    "- <a href=\"#5.-Write-Up\">5. Write up</a>\n",
    "- 6. Lesson learned\n",
    "- 7. References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 1. Scope of the Project\n",
    "\n",
    "\n",
    "The OTD want to run **pre-defined queries** on the data, with periodical timing.\n",
    "\n",
    "They also want to maintain the **flexibility** to run different queries on the data, using BI tools connected to an SQL-like database.\n",
    "\n",
    "The core data is the dataset provided by US governative agencies filing request of access in the USA (I94 module).\n",
    "    \n",
    "They also have other lower value data available, that are not part of the core analysis, whose use is unclear, therefore are stored in the data lake for a possible future use.\n",
    "    \n",
    "### 1.1 What data\n",
    "Following datasets are used in the project:\n",
    "- **I94 immigration data for year 2016**. Used for the main analysis\n",
    "- World Temperature Data\n",
    "- Airport Code Table\n",
    "- U.S. City Demographic Data\n",
    "    \n",
    "### 1.2 What tools\n",
    "\n",
    "Because of the nature of the data and the analysis that must be performed, not time-critical analysis, monthly or weekly batch, the choice fell on a cheaper S3-based **data lake** with on-demand on-the-fly analytical capability: **EMR cluster** with Apache **Spark**, and optionally Apache **Airflow** for scheduled execution (not implemented here).\n",
    "\n",
    "\n",
    "The architecture shown below has been implemented. \n",
    "\n",
    "![architecture](./figures/udacity-dend-3.png)\n",
    "\n",
    "\n",
    "- Starting from a **common storage** solution (currently Udacity workspace) where both the OTD and its partners have access, the data is then ingested into an **S3 bucket**, in raw format\n",
    "\n",
    "- To ease future operations, the data is immediately processed, validated and cleansed using a **Spark cluster** and stored into S3 in **parquet** format. Raw and parquet data formats coesist in the data lake.\n",
    "\n",
    "- By default, the project doesn''t use costly Redshift cluster, but data are queried **in-place** on the S3 parquet data.\n",
    "\n",
    "- The **EMR** cluster serves the analytical needs of the project. SQL based queries are performed using **Spark SQL** directly on the S3 parquet data\n",
    "\n",
    "- A **Spark job** can be triggered monthly, using the Parquet data. The data is aggregated to gain insights on the evolution of the migration flows\n",
    "\n",
    "### 1.3 The I94 immigration data\n",
    "\n",
    "The data are provided by the [US National Tourism and Trade Office](https://www.trade.gov/national-travel-and-tourism-office). It is a collection of all I94 that have been filed in 2016.\n",
    "\n",
    "#### 1.3.1 What is an I94?\n",
    "\n",
    "To give some context is useful to explain what an I94 file is.\n",
    "\n",
    "From the government [website](https://get-connected.fnal.gov/visa/entering-usa/i-94/): \n",
    "\"The I-94 is the Arrival/Departure Record, in either paper or electronic format, issued by a Customs and Border Protection (CBP) Officer to foreign visitors entering the United States.\"\n",
    "\n",
    "\n",
    "#### 1.3.2 The I94 dataset\n",
    "\n",
    "Each record contains these fields:\n",
    "\n",
    "- CICID, unique numer of the file\n",
    "- I94YR, 4 digit year of the application\n",
    "- I94MON, Numeric month of the application\n",
    "- I94CIT, city where the applicant is living\n",
    "- I94RES, state where the applicant is living\n",
    "- I94PORT, location (port) where the application is issued\n",
    "- ARRDATE, arrival date in USA in SAS date format\n",
    "- I94MODE, how did the applicant arrived in the USA\n",
    "- I94ADDR, US state where the port is\n",
    "- DEPDATE is the Departure Date from the USA\n",
    "- I94BIR, age of applicant in years\n",
    "- I94VISA, what kind of VISA\n",
    "- COUNT, used for summary statistics, always 1\n",
    "- DTADFILE, date added to I-94 Files\n",
    "- VISAPOST, department of State where where Visa was issued\n",
    "- OCCUP, occupation that will be performed in U.S.\n",
    "- ENTDEPA, arrival Flag\n",
    "- ENTDEPD, departure Flag\n",
    "- ENTDEPU, update Flag\n",
    "- MATFLAG, match flag\n",
    "- BIRYEAR, 4 digit year of birth\n",
    "- DTADDTO, date to which admitted to U.S. (allowed to stay until) \n",
    "- GENDER, non-immigrant sex \n",
    "- INSNUM, INS number\n",
    "- AIRLINE, airline used to arrive in USA\n",
    "- ADMNUM, admission Number\n",
    "- FLTNO, flight number of Airline used to arrive in USA\n",
    "- VISATYPE, class of admission legally admitting the non-immigrant to temporarily stay in USA\n",
    "\n",
    "More details in the file *I94_SAS_Labels_Descriptions.SAS*\n",
    "\n",
    "#### 1.3.3 The SAS date format\n",
    "Represent any date D0 as the number of days between D0 and the 1th January 1960 \n",
    "\n",
    "#### 1.3.4 Loading I94 SAS data\n",
    "The package **saurfang:spark-sas7bdat:2.0.0-s_2.11** and the dependency **parso-2.0.8** are needed to read SAS data format.\n",
    "\n",
    "To load them use the config option *spark.jars* and give the URL of the repositories, as Spark itself wasn't able to resolve the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .config(\"spark.jars\",\"https://repo1.maven.org/maven2/com/epam/parso/2.0.8/parso-2.0.8.jar,https://repos.spark-packages.org/saurfang/spark-sas7bdat/2.0.0-s_2.11/spark-sas7bdat-2.0.0-s_2.11.jar\")\\\n",
    "                    .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1.4 World temperature data\n",
    "The dataset is from Kaggle. It can be found [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "The dataset contains temperature data:\n",
    "- Global Land and Ocean-and-Land Temperatures (GlobalTemperatures.csv)\n",
    "- Global Average Land Temperature by Country (GlobalLandTemperaturesByCountry.csv)\n",
    "- Global Average Land Temperature by State (GlobalLandTemperaturesByState.csv)\n",
    "- Global Land Temperatures By Major City (GlobalLandTemperaturesByMajorCity.csv)\n",
    "- Global Land Temperatures By City (GlobalLandTemperaturesByCity.csv)\n",
    "\n",
    "Below is a snapshop of the global land temperatures by city file.\n",
    "![land temp](./figures/worldtemp.png)\n",
    "\n",
    "### 1.5 Airport codes data\n",
    "This is a table of airport codes, and information on the corresponding cities, like gps coordinates, elevation, country, etc. It comes from Datahub [website](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "Below is a snapshot of the data.\n",
    "![airport codes](./figures/airport_codes.png)\n",
    "\n",
    "\n",
    "### 1.6 U.S. City Demographic Data\n",
    "The dataset comes from OpenSoft. It can be found [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "\n",
    "It contains demographic info on US cities. The info are organized like in the picture below.\n",
    "![us city demo](./figures/US_demo_city.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "In this chapter we proceed identifying data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "The purpose is to identify the flow in the data pipeline to programmatically correct data issues.\n",
    "\n",
    "In this step we work on local data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.1 The I94 dataset\n",
    "- How many files are in the I94 dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 12 files\n"
     ]
    }
   ],
   "source": [
    "I94_DATASET_PATH = '../../data/18-83510-I94-Data-2016/'\n",
    "\n",
    "filelist = os.listdir(I94_DATASET_PATH)\n",
    "print(\"The dataset contains {} files\".format(len(filelist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- What is the size of the files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i94_apr16_sub.sas7bdat - dim(bytes): 471990272\n",
      "i94_sep16_sub.sas7bdat - dim(bytes): 569180160\n",
      "i94_nov16_sub.sas7bdat - dim(bytes): 444334080\n",
      "i94_mar16_sub.sas7bdat - dim(bytes): 481296384\n",
      "i94_jun16_sub.sas7bdat - dim(bytes): 716570624\n",
      "i94_aug16_sub.sas7bdat - dim(bytes): 625541120\n",
      "i94_may16_sub.sas7bdat - dim(bytes): 525008896\n",
      "i94_jan16_sub.sas7bdat - dim(bytes): 434176000\n",
      "i94_oct16_sub.sas7bdat - dim(bytes): 556269568\n",
      "i94_jul16_sub.sas7bdat - dim(bytes): 650117120\n",
      "i94_feb16_sub.sas7bdat - dim(bytes): 391905280\n",
      "i94_dec16_sub.sas7bdat - dim(bytes): 523304960\n"
     ]
    }
   ],
   "source": [
    "for file in filelist:\n",
    "    size = os.path.getsize('{}/{}'.format(I94_DATASET_PATH, file))\n",
    "    print('{} - dim(bytes): {}'.format(file, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.2 I94 SAS data load\n",
    "To read SAS data format I need to specify the *com.github.saurfang.sas.spark* format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "I94_TEST_FILE = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "df_I94 = spark.read.format('com.github.saurfang.sas.spark').load(I94_TEST_FILE).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130811</td>\n",
       "      <td>SEO</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0  6.0    2016.0  4.0     692.0   692.0   XXX     20573.0 NaN       None     \n",
       "1  7.0    2016.0  4.0     254.0   276.0   ATL     20551.0  1.0      AL       \n",
       "2  15.0   2016.0  4.0     101.0   101.0   WAS     20545.0  1.0      MI       \n",
       "3  16.0   2016.0  4.0     101.0   101.0   NYC     20545.0  1.0      MA       \n",
       "4  17.0   2016.0  4.0     101.0   101.0   NYC     20545.0  1.0      MA       \n",
       "\n",
       "   depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd  \\\n",
       "0 NaN       37.0    2.0      1.0    None      None     None  T       None     \n",
       "1 NaN       25.0    3.0      1.0    20130811  SEO      None  G       None     \n",
       "2  20691.0  55.0    2.0      1.0    20160401  None     None  T       O        \n",
       "3  20567.0  28.0    2.0      1.0    20160401  None     None  O       O        \n",
       "4  20567.0  4.0     2.0      1.0    20160401  None     None  O       O        \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline        admnum  \\\n",
       "0  U       None    1979.0   10282016  None   None   None    1.897628e+09   \n",
       "1  Y       None    1991.0   D/S       M      None   None    3.736796e+09   \n",
       "2  None    M       1961.0   09302016  M      None   OS      6.666432e+08   \n",
       "3  None    M       1988.0   09302016  None   None   AA      9.246846e+10   \n",
       "4  None    M       2012.0   09302016  None   None   AA      9.246846e+10   \n",
       "\n",
       "   fltno visatype  \n",
       "0  None   B2       \n",
       "1  00296  F1       \n",
       "2  93     B2       \n",
       "3  00199  B2       \n",
       "4  00199  B2       "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_I94.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Let's see the **schema** Spark applied on reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_I94.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The most columns are categorical data, this means the information is coded, for example `I94CIT=101`, 101 is the country code for Albania.\n",
    "\n",
    "Other columns represent integer data. \n",
    "\n",
    "It appears clear that there is no need to have data that are defined as **double** => let's change those fields to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "toInt = udf(lambda x: int(x) if x!=None else x, IntegerType())\n",
    "\n",
    "for colname, coltype in df_I94.dtypes:\n",
    "    if coltype == 'double':\n",
    "        df_I94 = df_I94.withColumn(colname, toInt(colname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Verifying the schema is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: integer (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: integer (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: integer (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_I94.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- convert string columns **dtadfile** and **dtaddto** to date type\n",
    "\n",
    "These fields come in a simple string format. To be able to run time-based queries they are converted to **date** type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_I94 = df_I94.withColumn('dtadfile', to_date(col('dtadfile'), format='yyyyMMdd'))\\\n",
    "               .withColumn('dtadddto', to_date(col('dtaddto'), format='MMddyyyy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- convert columns *arrdate* and *depdate* from SAS-date format to a timestamp type. \n",
    "\n",
    "A date in SAS format is simply the number of days between the chosen date and the reference date (01-01-1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "      <th>dtadddto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>692</td>\n",
       "      <td>692</td>\n",
       "      <td>XXX</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1897628485</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>254</td>\n",
       "      <td>276</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-08-11</td>\n",
       "      <td>SEO</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-558170966</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>2016-08-25</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>666643185</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>-2020819182</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>-2020817382</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "      <td>2016-09-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid  i94yr  i94mon  i94cit  i94res i94port     arrdate  i94mode i94addr  \\\n",
       "0  6      2016   4       692     692     XXX     2016-04-29 NaN       None     \n",
       "1  7      2016   4       254     276     ATL     2016-04-07  1.0      AL       \n",
       "2  15     2016   4       101     101     WAS     2016-04-01  1.0      MI       \n",
       "3  16     2016   4       101     101     NYC     2016-04-01  1.0      MA       \n",
       "4  17     2016   4       101     101     NYC     2016-04-01  1.0      MA       \n",
       "\n",
       "      depdate  i94bir  i94visa  count    dtadfile visapost occup entdepa  \\\n",
       "0  1900-01-01  37      2        1      None        None     None  T        \n",
       "1  1900-01-01  25      3        1      2013-08-11  SEO      None  G        \n",
       "2  2016-08-25  55      2        1      2016-04-01  None     None  T        \n",
       "3  2016-04-23  28      2        1      2016-04-01  None     None  O        \n",
       "4  2016-04-23  4       2        1      2016-04-01  None     None  O        \n",
       "\n",
       "  entdepd entdepu matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0  None    U       None    1979     10282016  None   None   None     \n",
       "1  None    Y       None    1991     D/S       M      None   None     \n",
       "2  O       None    M       1961     09302016  M      None   OS       \n",
       "3  O       None    M       1988     09302016  None   None   AA       \n",
       "4  O       None    M       2012     09302016  None   None   AA       \n",
       "\n",
       "       admnum  fltno visatype    dtadddto  \n",
       "0  1897628485  None   B2       2016-10-28  \n",
       "1 -558170966   00296  F1       None        \n",
       "2  666643185   93     B2       2016-09-30  \n",
       "3 -2020819182  00199  B2       2016-09-30  \n",
       "4 -2020817382  00199  B2       2016-09-30  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@udf(TimestampType())\n",
    "def to_timestamp_udf(x):\n",
    "    try:\n",
    "        return pd.to_timedelta(x, unit='D') + pd.Timestamp('1960-1-1')\n",
    "    except:\n",
    "        return pd.Timestamp('1900-1-1')\n",
    "\n",
    "df_I94 = df_I94.withColumn('arrdate', to_date(to_timestamp_udf(col('arrdate'))))\\\n",
    "               .withColumn('depdate', to_date(to_timestamp_udf(col('depdate'))))\n",
    "\n",
    "df_I94.limit(5).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- print final schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: date (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: date (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- dtadfile: date (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: integer (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- dtadddto: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_I94.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.3 Explore I94 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- How many rows does the **I94** database has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_I94.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Let's see the gender distribution of the applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     F|1302743|\n",
      "|  null| 414269|\n",
      "|     M|1377224|\n",
      "|     U|    467|\n",
      "|     X|   1610|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_I94.select(\"gender\").groupBy(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Where are the I94 applicants **coming** from? \n",
    "\n",
    "I want to know the 10 most represented nations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|i94res| count|\n",
      "+------+------+\n",
      "|   135|368421|\n",
      "|   209|249167|\n",
      "|   245|185609|\n",
      "|   111|185339|\n",
      "|   582|179603|\n",
      "|   112|156613|\n",
      "|   276|136312|\n",
      "|   689|134907|\n",
      "|   438|112407|\n",
      "|   213|107193|\n",
      "+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_I94_count_visitors = df_I94.select(\"i94res\").groupby(\"i94res\").count().sort(col(\"count\").desc()).persist()\n",
    "df_I94_count_visitors.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The i94res code 135, where the highest number of visitors come from, corresponds to the the United Kingdom, as can be read in the accompanying file *I94_SAS_Labels_Descriptions.SAS*\n",
    "\n",
    "- What port registered the highest number of arrivals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|i94port| count|\n",
      "+-------+------+\n",
      "|    NYC|485916|\n",
      "|    MIA|343941|\n",
      "|    LOS|310163|\n",
      "|    SFR|152586|\n",
      "|    ORL|149195|\n",
      "|    HHW|142720|\n",
      "|    NEW|136122|\n",
      "|    CHI|130564|\n",
      "|    HOU|101481|\n",
      "|    FTL| 95977|\n",
      "+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_I94_total_arrivals_per_port = df_I94.groupBy('i94port').count().sort(col('count').desc()).persist()\n",
    "df_I94_total_arrivals_per_port.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "New York City port registered the highest number of arrivals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.4 Cleaning the I94 dataset\n",
    "\n",
    "These are the steps to perform on the I94 database:\n",
    "1. Identify null and NaN values. Remove duplicates (**quality check**).\n",
    "2. Find errors in the records (**quality check**) for example dates not in year 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Counting how many NaN in each column, excluding the date type columns *dtadfile*, *dtadddto*, *arrdate*, *depdate* because the **isnan** function works only on numerical types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid  i94yr  i94mon  i94cit  i94res  i94port  i94mode  i94addr  i94bir  \\\n",
       "0  0      0      0       0       0       0        0        0        0        \n",
       "\n",
       "   i94visa  count  visapost  occup  entdepa  entdepd  entdepu  matflag  \\\n",
       "0  0        0      0         0      0        0        0        0         \n",
       "\n",
       "   biryear  dtaddto  gender  insnum  airline  admnum  fltno  visatype  \n",
       "0  0        0        0       0       0        0       0      0         "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_I94.select([count(when(isnan(colname), 1)).alias(colname) for colname in df_I94.drop('dtadfile','dtadddto','arrdate','depdate').columns]).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- How many rows of the **I94** database have null value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_I94.na.fill(False).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The number of nulls equal the number of rows. It means there is at least one null on each row of the dataframe. \n",
    "\n",
    "- Now we can count how many null there are in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 101 ms, sys: 24.4 ms, total: 126 ms\n",
      "Wall time: 8min 13s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_cicid</th>\n",
       "      <th>sum_i94yr</th>\n",
       "      <th>sum_i94mon</th>\n",
       "      <th>sum_i94cit</th>\n",
       "      <th>sum_i94res</th>\n",
       "      <th>sum_i94port</th>\n",
       "      <th>sum_arrdate</th>\n",
       "      <th>sum_i94mode</th>\n",
       "      <th>sum_i94addr</th>\n",
       "      <th>sum_depdate</th>\n",
       "      <th>sum_i94bir</th>\n",
       "      <th>sum_i94visa</th>\n",
       "      <th>sum_count</th>\n",
       "      <th>sum_dtadfile</th>\n",
       "      <th>sum_visapost</th>\n",
       "      <th>sum_occup</th>\n",
       "      <th>sum_entdepa</th>\n",
       "      <th>sum_entdepd</th>\n",
       "      <th>sum_entdepu</th>\n",
       "      <th>sum_matflag</th>\n",
       "      <th>sum_biryear</th>\n",
       "      <th>sum_dtaddto</th>\n",
       "      <th>sum_gender</th>\n",
       "      <th>sum_insnum</th>\n",
       "      <th>sum_airline</th>\n",
       "      <th>sum_admnum</th>\n",
       "      <th>sum_fltno</th>\n",
       "      <th>sum_visatype</th>\n",
       "      <th>sum_dtadddto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>152592</td>\n",
       "      <td>0</td>\n",
       "      <td>802</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1881250</td>\n",
       "      <td>3088187</td>\n",
       "      <td>238</td>\n",
       "      <td>138429</td>\n",
       "      <td>3095921</td>\n",
       "      <td>138429</td>\n",
       "      <td>802</td>\n",
       "      <td>477</td>\n",
       "      <td>414269</td>\n",
       "      <td>2982605</td>\n",
       "      <td>83627</td>\n",
       "      <td>0</td>\n",
       "      <td>19549</td>\n",
       "      <td>0</td>\n",
       "      <td>45824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sum_cicid  sum_i94yr  sum_i94mon  sum_i94cit  sum_i94res  sum_i94port  \\\n",
       "0  0          0          0           0           0           0             \n",
       "\n",
       "   sum_arrdate  sum_i94mode  sum_i94addr  sum_depdate  sum_i94bir  \\\n",
       "0  0            239          152592       0            802          \n",
       "\n",
       "   sum_i94visa  sum_count  sum_dtadfile  sum_visapost  sum_occup  sum_entdepa  \\\n",
       "0  0            0          1             1881250       3088187    238           \n",
       "\n",
       "   sum_entdepd  sum_entdepu  sum_matflag  sum_biryear  sum_dtaddto  \\\n",
       "0  138429       3095921      138429       802          477           \n",
       "\n",
       "   sum_gender  sum_insnum  sum_airline  sum_admnum  sum_fltno  sum_visatype  \\\n",
       "0  414269      2982605     83627        0           19549      0              \n",
       "\n",
       "   sum_dtadddto  \n",
       "0  45824         "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_I94.select([count(when(col(colname).isNull(),1)).alias('sum_'+colname) for colname in df_I94.columns]).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are many nulls in many columns. \n",
    "\n",
    "The question is, if there is a need to correct/fill those nulls. \n",
    "\n",
    "Looking at the data, it seems like some field have been left empty for lack of information.\n",
    "\n",
    "Because these are categorical data there is no use, at this step, in assigning arbitrary values to the nulls.\n",
    "\n",
    "The nulls are not going to be filled apriori, but only if a specific need comes up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Are there duplicated rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_before = df_I94.count()\n",
    "count_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dropping duplicate row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_I94 = df_I94.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Cheching if the number changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_after = df_I94.count()\n",
    "count_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "No row has been dropped => no duplicated row\n",
    "\n",
    "- Verify that all rows have **i94yr** column equal 2016\n",
    "\n",
    "This gives confidence on the consistence of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_I94.where(col('i94yr')==2016).count() == df_I94.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.5 Store I94 data as parquet\n",
    "\n",
    "I94 data are stored in parquet format in an S3 bucket, they are partinioned using the fields: *year, month*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "S3_bucket_I94 = 'data/S3bucket_temp/I94_data'\n",
    "df_I94.write.format('parquet').mode('overwrite').partitionBy('i94yr','i94mon').save(S3_bucket_I94)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.6 The Airport codes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes = spark.read.csv('data/csv/airport-codes_csv.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_codes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "A snippet of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>None</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>None</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>None</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0  00A   heliport       Total Rf Heliport                   11.0           \n",
       "1  00AA  small_airport  Aero B Ranch Airport                3435.0         \n",
       "2  00AK  small_airport  Lowell Field                        450.0          \n",
       "3  00AL  small_airport  Epps Airpark                        820.0          \n",
       "4  00AR  closed         Newport Hospital & Clinic Heliport  237.0          \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0  NA        US          US-PA      Bensalem      00A      None       \n",
       "1  NA        US          US-KS      Leoti         00AA     None       \n",
       "2  NA        US          US-AK      Anchor Point  00AK     None       \n",
       "3  NA        US          US-AL      Harvest       00AL     None       \n",
       "4  NA        US          US-AR      Newport       None     None       \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0  00A        -74.93360137939453, 40.07080078125     \n",
       "1  00AA       -101.473911, 38.704022                 \n",
       "2  00AK       -151.695999146, 59.94919968            \n",
       "3  00AL       -86.77030181884766, 34.86479949951172  \n",
       "4  None       -91.254898, 35.6087                    "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_codes.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "How many records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_codes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_codes.drop_duplicates().count() == df_airport_codes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We discover there are some null fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_codes.dropna().count() == df_airport_codes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The nulls are in these colomuns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_ident</th>\n",
       "      <th>sum_type</th>\n",
       "      <th>sum_name</th>\n",
       "      <th>sum_elevation_ft</th>\n",
       "      <th>sum_continent</th>\n",
       "      <th>sum_iso_country</th>\n",
       "      <th>sum_iso_region</th>\n",
       "      <th>sum_municipality</th>\n",
       "      <th>sum_gps_code</th>\n",
       "      <th>sum_iata_code</th>\n",
       "      <th>sum_local_code</th>\n",
       "      <th>sum_coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5676</td>\n",
       "      <td>14045</td>\n",
       "      <td>45886</td>\n",
       "      <td>26389</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sum_ident  sum_type  sum_name  sum_elevation_ft  sum_continent  \\\n",
       "0  0          0         0         7006              0               \n",
       "\n",
       "   sum_iso_country  sum_iso_region  sum_municipality  sum_gps_code  \\\n",
       "0  0                0               5676              14045          \n",
       "\n",
       "   sum_iata_code  sum_local_code  sum_coordinates  \n",
       "0  45886          26389           0                "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_codes.select([count(when(col(colname).isNull(),1)).alias('sum_'+colname) for colname in df_airport_codes.columns]).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "No action taken to fill the nulls\n",
    "\n",
    "Finally, let's save the data in parquet format in our temporary folder mimicking the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "S3_airport_codes = \"data/S3bucket_temp/airport_codes\"\n",
    "\n",
    "df_airport_codes.write.parquet(S3_airport_codes, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 3. The Data Model\n",
    "\n",
    "The core of the architecture is a **data lake**, with S3 storage and EMR processing.\n",
    "\n",
    "The data are stored into S3 in **raw** and **parquet** format.\n",
    "\n",
    "Apache Spark is the tool elected for analytical tasks, therefore all data are loaded into Spark **dataframe** using a schema-on-read approach.\n",
    "\n",
    "For SQL queries style on the data, Spark temporary views are generated.\n",
    "\n",
    "### 3.1 Mapping Out Data Pipelines\n",
    "\n",
    "1. Provision the AWS S3 infrastructure\n",
    "2. Transfer data from the common storage to the S3 lake storage\n",
    "3. Provision an EMR cluster. It runs 2 steps then autoterminate, these are the 2 steps:  \n",
    "    3.1 Run a spark job to extract codes from file I94_SAS_Labels_Descriptions.SAS and save to S3  \n",
    "    3.2 Data cleaning. Find nan, null, duplicate. Save the clean data to parquet files  \n",
    "4. Generate reports using Spark query on S3 parquet data\n",
    "5. On-the-fly queries with Spark SQL\n",
    "\n",
    "![data lineage](./figures/udacity-dend-5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4. Run Pipeline to Model the Data\n",
    "\n",
    "#### 4.1 Provision the AWS S3 infrastructure\n",
    "\n",
    "Reading credentials and configuration from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('config.cfg'))\n",
    "\n",
    "KEY     = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "SECRET  = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "S3_BUCKET   = config['S3']['S3_bucket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create the bucket if it's not existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                    region_name=\"us-west-2\",\n",
    "                    aws_access_key_id     = KEY,\n",
    "                    aws_secret_access_key = SECRET)\n",
    "\n",
    "try:\n",
    "    s3.create_bucket(Bucket=S3_BUCKET, CreateBucketConfiguration={\n",
    "                    'LocationConstraint': 'us-west-2'})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Transfer raw data to S3 bucket\n",
    "\n",
    "Transfer the data from current shared storage (currently Udacity workspace) to S3 lake storage.\n",
    "\n",
    "A naive metadata system is implemented. It uses a json file to store basic information on each file added to the S3 bucket:\n",
    "- file name: file being processed\n",
    "- added by: user logged as | aws access id\n",
    "- date added: timestamp of date of processing\n",
    "- modified on: timestamp of modification time\n",
    "- notes: any additional information\n",
    "- access granted to (role or policy): admin | anyone | I94 access policy | weather data access policy |\n",
    "- expire date: 5 years (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "SHARED_STORAGE  = config['S3']['SHARED_STORAGE']\n",
    "\n",
    "S3_LAKE_RAWDATA = config['S3']['RAW_DATA']\n",
    "I94_DATA        = config['S3']['I94_DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def addMetadataRawFile(json_filepath, origin_filepath, notes = '', access_granted_to = 'anyone', expire_date = '5y'):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function stores metadata into a specified file\n",
    "    :json_filepath, json file containing the metadata\n",
    "    :origin_filepath, file which metadata are going to be appended to the metadata json file\n",
    "    :notes individual notes for each file\n",
    "    :access_granted_to, possibly values are secret, admin, anyone, policy_name\n",
    "    :expire_date\n",
    "    \"\"\"\n",
    "    \n",
    "    import pathlib\n",
    "    import json\n",
    "    import re\n",
    "    import datetime\n",
    "    \n",
    "    fname = pathlib.Path(origin_filepath)\n",
    "    \n",
    "    if os.path.isfile(json_filepath):\n",
    "        with open(json_filepath) as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = {}\n",
    "        metadata['rawData'] = []\n",
    "    \n",
    "    metadata['rawData'].append({\n",
    "        'filename': re.findall('([^\\/]*$)', origin_filepath)[0],\n",
    "        'filename_complete': origin_filepath,\n",
    "        'added_by': KEY,\n",
    "        'date_added': datetime.datetime.timestamp(datetime.datetime.now()),\n",
    "        'modified_on': fname.stat().st_mtime,\n",
    "        'notes': notes,\n",
    "        'access_granted_to': access_granted_to,\n",
    "        'expire_date': expire_date})\n",
    "    \n",
    "    with open(json_filepath, 'w') as outfile:\n",
    "        json.dump(metadata, outfile)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "def fileTransferToS3(origin_filepath, destination_folder, destination_file=None, json_filepath='./data/metadata/rawDataMetadata.json', notes = '', access_granted_to = 'anyone', expire_date = '5y'):\n",
    "    \"\"\"\n",
    "    transfer folder or file to S3 bucket\n",
    "    :origin_filepath, a folder or a file. If a folder is given, all the files inside are transferred to the remote destination_folder.\n",
    "    :destination_folder, folder in the S3 bucket, where to copy the files\n",
    "    :destination_file, it is valid only when origin_filepath is a file. If None, the name of the file in the origin_filepath will be used\n",
    "    :metadata_file, file contains the metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    #if a directory is passed as origin_filepath parameter all files are copied into the destination_folder\n",
    "    if os.path.isdir(origin_filepath):\n",
    "        filelist = os.walk(origin_filepath)\n",
    "\n",
    "        for root, subFolders, files in filelist:\n",
    "            for file in files:\n",
    "                origin_file_path_name = os.path.join(root, file)\n",
    "                bucket_file_path_name = os.path.join(destination_folder, origin_file_path_name.replace(origin_filepath, ''))\n",
    "\n",
    "                print('Transfering file: ', origin_file_path_name, ' ===> ', S3_BUCKET, '/', bucket_file_path_name, '\\n')\n",
    "\n",
    "                s3.meta.client.upload_file(origin_file_path_name, S3_BUCKET, bucket_file_path_name)\n",
    "                \n",
    "                addMetadataRawFile(json_filepath, origin_file_path_name, notes, access_granted_to, expire_date)\n",
    "    \n",
    "    #if a file is passed as origin_filepath\n",
    "    elif os.path.isfile(origin_filepath):\n",
    "        origin_file_path_name = origin_filepath\n",
    "        if destination_file == None:\n",
    "            filename = re.findall('([^\\/]*$)', origin_filepath)[0]\n",
    "            bucket_file_path_name = os.path.join(destination_folder, filename)\n",
    "        else:\n",
    "            bucket_file_path_name = os.path.join(destination_folder, destination_file)\n",
    "        \n",
    "        print('Transfering file: ', origin_file_path_name, ' ===> ', S3_BUCKET, '/', bucket_file_path_name, '\\n')\n",
    "        s3.meta.client.upload_file(origin_file_path_name, S3_BUCKET, bucket_file_path_name)\n",
    "        \n",
    "        addMetadataRawFile(json_filepath, origin_filepath, notes, access_granted_to, expire_date)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These dataset are moved to the S3 lake storage:\n",
    "- I94 immigration data\n",
    "- airport codes\n",
    "- US cities demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering file:  ../../data/I94_SAS_Labels_Descriptions.SAS  ===>  helptheplanet / raw/i94_data/I94_SAS_Labels_Descriptions.SAS \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat \n",
      "\n",
      "Transfering file:  ../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat  ===>  helptheplanet / raw/i94_data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileTransferToS3(SHARED_STORAGE, os.path.join(S3_LAKE_RAWDATA, I94_DATA), \n",
    "                 json_filepath='./data/metadata/rawDataMetadata.json', \n",
    "                 access_granted_to='I94_policy',\n",
    "                 expire_date = '20y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering file:  ./data/csv/airport-codes_csv.csv  ===>  helptheplanet / raw/csv/airport-codes_csv.csv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileTransferToS3('./data/csv/airport-codes_csv.csv', os.path.join(S3_LAKE_RAWDATA, 'csv'), \n",
    "                 json_filepath='./data/metadata/rawDataMetadata.json', \n",
    "                 access_granted_to='anyone',\n",
    "                 expire_date = '3y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering file:  ./data/csv/us-cities-demographics.csv  ===>  helptheplanet / raw/csv/us-cities-demographics.csv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileTransferToS3('./data/csv/us-cities-demographics.csv', os.path.join(S3_LAKE_RAWDATA, 'csv'),\n",
    "                 json_filepath='./data/metadata/rawDataMetadata.json', \n",
    "                 access_granted_to='anyone',\n",
    "                 expire_date = '20y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering file:  ./data/csv/temperatureData/GlobalLandTemperaturesByMajorCity.csv.zip  ===>  helptheplanet / raw/csv/GlobalLandTemperaturesByMajorCity.csv.zip \n",
      "\n",
      "Transfering file:  ./data/csv/temperatureData/GlobalLandTemperaturesByState.csv.zip  ===>  helptheplanet / raw/csv/GlobalLandTemperaturesByState.csv.zip \n",
      "\n",
      "Transfering file:  ./data/csv/temperatureData/GlobalLandTemperaturesByCity.csv.zip  ===>  helptheplanet / raw/csv/GlobalLandTemperaturesByCity.csv.zip \n",
      "\n",
      "Transfering file:  ./data/csv/temperatureData/GlobalLandTemperaturesByCountry.csv.zip  ===>  helptheplanet / raw/csv/GlobalLandTemperaturesByCountry.csv.zip \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileTransferToS3('./data/csv/temperatureData/', os.path.join(S3_LAKE_RAWDATA, 'csv'),\n",
    "                 json_filepath='./data/metadata/rawDataMetadata.json', \n",
    "                 access_granted_to='anyone',\n",
    "                 expire_date = '5y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 EMR cluster on EC2\n",
    "\n",
    "An EMR cluster on EC2 instances with Apache Spark preinstalled is used to perform the ELT work.\n",
    "\n",
    "A **3-nodes** cluster of **m5.xlarge** istances is configured by default in the *config.cfg* file.\n",
    "\n",
    "If the performance requires it, the cluster can be scaled up to use more nodes and/or bigger instances.\n",
    "\n",
    "After the cluster has been created, the steps to execute spark cleaning jobs are added to the EMR job flow, the steps are in separate .py files. These steps are added:\n",
    "\n",
    "- Spark job 1\n",
    "    - extract I94res, i94cit, i94port codes\n",
    "    - save the codes in a json file in S3\n",
    "\n",
    "\n",
    "- Spark job 2\n",
    "    - load I94 raw data from S3 \n",
    "    - change schema\n",
    "    - data cleaning\n",
    "    - save parquet data to S3\n",
    "\n",
    "The cluster is set to auto-terminate by default after executing all the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.1 Provision the EMR cluster\n",
    "\n",
    "Create the cluster using the code **emr_cluster.py** [Ref. 3] and **emr_cluster_spark_submit.py** and and set the steps to execute *spark_script_1* and *spark_script_2*.\n",
    "\n",
    "These scripts have already been previously uploaded to a dedicated folder in the project's S3 bucket, and are accessible from the EMR cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 21:26:30,293 - lib.emr_cluster - INFO - {'JobFlowId': 'j-Y4TX4JK0GZC5', 'ResponseMetadata': {'RequestId': '40ed075e-9c08-4d30-96f5-bbe6a48c469b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '40ed075e-9c08-4d30-96f5-bbe6a48c469b', 'content-type': 'application/x-amz-json-1.1', 'content-length': '116', 'date': 'Tue, 24 Aug 2021 21:26:29 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "emrCluster = emr_cluster.EMRLoader(config)\n",
    "\n",
    "spark_script_1 = 'spark_4_emr_codes_extraction.py'\n",
    "spark_script_2 = 'spark_4_emr_I94_processing.py'\n",
    "\n",
    "emrCluster.custom_steps = [\n",
    "    {\n",
    "        'Name': 'Run spark_script_1',\n",
    "        'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': [\n",
    "                'spark-submit',\n",
    "                '--deploy-mode', 'cluster',\n",
    "                '--py-files', 's3://helptheplanet/code/{}'.format(spark_script_1),\n",
    "                's3://helptheplanet/code/{}'.format(spark_script_1),\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'Name': 'Run spark_script_2',\n",
    "        'ActionOnFailure': 'TERMINATE_CLUSTER',\n",
    "        'HadoopJarStep': {\n",
    "            'Jar': 'command-runner.jar',\n",
    "            'Args': [\n",
    "                'spark-submit',\n",
    "                '--deploy-mode', 'cluster',\n",
    "                '--py-files', 's3://helptheplanet/code/{}'.format(spark_script_2),\n",
    "                's3://helptheplanet/code/{}'.format(spark_script_2),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "emrCluster.create_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The file *spark_4_emr_codes_extraction.py* contains the code for following paragraphs 4.3.1\n",
    "\n",
    "The file *spark_4_emr_I94_processing.py* contains the code for following paragraphs 4.3.2, 4.3.3, 4.3.4\n",
    "\n",
    "##### 4.3.2 Coded fields: I94CIT and I94RES\n",
    "\n",
    "I94CIT, I94RES contain codes indicating the country where the applicant is born (I94CIT), or resident (I94RES).\n",
    "\n",
    "The data is extracted from *I94_SAS_Labels_Descriptions.SAS*. This can be done sporadically or every time a change occurred, for example a new code has been added.\n",
    "\n",
    "The conceptual flow below was implemented. \n",
    "\n",
    "![data trasform](./figures/udacity-dend-4.png)\n",
    "\n",
    "First steps are define credential to access S3a then load the data in a dataframe, in a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID     = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_file = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "text_file = os.path.join('s3a://', S3_BUCKET, S3_LAKE_RAWDATA, text_file)\n",
    "\n",
    "df_label_wholetext = spark.read.text(text_file, wholetext=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Find the section of the file where **I94CIT** and **I94RES** are specified.\n",
    "\n",
    "It start with **I94CIT & I94RES** and finish with the **semicolon** character.\n",
    "\n",
    "To match the section, it is important to have the complete text in a single row, I did this using the option *wholetext=True* in the previous dataFrame read operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def match_section(filetext, pattern):\n",
    "#     pattern = '(\\/\\* I94CIT & I94RES[^;]+)'\n",
    "    match = re.search(pattern, filetext)\n",
    "    return match.group(0)\n",
    "\n",
    "pattern = \"(\\/\\* I94CIT & I94RES[^;]+)\"\n",
    "\n",
    "df_I94CIT_RES = df_label_wholetext.withColumn('I94CIT&RES', match_section(\"value\", lit(pattern))).select(\"I94CIT&RES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now I can split in a dataframe with multiple rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_I94CIT_RES_multi_rows = df_I94CIT_RES.withColumn('I94CIT&RES2', explode(split(\"I94CIT&RES\", \"[\\r\\n]+\"))).select('I94CIT&RES2')\n",
    "df_I94CIT_RES_multi_rows.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I filter the rows with structure \\<3-digit code> = \\<country_name>\n",
    "\n",
    "And then create 2 differents columns with `code` and `country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def parse_country_name(line):\n",
    "    pattern = \"(?<=')([0-9A-Za-z ,\\-()]+)(?=')\"\n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "df_I94CIT_RES_multi_rows_filtered = df_I94CIT_RES_multi_rows.filter(df_I94CIT_RES_multi_rows['I94CIT&RES2'].rlike(\"([0-9]+ *\\= *[0-9A-Za-z ',\\-()]+)\"))\\\n",
    "                                                            .withColumn('code', regexp_extract(df_I94CIT_RES_multi_rows['I94CIT&RES2'], \"[0-9]+\", 0))\\\n",
    "                                                            .withColumn('country', parse_country_name(col('I94CIT&RES2')))\\\n",
    "                                                            .drop(\"I94CIT&RES2\")\n",
    "\n",
    "df_I94CIT_RES_multi_rows_filtered.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I can finally store the data in a single file in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "PROC_DATA_JSON = config['S3']['PROC_DATA_JSON']\n",
    "file_pathname = 'country_codes'\n",
    "\n",
    "# S3_bucket_country_codes = 'data/S3bucket_temp/country_codes' #for local testing\n",
    "S3_bucket_country_codes = os.path.join('s3a://',S3_BUCKET, PROC_DATA_JSON, file_pathname )\n",
    "\n",
    "# print(S3_bucket_country_codes)\n",
    "df_I94CIT_RES_multi_rows_filtered.write.mode(\"overwrite\").format('json').save(S3_bucket_country_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.3 Coded field: I94PORT\n",
    "\n",
    "Similarly to extract the I94PORT codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def match_section(filetext, pattern):\n",
    "#     pattern = '(\\/\\* I94CIT & I94RES[^;]+)'\n",
    "    match = re.search(pattern, filetext)\n",
    "    return match.group(0)\n",
    "\n",
    "pattern = \"(\\/\\* I94PORT[^;]+)\"\n",
    "\n",
    "df_I94PORT = df_label_wholetext.withColumn('I94PORT', match_section(\"value\", lit(pattern))).select(\"I94PORT\")\n",
    "\n",
    "df_I94PORT_multi_rows = df_I94PORT.withColumn('I94PORT2', explode(split(\"I94PORT\", \"[\\r\\n]+\"))).select('I94PORT2')\n",
    "\n",
    "df_I94PORT_multi_rows_filtered = df_I94PORT_multi_rows.filter(df_I94PORT_multi_rows['I94PORT2'].rlike(\"([0-9A-Z.' ]+\\t*\\=\\t*[0-9A-Za-z \\',\\-()\\/\\.#&]*)\"))\\\n",
    "                                                    .withColumn('code', regexp_extract(df_I94PORT_multi_rows['I94PORT2'], \"(?<=')[0-9A-Z. ]+(?=')\", 0))\\\n",
    "                                                    .withColumn('city_state', regexp_extract(col('I94PORT2'), \"(?<=\\t')([0-9A-Za-z ,\\-()\\/\\.#&]+)(?=')\", 0))\\\n",
    "                                                    .withColumn('city', split(col('city_state'), ',').getItem(0))\\\n",
    "                                                    .withColumn('state', split(col('city_state'), ',').getItem(1))\\\n",
    "                                                    .withColumn('state', regexp_replace(col('state'), ' *$', ''))\\\n",
    "                                                    .drop('I94PORT2')\n",
    "\n",
    "df_I94PORT_multi_rows_filtered.show(5,truncate=False)\n",
    "\n",
    "file_pathname = 'port_codes'\n",
    "# S3_bucket_port_codes = 'data/S3bucket_temp/port_codes'\n",
    "S3_bucket_port_codes = os.path.join('s3a://',S3_BUCKET, PROC_DATA_JSON, file_pathname )\n",
    "\n",
    "df_I94PORT_multi_rows_filtered.write.mode(\"overwrite\").format('json').save(S3_bucket_port_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>The complete code for codes extraction is in <b>spark_4_emr_codes_extraction.py</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.4 Data cleaning\n",
    "\n",
    "The cleaning steps have already been shown in section 2, here are only summarized\n",
    "- Load dataset\n",
    "- Set schema \n",
    "    - Numeric fields: double to integer\n",
    "    - Fields *dtadfile* and *dtaddto*: string to date\n",
    "    - Fields *arrdate* and *depdate*: sas to date\n",
    "- Handle nulls: no fill is set by default\n",
    "- Drop duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_env = True\n",
    "I94_TEST_FILE = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "# load single file, entire dataset\n",
    "if test_env:\n",
    "    df_I94 = spark.read.format('com.github.saurfang.sas.spark').load(I94_TEST_FILE).persist()\n",
    "else:\n",
    "    df_I94 = spark.read.format('com.github.saurfang.sas.spark').load('{}/*/*.*'.format(I94_DATASET_PATH)).persist()\n",
    "    \n",
    "# modifying schema: Double => Integer\n",
    "toInt = udf(lambda x: int(x) if x!=None else x, IntegerType())\n",
    "df_I94 = df_I94.select( [ toInt(colname).alias(colname) if coltype == 'double' else colname for colname, coltype in df_I94.dtypes])\n",
    "\n",
    "\n",
    "# modifying schema: string to date\n",
    "df_I94 = df_I94.withColumn('dtadfile', to_date(col('dtadfile'), format='yyyyMMdd'))\\\n",
    "               .withColumn('dtadddto', to_date(col('dtaddto'), format='MMddyyyy'))\n",
    "\n",
    "@udf(TimestampType())\n",
    "def to_timestamp_udf(x):\n",
    "    try:\n",
    "        return pd.to_timedelta(x, unit='D') + pd.Timestamp('1960-1-1')\n",
    "    except:\n",
    "        return pd.Timestamp('1900-1-1')\n",
    "    \n",
    "# modifying schema: sas to date\n",
    "df_I94 = df_I94.withColumn('arrdate', to_date(to_timestamp_udf(col('arrdate'))))\\\n",
    "               .withColumn('depdate', to_date(to_timestamp_udf(col('depdate'))))\n",
    "\n",
    "# handle null, duplicate\n",
    "fill_value = False\n",
    "df_I94 = df_I94.na.fill(fill_value)   #null\n",
    "df_I94 = df_I94.drop_duplicates()     #duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3.5 Save clean data (parquet/json) to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "is_local  = False\n",
    "PROC_DATA = config['S3']['PROC_DATA']\n",
    "I94_DATA  = config['S3']['I94_DATA']\n",
    "\n",
    "if is_local:\n",
    "    S3_bucket_I94 = 'data/S3bucket_temp/I94_data' #for local testing\n",
    "else:\n",
    "    S3_bucket_I94 = os.path.join('s3a://', S3_BUCKET, PROC_DATA, I94_DATA)\n",
    "\n",
    "df_I94.write.format('parquet').mode('overwrite').partitionBy('i94yr', 'i94mon').save(S3_bucket_I94)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>The complete code, refactorized and modularized, is in **spark_4_emr_I94_processing.py**</font>\n",
    "\n",
    "As a side note, saving the test file as parquet takes about 3 minute on the provisioned cluster. The complete script execution takes 6 minutes.\n",
    "\n",
    "##### 4.3.6 Loading, cleaning and saving airport codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load the data from the shared location\n",
    "df_airport_codes = spark.read.csv('data/csv/airport-codes_csv.csv', header=True, inferSchema=True)\n",
    "\n",
    "#dropping duplicates\n",
    "df_airport_codes = df_airport_codes.drop_duplicates()\n",
    "\n",
    "#selecting the save path\n",
    "AIRPORT_CODES = config['S3']['AIRPORT_CODES']\n",
    "\n",
    "if is_local:\n",
    "    S3_airport_codes = \"data/S3bucket_temp/airport_codes\"\n",
    "else:\n",
    "    S3_bucket_I94 = os.path.join('s3a://',S3_BUCKET, PROC_DATA, AIRPORT_CODES)\n",
    "\n",
    "#save parquet\n",
    "df_airport_codes.write.parquet(S3_airport_codes, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Querying data on-the-fly\n",
    "\n",
    "The data in the data lake can be queried on-place. That is the Spark cluster on EMR is directly operating on S3 data.\n",
    "\n",
    "There are two possible ways to query the data:\n",
    "- using Spark dataframe functions\n",
    "- using SQL on tables\n",
    "\n",
    "We see example of both programming styles.\n",
    "\n",
    "These are some typical queries that are run on the data:\n",
    "\n",
    "1. For each port, in a given period, how many arrivals there are in each day?\n",
    "2. Where are the I94 applicants coming from, in a given period?\n",
    "3. In the given period, what port registered the highest number of arrivals?\n",
    "4. Number of arrivals in a given city for a given period\n",
    "5. Travelers genders\n",
    "6. Is there a city where the difference between male and female travelers is higher?\n",
    "7. Find most visited city (the function)\n",
    "\n",
    "The queries are collected in the Jupyter notebook **Capstone project 1 - Querying the data lake.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.5 Querying data using the SQL querying style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_I94.createOrReplaceTempView('DF_I94_TABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM DF_I94_TABLE\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.6 Data Quality Checks\n",
    " \n",
    "The query-in-place concept implemented here uses a very short pipeline, data are loaded from S3 and after a cleaning process are saved as parquet. Quality of the data is guaranteed by design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 5. Write Up\n",
    "\n",
    "The project has been set up with scalability in mind. All components used, S3 and EMR, offer higher degree of scalability, either horizontal and vertical. \n",
    "\n",
    "The tool used for the processing, Apache Spark, is the de facto tool for big data processing.\n",
    "\n",
    "To achieve such a level of scalability we sacrified processing speed. A data warehouse solution with a Redshift database or an OLAP cube would have been faster answering the queries. Anyway nothing forbids to add a DWH to stage the data in case of a more intensive, real-time responsive, usage of the data. \n",
    "\n",
    "An important part of an ELT/ETL process is automation. Although it has not been touched here, I believe the code developed here is prone to be automatized with a reasonable small effort. A tool like Apache Airflow can be used for the purpose.\n",
    "\n",
    "###### Scenario extension\n",
    "- The data was increased by 100x.\n",
    "\n",
    "In an increased data scenario, the EMR hardware needs to be scaled up accordingly. This is done by simply changing configuration in the **config.cfg** file. Apache Spark is the tool for big data processing, and is already used as the project analityc tool.\n",
    "\n",
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "In this case an orchestration tool like Apache Airflow is required. A DAG that trigger Phython scripts and Spark jobs executions, needs to be scheduled for daily execution at 7am.\n",
    "\n",
    "The results of the queries for the dashboard can be saved in a file.\n",
    "\n",
    "- The database needed to be accessed by 100+ people.\n",
    "\n",
    "A proper database wasn't used, on the contrary Amazon S3 is used to store data and queries them in-place. S3 is designed to massive scale in mind, it is able to handle sudden traffic spikes. Therefore, accessing the data by many people shouldn't be an issue.\n",
    "\n",
    "The programming used in the project, provision an EMR cluster for any user that plan to run it's queries. 100+ EMRs is probably going to be expensive for the company. A more efficient sharing of processing resources must be realized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 6. Lessons learned\n",
    "\n",
    "\n",
    "###### emr 5.28.1 use Python 2 as default\n",
    "- As a consequence important Python packages like **pandas** are not installed by default for Python 3.\n",
    "- To install them add an **action** to the EMR bootstrapping that install all packets\n",
    "    - install packages for Python 3: **python 3 -m pip install \\<packages list>**\n",
    "\n",
    "###### Adding jars packages to Spark\n",
    "For some reason adding the packages in the Python programm when instantiating the sparkSession doesn't work (error message package not found).\n",
    "This doesn't work:\n",
    "```\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(\"spark.jars\",\"https://repo1.maven.org/maven2/com/epam/parso/2.0.8/parso-2.0.8.jar,https://repos.spark-packages.org/saurfang/spark-sas7bdat/2.0.0-s_2.11/spark-sas7bdat-2.0.0-s_2.11.jar\")\\\n",
    "                    .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()\n",
    "```\n",
    "The packages must be added in the spark-submit:\n",
    "```\n",
    "spark-submit -jars https://repo1.maven.org/maven2/com/epam/parso/2.0.8/parso-2.0.8.jar,https://repos.spark-packages.org/saurfang/spark-sas7bdat/2.0.0-s_2.11/spark-sas7bdat-2.0.0-s_2.11.jar s3://helptheplanet/code/spark_4_emr_I94_processing.py\n",
    "```\n",
    "###### Debugging Spark on EMR\n",
    "While evrything work locally, it doesn't necessarily means that is going to work on the EMR cluster. Debugging the code is easier with SSH on EMR.\n",
    "\n",
    "###### Reading an S3 file from Python is tricky\n",
    "While reading with Spark is straightforward, one just needs to give the address s3://...., with Python boto3 must be used.\n",
    "\n",
    "###### Transfering file to S3\n",
    "During the debbuging phase, when the code on S3 must be changed many time using the web interface is slow and unpractical (*permanently delete* 🤪). Memorize this command:\n",
    "`aws s3 cp <local file> <s3 folder>`\n",
    "\n",
    "###### Removing the content of a directory from Python\n",
    "```import shutil\n",
    "dirPath = 'metastore_db'\n",
    "shutil.rmtree(dirPath)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 7. References\n",
    "1. [AWS CLI Command Reference](https://docs.aws.amazon.com/cli/latest/index.html)\n",
    "3. EMR provisioning is based on: Github repo [Boto-3 provisioning](https://github.com/marshackVB/boto3-provisioning/blob/master/emr/create_cluster.py)\n",
    "4. [Boto3 Command Reference](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
