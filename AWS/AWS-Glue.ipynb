{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0e021d-6f2e-4ec5-80f4-4ffaa3098589",
   "metadata": {},
   "source": [
    "https://www.tutorialspoint.com/how-to-use-boto3-to-start-a-crawler-in-aws-glue-data-catalog\n",
    "\n",
    "\n",
    "Step 1: Import boto3 and botocore exceptions to handle exceptions\n",
    "\n",
    "Step 2: crawler_name is the parameter in this function.\n",
    "\n",
    "Step 3: Create an AWS session using boto3 lib. Make sure region_name is mentioned in the default profile. If it is not mentioned, then explicitly pass the region_name while creating the session.\n",
    "\n",
    "Step 4: Create an AWS client for glue.\n",
    "\n",
    "Step 5: Now use the start_crawler function and pass the parameter crawler_name as Name.\n",
    "\n",
    "Step 6: It returns the response metadata and starts the crawler irrespective of its schedule. If the status of crawler is running, then it throws CrawlerRunningException.\n",
    "\n",
    "Step 7: Handle the generic exception if something went wrong while starting a crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab062167-27ba-4473-ac6d-841e71983b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f76e1b-4045-461c-81c3-8e16f331c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ····················\n"
     ]
    }
   ],
   "source": [
    "accessKeyID = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3bbee9-ef47-4ac9-a119-7696a558f572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········································\n"
     ]
    }
   ],
   "source": [
    "secretAccessKeyID = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d204ca-5088-4547-8066-c445f07ece44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd1cce-7c61-4dfe-983b-393b92dfb878",
   "metadata": {},
   "source": [
    "The following code starts an already existing crawler in AWS Glue Data Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bce18c9-8d28-404d-bb8b-2154ffabb42c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "boto3 client error in start_a_crawler: An error occurred (EntityNotFoundException) when calling the StartCrawler operation: Crawler with name Data Dimension does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEntityNotFoundException\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mstart_a_crawler\u001b[1;34m(crawler_name)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mglue_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrawler_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\de2022\\lib\\site-packages\\botocore\\client.py:401\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\de2022\\lib\\site-packages\\botocore\\client.py:731\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    730\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m--> 731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mEntityNotFoundException\u001b[0m: An error occurred (EntityNotFoundException) when calling the StartCrawler operation: Crawler with name Data Dimension does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error in start_a_crawler: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#1st time start the crawler\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstart_a_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData Dimension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#2nd time run, before crawler completes the operation\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(start_a_crawler(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mstart_a_crawler\u001b[1;34m(crawler_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboto3 client error in start_a_crawler: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m())\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error in start_a_crawler: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m())\n",
      "\u001b[1;31mException\u001b[0m: boto3 client error in start_a_crawler: An error occurred (EntityNotFoundException) when calling the StartCrawler operation: Crawler with name Data Dimension does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "def start_a_crawler(crawler_name):\n",
    "    session = boto3.session.Session(aws_access_key_id=accessKeyID, aws_secret_access_key=secretAccessKeyID)\n",
    "    glue_client = session.client('glue')\n",
    "    try:\n",
    "        response = glue_client.start_crawler(Name=crawler_name)\n",
    "        return response\n",
    "    except ClientError as e:\n",
    "        raise Exception(\"boto3 client error in start_a_crawler: \" + e.__str__())\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Unexpected error in start_a_crawler: \" + e.__str__())\n",
    "\n",
    "#1st time start the crawler\n",
    "print(start_a_crawler(\"Data Dimension\"))\n",
    "#2nd time run, before crawler completes the operation\n",
    "print(start_a_crawler(\"Data Dimension\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34d5265d-31a5-45c5-970d-763e334501e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(aws_access_key_id=accessKeyID, aws_secret_access_key=secretAccessKeyID)\n",
    "glue_client = session.client('glue', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282d15b-3b5e-4b6e-95f2-99f1ce3d18cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b39699d-325d-4017-ae34-6911e2830f5d",
   "metadata": {},
   "source": [
    "https://hands-on.cloud/working-with-aws-glue-in-python-using-boto3/#h-creating-an-aws-glue-crawler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c958eb7d-fe22-41f7-b386-5e3e73bf38c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidInputException",
     "evalue": "An error occurred (InvalidInputException) when calling the CreateCrawler operation: Service is unable to assume role arn:aws:iam::877061436404:role/GlueFullAccess. Please verify role's TrustPolicy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidInputException\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mglue_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCrawlerAZ1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mRole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGlueFullAccess\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mDatabaseName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmyGlueDb1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mTargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS3Targets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExclusions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mConnectionName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSampleSize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEventQueueArn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDlqEventQueueArn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mSchedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcron(15 12 * * ? *)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mSchemaChangePolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUpdateBehavior\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUPDATE_IN_DATABASE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDeleteBehavior\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDEPRECATE_IN_DATABASE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mRecrawlPolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRecrawlBehavior\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCRAWL_EVERYTHING\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mLineageConfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCrawlerLineageSettings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDISABLE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\de2022\\lib\\site-packages\\botocore\\client.py:401\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m py_operation_name)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\de2022\\lib\\site-packages\\botocore\\client.py:731\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    729\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    730\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m--> 731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mInvalidInputException\u001b[0m: An error occurred (InvalidInputException) when calling the CreateCrawler operation: Service is unable to assume role arn:aws:iam::877061436404:role/GlueFullAccess. Please verify role's TrustPolicy"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = glue_client.create_crawler(Name='CrawlerAZ1',\n",
    "                                      Role='GlueFullAccess',\n",
    "                                      DatabaseName='myGlueDb1',\n",
    "                                      Targets={ \n",
    "                                          'S3Targets': [\n",
    "                                              {\n",
    "                                                  'Path': 'string',\n",
    "                                                  'Exclusions': [\n",
    "                                                      'string',\n",
    "                                                  ],\n",
    "                                                  'ConnectionName': 'string',\n",
    "                                                  'SampleSize': 123,\n",
    "                                                  'EventQueueArn': 'string',\n",
    "                                                  'DlqEventQueueArn': 'string'\n",
    "                                              },\n",
    "                                          ],\n",
    "                                      },\n",
    "                                      Schedule='cron(15 12 * * ? *)',\n",
    "                                      SchemaChangePolicy={\n",
    "                                          'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                                          'DeleteBehavior': 'DEPRECATE_IN_DATABASE'\n",
    "                                      },\n",
    "                                      RecrawlPolicy={\n",
    "                                          'RecrawlBehavior': 'CRAWL_EVERYTHING'\n",
    "                                      },\n",
    "                                      LineageConfiguration={\n",
    "                                          'CrawlerLineageSettings': 'DISABLE'\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641ffa0-9014-43d4-9930-2c6f08ef8424",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a01dbc9d-a81a-4aed-a7e9-91461514c5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mglue_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Creates a new crawler with specified targets, role, configuration, and optional schedule. At least one crawl target must be specified, in the ``s3Targets`` field, the ``jdbcTargets`` field, or the ``DynamoDBTargets`` field.\n",
       "\n",
       "\n",
       "\n",
       "See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/glue-2017-03-31/CreateCrawler>`_\n",
       "\n",
       "\n",
       "**Request Syntax** \n",
       "::\n",
       "\n",
       "  response = client.create_crawler(\n",
       "      Name='string',\n",
       "      Role='string',\n",
       "      DatabaseName='string',\n",
       "      Description='string',\n",
       "      Targets={\n",
       "          'S3Targets': [\n",
       "              {\n",
       "                  'Path': 'string',\n",
       "                  'Exclusions': [\n",
       "                      'string',\n",
       "                  ],\n",
       "                  'ConnectionName': 'string',\n",
       "                  'SampleSize': 123,\n",
       "                  'EventQueueArn': 'string',\n",
       "                  'DlqEventQueueArn': 'string'\n",
       "              },\n",
       "          ],\n",
       "          'JdbcTargets': [\n",
       "              {\n",
       "                  'ConnectionName': 'string',\n",
       "                  'Path': 'string',\n",
       "                  'Exclusions': [\n",
       "                      'string',\n",
       "                  ]\n",
       "              },\n",
       "          ],\n",
       "          'MongoDBTargets': [\n",
       "              {\n",
       "                  'ConnectionName': 'string',\n",
       "                  'Path': 'string',\n",
       "                  'ScanAll': True|False\n",
       "              },\n",
       "          ],\n",
       "          'DynamoDBTargets': [\n",
       "              {\n",
       "                  'Path': 'string',\n",
       "                  'scanAll': True|False,\n",
       "                  'scanRate': 123.0\n",
       "              },\n",
       "          ],\n",
       "          'CatalogTargets': [\n",
       "              {\n",
       "                  'DatabaseName': 'string',\n",
       "                  'Tables': [\n",
       "                      'string',\n",
       "                  ],\n",
       "                  'ConnectionName': 'string'\n",
       "              },\n",
       "          ],\n",
       "          'DeltaTargets': [\n",
       "              {\n",
       "                  'DeltaTables': [\n",
       "                      'string',\n",
       "                  ],\n",
       "                  'ConnectionName': 'string',\n",
       "                  'WriteManifest': True|False\n",
       "              },\n",
       "          ]\n",
       "      },\n",
       "      Schedule='string',\n",
       "      Classifiers=[\n",
       "          'string',\n",
       "      ],\n",
       "      TablePrefix='string',\n",
       "      SchemaChangePolicy={\n",
       "          'UpdateBehavior': 'LOG'|'UPDATE_IN_DATABASE',\n",
       "          'DeleteBehavior': 'LOG'|'DELETE_FROM_DATABASE'|'DEPRECATE_IN_DATABASE'\n",
       "      },\n",
       "      RecrawlPolicy={\n",
       "          'RecrawlBehavior': 'CRAWL_EVERYTHING'|'CRAWL_NEW_FOLDERS_ONLY'|'CRAWL_EVENT_MODE'\n",
       "      },\n",
       "      LineageConfiguration={\n",
       "          'CrawlerLineageSettings': 'ENABLE'|'DISABLE'\n",
       "      },\n",
       "      LakeFormationConfiguration={\n",
       "          'UseLakeFormationCredentials': True|False,\n",
       "          'AccountId': 'string'\n",
       "      },\n",
       "      Configuration='string',\n",
       "      CrawlerSecurityConfiguration='string',\n",
       "      Tags={\n",
       "          'string': 'string'\n",
       "      }\n",
       "  )\n",
       ":type Name: string\n",
       ":param Name: **[REQUIRED]** \n",
       "\n",
       "  Name of the new crawler.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       ":type Role: string\n",
       ":param Role: **[REQUIRED]** \n",
       "\n",
       "  The IAM role or Amazon Resource Name (ARN) of an IAM role used by the new crawler to access customer resources.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       ":type DatabaseName: string\n",
       ":param DatabaseName: \n",
       "\n",
       "  The Glue database where results are written, such as: ``arn:aws:daylight:us-east-1::database/sometable/*`` .\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       ":type Description: string\n",
       ":param Description: \n",
       "\n",
       "  A description of the new crawler.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       ":type Targets: dict\n",
       ":param Targets: **[REQUIRED]** \n",
       "\n",
       "  A list of collection of targets to crawl.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       "  - **S3Targets** *(list) --* \n",
       "\n",
       "    Specifies Amazon Simple Storage Service (Amazon S3) targets.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "    - *(dict) --* \n",
       "\n",
       "      Specifies a data store in Amazon Simple Storage Service (Amazon S3).\n",
       "\n",
       "      \n",
       "\n",
       "    \n",
       "      - **Path** *(string) --* \n",
       "\n",
       "        The path to the Amazon S3 target.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **Exclusions** *(list) --* \n",
       "\n",
       "        A list of glob patterns used to exclude from the crawl. For more information, see `Catalog Tables with a Crawler <https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html>`__ .\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "        - *(string) --* \n",
       "\n",
       "        \n",
       "    \n",
       "      - **ConnectionName** *(string) --* \n",
       "\n",
       "        The name of a connection which allows a job or crawler to access data in Amazon S3 within an Amazon Virtual Private Cloud environment (Amazon VPC).\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **SampleSize** *(integer) --* \n",
       "\n",
       "        Sets the number of files in each leaf folder to be crawled when crawling sample files in a dataset. If not set, all the files are crawled. A valid value is an integer between 1 and 249.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **EventQueueArn** *(string) --* \n",
       "\n",
       "        A valid Amazon SQS ARN. For example, ``arn:aws:sqs:region:account:sqs`` .\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **DlqEventQueueArn** *(string) --* \n",
       "\n",
       "        A valid Amazon dead-letter SQS ARN. For example, ``arn:aws:sqs:region:account:deadLetterQueue`` .\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "    \n",
       "\n",
       "  - **JdbcTargets** *(list) --* \n",
       "\n",
       "    Specifies JDBC targets.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "    - *(dict) --* \n",
       "\n",
       "      Specifies a JDBC data store to crawl.\n",
       "\n",
       "      \n",
       "\n",
       "    \n",
       "      - **ConnectionName** *(string) --* \n",
       "\n",
       "        The name of the connection to use to connect to the JDBC target.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **Path** *(string) --* \n",
       "\n",
       "        The path of the JDBC target.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **Exclusions** *(list) --* \n",
       "\n",
       "        A list of glob patterns used to exclude from the crawl. For more information, see `Catalog Tables with a Crawler <https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html>`__ .\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "        - *(string) --* \n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "\n",
       "  - **MongoDBTargets** *(list) --* \n",
       "\n",
       "    Specifies Amazon DocumentDB or MongoDB targets.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "    - *(dict) --* \n",
       "\n",
       "      Specifies an Amazon DocumentDB or MongoDB data store to crawl.\n",
       "\n",
       "      \n",
       "\n",
       "    \n",
       "      - **ConnectionName** *(string) --* \n",
       "\n",
       "        The name of the connection to use to connect to the Amazon DocumentDB or MongoDB target.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **Path** *(string) --* \n",
       "\n",
       "        The path of the Amazon DocumentDB or MongoDB target (database/collection).\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **ScanAll** *(boolean) --* \n",
       "\n",
       "        Indicates whether to scan all the records, or to sample rows from the table. Scanning all the records can take a long time when the table is not a high throughput table.\n",
       "\n",
       "         \n",
       "\n",
       "        A value of ``true`` means to scan all records, while a value of ``false`` means to sample the records. If no value is specified, the value defaults to ``true`` .\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "    \n",
       "\n",
       "  - **DynamoDBTargets** *(list) --* \n",
       "\n",
       "    Specifies Amazon DynamoDB targets.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "    - *(dict) --* \n",
       "\n",
       "      Specifies an Amazon DynamoDB table to crawl.\n",
       "\n",
       "      \n",
       "\n",
       "    \n",
       "      - **Path** *(string) --* \n",
       "\n",
       "        The name of the DynamoDB table to crawl.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **scanAll** *(boolean) --* \n",
       "\n",
       "        Indicates whether to scan all the records, or to sample rows from the table. Scanning all the records can take a long time when the table is not a high throughput table.\n",
       "\n",
       "         \n",
       "\n",
       "        A value of ``true`` means to scan all records, while a value of ``false`` means to sample the records. If no value is specified, the value defaults to ``true`` .\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **scanRate** *(float) --* \n",
       "\n",
       "        The percentage of the configured read capacity units to use by the Glue crawler. Read capacity units is a term defined by DynamoDB, and is a numeric value that acts as rate limiter for the number of reads that can be performed on that table per second.\n",
       "\n",
       "         \n",
       "\n",
       "        The valid values are null or a value between 0.1 to 1.5. A null value is used when user does not provide a value, and defaults to 0.5 of the configured Read Capacity Unit (for provisioned tables), or 0.25 of the max configured Read Capacity Unit (for tables using on-demand mode).\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "    \n",
       "\n",
       "  - **CatalogTargets** *(list) --* \n",
       "\n",
       "    Specifies Glue Data Catalog targets.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "    - *(dict) --* \n",
       "\n",
       "      Specifies an Glue Data Catalog target.\n",
       "\n",
       "      \n",
       "\n",
       "    \n",
       "      - **DatabaseName** *(string) --* **[REQUIRED]** \n",
       "\n",
       "        The name of the database to be synchronized.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **Tables** *(list) --* **[REQUIRED]** \n",
       "\n",
       "        A list of the tables to be synchronized.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "        - *(string) --* \n",
       "\n",
       "        \n",
       "    \n",
       "      - **ConnectionName** *(string) --* \n",
       "\n",
       "        The name of the connection for an Amazon S3-backed Data Catalog table to be a target of the crawl when using a ``Catalog`` connection type paired with a ``NETWORK`` Connection type.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "    \n",
       "\n",
       "  - **DeltaTargets** *(list) --* \n",
       "\n",
       "    Specifies Delta data store targets.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "    - *(dict) --* \n",
       "\n",
       "      Specifies a Delta data store to crawl one or more Delta tables.\n",
       "\n",
       "      \n",
       "\n",
       "    \n",
       "      - **DeltaTables** *(list) --* \n",
       "\n",
       "        A list of the Amazon S3 paths to the Delta tables.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "        - *(string) --* \n",
       "\n",
       "        \n",
       "    \n",
       "      - **ConnectionName** *(string) --* \n",
       "\n",
       "        The name of the connection to use to connect to the Delta table target.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "      - **WriteManifest** *(boolean) --* \n",
       "\n",
       "        Specifies whether to write the manifest files to the Delta table path.\n",
       "\n",
       "        \n",
       "\n",
       "      \n",
       "    \n",
       "\n",
       "\n",
       ":type Schedule: string\n",
       ":param Schedule: \n",
       "\n",
       "  A ``cron`` expression used to specify the schedule (see `Time-Based Schedules for Jobs and Crawlers <https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html>`__ . For example, to run something every day at 12:15 UTC, you would specify: ``cron(15 12 * * ? *)`` .\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       ":type Classifiers: list\n",
       ":param Classifiers: \n",
       "\n",
       "  A list of custom classifiers that the user has registered. By default, all built-in classifiers are included in a crawl, but these custom classifiers always override the default classifiers for a given classification.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       "  - *(string) --* \n",
       "\n",
       "  \n",
       "\n",
       ":type TablePrefix: string\n",
       ":param TablePrefix: \n",
       "\n",
       "  The table prefix used for catalog tables that are created.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       ":type SchemaChangePolicy: dict\n",
       ":param SchemaChangePolicy: \n",
       "\n",
       "  The policy for the crawler's update and deletion behavior.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       "  - **UpdateBehavior** *(string) --* \n",
       "\n",
       "    The update behavior when the crawler finds a changed schema.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "  - **DeleteBehavior** *(string) --* \n",
       "\n",
       "    The deletion behavior when the crawler finds a deleted object.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "\n",
       ":type RecrawlPolicy: dict\n",
       ":param RecrawlPolicy: \n",
       "\n",
       "  A policy that specifies whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       "  - **RecrawlBehavior** *(string) --* \n",
       "\n",
       "    Specifies whether to crawl the entire dataset again or to crawl only folders that were added since the last crawler run.\n",
       "\n",
       "     \n",
       "\n",
       "    A value of ``CRAWL_EVERYTHING`` specifies crawling the entire dataset again.\n",
       "\n",
       "     \n",
       "\n",
       "    A value of ``CRAWL_NEW_FOLDERS_ONLY`` specifies crawling only folders that were added since the last crawler run.\n",
       "\n",
       "     \n",
       "\n",
       "    A value of ``CRAWL_EVENT_MODE`` specifies crawling only the changes identified by Amazon S3 events.\n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "\n",
       ":type LineageConfiguration: dict\n",
       ":param LineageConfiguration: \n",
       "\n",
       "  Specifies data lineage configuration settings for the crawler.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       "  - **CrawlerLineageSettings** *(string) --* \n",
       "\n",
       "    Specifies whether data lineage is enabled for the crawler. Valid values are:\n",
       "\n",
       "     \n",
       "\n",
       "     \n",
       "    * ENABLE: enables data lineage for the crawler \n",
       "     \n",
       "    * DISABLE: disables data lineage for the crawler \n",
       "     \n",
       "\n",
       "    \n",
       "\n",
       "  \n",
       "\n",
       ":type LakeFormationConfiguration: dict\n",
       ":param LakeFormationConfiguration: \n",
       "\n",
       "\n",
       "  - **UseLakeFormationCredentials** *(boolean) --* \n",
       "\n",
       "  \n",
       "  - **AccountId** *(string) --* \n",
       "\n",
       "  \n",
       "\n",
       ":type Configuration: string\n",
       ":param Configuration: \n",
       "\n",
       "  Crawler configuration information. This versioned JSON string allows users to specify aspects of a crawler's behavior. For more information, see `Configuring a Crawler <https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html>`__ .\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       ":type CrawlerSecurityConfiguration: string\n",
       ":param CrawlerSecurityConfiguration: \n",
       "\n",
       "  The name of the ``SecurityConfiguration`` structure to be used by this crawler.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       ":type Tags: dict\n",
       ":param Tags: \n",
       "\n",
       "  The tags to use with this crawler request. You may use tags to limit access to the crawler. For more information about tags in Glue, see `Amazon Web Services Tags in Glue <https://docs.aws.amazon.com/glue/latest/dg/monitor-tags.html>`__ in the developer guide.\n",
       "\n",
       "  \n",
       "\n",
       "\n",
       "  - *(string) --* \n",
       "\n",
       "  \n",
       "    - *(string) --* \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       ":rtype: dict\n",
       ":returns: \n",
       "  \n",
       "  **Response Syntax** \n",
       "\n",
       "  \n",
       "  ::\n",
       "\n",
       "    {}\n",
       "  **Response Structure** \n",
       "\n",
       "  \n",
       "\n",
       "  - *(dict) --* \n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "        \u001b[1;32mdef\u001b[0m \u001b[0m_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# We're accepting *args so that we can give a more helpful\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# error message than TypeError: _api_call takes exactly\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# 1 argument.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"%s() only accepts keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\anto\\anaconda3\\envs\\de2022\\lib\\site-packages\\botocore\\client.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??glue_client.create_crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82441b9c-fd16-43fe-904e-a9c11f39eab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python de2022",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
